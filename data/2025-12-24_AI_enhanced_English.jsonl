{"id": "2512.19799", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19799", "abs": "https://arxiv.org/abs/2512.19799", "authors": ["Tingjia Miao", "Jiawen Dai", "Jingkun Liu", "Jinxin Tan", "Muhua Zhang", "Wenkai Jin", "Yuwen Du", "Tian Jin", "Xianghe Pang", "Zexi Liu", "Tu Guo", "Zhengliang Zhang", "Yunjie Huang", "Shuo Chen", "Rui Ye", "Yuzhi Zhang", "Linfeng Zhang", "Kun Chen", "Wei Wang", "Weinan E", "Siheng Chen"], "title": "PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research", "comment": "32 pages, 10 figures", "summary": "Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.", "AI": {"tldr": "PhysMaster is an LLM-based autonomous physics research agent that combines abstract reasoning, numerical computation, and a structured knowledge/method repository to tackle long-horizon theoretical and computational physics problems, achieving large acceleration, automation, and some autonomous discovery.", "motivation": "Existing LLM-based scientific agents are mostly evaluated on narrow, well-defined benchmarks or simple tasks (e.g., literature search) and struggle with end-to-end, open-ended scientific problem solving, especially in physics, which is highly abstract, mathematically intensive, and code-heavy. There is a need for an AI system that can act more like a real theoretical/computational physicist, integrating reasoning, coding, literature use, and iterative hypothesis testing over long research horizons.", "method": "The authors build PhysMaster, an LLM-based agent architected to function as an autonomous theoretical and computational physicist. It tightly couples symbolic/abstract reasoning with numerical computation and is integrated with LANDAU (Layered Academic Data Universe), a structured environment that stores retrieved literature, curated domain knowledge, and validated methodological traces. This memory/trace system is used to improve the reliability and stability of decisions. PhysMaster also uses an adaptive exploration strategy that dynamically trades off efficiency against open-ended exploration, designed to handle ultra-long-horizon research tasks. The system is then applied to various domains of physics (high-energy theory, condensed matter theory, astrophysics).", "result": "In evaluations across multiple physics subfields, PhysMaster is reported to: (i) accelerate research by compressing work that would typically take months into hours, (ii) automate hypothesis-driven research loops with minimal human intervention, and (iii) demonstrate autonomous discovery capabilities by independently exploring and advancing open research problems. Quantitative metrics are not detailed in the abstract, but the claimed outcomes include substantial time savings and robust performance on complex, long-horizon tasks.", "conclusion": "The paper concludes that PhysMaster represents a significant step toward autonomous AI physicists, showing that LLM-based agents, when combined with structured knowledge/method repositories and adaptive exploration strategies, can meaningfully accelerate, automate, and even independently drive research in demanding theoretical and computational physics domains."}}
{"id": "2512.19882", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.19882", "abs": "https://arxiv.org/abs/2512.19882", "authors": ["Mahdi Mostajabdaveh", "F. Sibel Salman", "Walter J. Gutjahr"], "title": "A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution", "comment": null, "summary": "The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $\u03b5$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.", "AI": {"tldr": "The paper develops and solves a bi-objective vehicle routing and relief allocation problem that jointly considers fairness (via a Gini-based inequity measure) and efficiency (travel time) for post-disaster supply distribution, using an MIP model enhanced by a branch-and-price algorithm and valid inequalities, and demonstrates substantial improvements in equity without loss of efficiency on real and predicted earthquake data from Turkey.", "motivation": "In large-scale disasters, prepositioned relief supplies are typically insufficient to satisfy all shelter demands. Traditional routing models focus mainly on efficiency (e.g., minimizing travel time or cost) and often ignore or inadequately handle fairness, potentially leading to highly unequal allocations of critical supplies among shelters. Humanitarian operations, however, must balance fast delivery with equitable distribution, especially under tight resource and time constraints. There is a need for a rigorous optimization framework that explicitly models and efficiently solves this trade-off between equity and efficiency in vehicle routing and allocation of limited relief supplies.", "method": "The authors formulate a bi-objective Mixed Integer Programming (MIP) model for routing vehicles from a central depot to multiple shelters while allocating limited relief supplies. The two explicit objectives are: (1) minimizing a Gini-index-based measure of inequity in unsatisfied demand to capture fairness, and (2) minimizing total travel time to capture efficiency. They apply the \u03b5-constraint method to handle the bi-objective nature, converting it into a series of single-objective problems with constraints on the other objective. They derive structural properties of optimal solutions, use these to generate valid inequalities that strengthen the formulation, and design an algorithm that gives optimal allocation decisions for any given set of feasible routes. To address computational complexity, they develop a branch-and-price (B&P) algorithm, where routes are generated dynamically via column generation within a branch-and-bound framework, enabling efficient solution of realistically sized instances.", "result": "Computational experiments are conducted on realistic datasets derived from an actual earthquake in Van, Turkey, as well as on predicted disaster scenarios for the Kartal region of Istanbul. The proposed branch-and-price algorithm substantially outperforms standard commercial MIP solvers in terms of computation time and scalability. The bi-objective framework achieves a 34% reduction in inequity of aid distribution, as measured by the Gini-based index of unsatisfied demand, without increasing total travel time, showing that significant fairness gains can be obtained at little to no efficiency cost. Sensitivity analyses across different time constraint regimes reveal distinct behavioral patterns in the trade-off between fairness and efficiency.", "conclusion": "The paper concludes that explicitly modeling fairness via a Gini-based inequity measure within a bi-objective routing and allocation framework is both computationally tractable and practically impactful for post-disaster logistics. The branch-and-price solution approach, strengthened by valid inequalities and specialized allocation algorithms, is more effective than generic MIP solvers on realistic disaster instances. The study highlights that decision rules should depend on time pressure: when time limits are extremely loose or extremely tight, a lexicographic strategy that first maximizes demand coverage and then considers fairness is appropriate; however, under moderately restrictive time constraints, a balanced bi-objective optimization is necessary to avoid severe inequities. Overall, integrating equity and efficiency in humanitarian logistics planning can markedly improve the fairness of relief distribution without sacrificing delivery timeliness."}}
{"id": "2512.19937", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19937", "abs": "https://arxiv.org/abs/2512.19937", "authors": ["Eric Yeh", "John Cadigan", "Ran Chen", "Dick Crouch", "Melinda Gervasio", "Dayne Freitag"], "title": "Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs", "comment": "20 pages, 5 figures", "summary": "Recent research has explored using very large language models (LLMs) as proxies for humans in tasks such as simulation, surveys, and studies. While LLMs do not possess a human psychology, they often can emulate human behaviors with sufficiently high fidelity to drive simulations to test human behavioral hypotheses, exhibiting more nuance and range than the rule-based agents often employed in behavioral economics. One key area of interest is the effect of personality on decision making, but the requirement that a prompt must be created for every tested personality profile introduces experimental overhead and degrades replicability. To address this issue, we leverage interpolative decoding, representing each dimension of personality as a pair of opposed prompts and employing an interpolation parameter to simulate behavior along the dimension. We show that interpolative decoding reliably modulates scores along each of the Big Five dimensions. We then show how interpolative decoding causes LLMs to mimic human decision-making behavior in economic games, replicating results from human psychological research. Finally, we present preliminary results of our efforts to ``twin'' individual human players in a collaborative game through systematic search for points in interpolation space that cause the system to replicate actions taken by the human subject.", "AI": {"tldr": "The paper proposes using interpolative decoding with large language models to simulate human-like personalities and decisions in behavioral experiments, reducing prompt-engineering overhead while maintaining control and replicability.", "motivation": "Existing uses of LLMs as human proxies in behavioral research require crafting a separate, detailed prompt for each personality profile. This is labor-intensive, hard to standardize, and undermines replicability, especially when studying how personality traits affect decisions. The authors want a more systematic, parameterized way to control personality in LLM-based agents so they can emulate human behavioral patterns in a scalable and reproducible manner.", "method": "The authors introduce interpolative decoding for personality control in LLMs. For each Big Five personality dimension, they define two opposing prompts (e.g., high vs. low trait descriptions). During generation, they interpolate between these prompts using a continuous parameter that effectively positions the model along that personality spectrum. They then (1) measure how this affects Big Five personality scores, (2) evaluate whether this parameterization reproduces known human behavioral patterns in economic games, and (3) conduct a preliminary search procedure in this interpolation space to find points that best match the behavior of specific human participants in a collaborative game (\"twining\").", "result": "Interpolative decoding successfully modulates personality traits along each of the Big Five dimensions as assessed by relevant scoring procedures, indicating that the method gives continuous control over the model\u2019s expressed personality. Under these controlled personalities, LLM agents exhibit decision patterns in economic games that resemble human psychological research findings, suggesting the method can replicate known personality\u2013behavior relationships. Initial twinning experiments show that adjusting interpolation parameters enables the system to approximate the observed actions of individual human players in a collaborative game, though these results are preliminary.", "conclusion": "Interpolative decoding provides a structured, low-overhead, and reproducible way to control simulated personality in LLM-based agents, enabling them to better serve as proxies for humans in behavioral and economic research. The approach reliably adjusts Big Five trait expression, reproduces human-like decision-making in economic tasks, and shows early promise for matching the behavior of specific individuals. This suggests LLMs, when parameterized through such methods, can become practical tools for scalable, personality-sensitive simulations and experiments."}}
{"id": "2512.19957", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19957", "abs": "https://arxiv.org/abs/2512.19957", "authors": ["Luciano Araujo Dourado Filho", "Almir Moreira da Silva Neto", "Rodrigo Pereira David", "Rodrigo Tripodi Calumby"], "title": "Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification", "comment": null, "summary": "This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \\href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.", "AI": {"tldr": "They propose a prototype-guided segmentation ViT approach for multi-label plant species identification in high-resolution images, achieving competitive performance (5th place) in PlantCLEF 2025.", "motivation": "PlantCLEF 2025 requires accurate multi-label identification of many plant species in large, high-resolution vegetation plot images, which is challenging due to fine-grained differences, clutter, and domain shift between single-plant training images and complex test scenes. The authors seek a way to transfer knowledge from single-species training data to multi-species test plots and to better localize species within large images.", "method": "1) Extract visual features from training images using a DinoV2 model pre-trained for single-species classification on the PlantCLEF data. 2) Cluster these features with K-Means, using K equal to the number of classes, to obtain a prototype representation for each species. 3) Build a narrow segmentation ViT, where the patch embedding layer is replaced by the frozen DinoV2 feature extractor. 4) Train this ViT on test-set images in a reconstruction-style task: given a high-resolution test image, the network is optimized to reconstruct the class prototypes (from step 2), effectively aligning test-image patches with species-level prototypes. 5) Use the ViT\u2019s attention maps and reconstruction outputs as attention scores to localize relevant regions and guide multi-label classification over the whole plot image.", "result": "The approach successfully adapts a model trained for single-species classification to the multi-label, high-resolution PlantCLEF 2025 setting. On the private leaderboard, it achieves an F1 score of 0.33331, ranking 5th overall and only about 0.03 F1 behind the top-performing method, indicating competitive performance.", "conclusion": "Prototype-guided segmentation using a ViT with a frozen DinoV2 backbone can effectively bridge the gap between single-species training data and multi-label, high-resolution vegetation plots. By reconstructing class prototypes and exploiting attention maps, the method provides useful localization cues and attains near\u2013state-of-the-art performance in PlantCLEF 2025. The authors release their code to facilitate reproducibility and further research."}}
{"id": "2512.19864", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19864", "abs": "https://arxiv.org/abs/2512.19864", "authors": ["Shashi Kant Gupta", "Arijeet Pramanik", "Jerrin John Thomas", "Regina Schwind", "Lauren Wiener", "Avi Raju", "Jeremy Kornbluth", "Yanshan Wang", "Zhaohui Su", "Hrituraj Singh"], "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data", "comment": "39 Pages, Supplementary Included", "summary": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale", "AI": {"tldr": "They build an LLM-based agentic system that turns messy oncology notes and PDFs into accurate, structured cancer variables at the patient level, achieving very high F1 and reducing manual chart review effort.", "motivation": "Oncology-relevant information is mostly buried in unstructured EHR notes and scanned reports, making it hard to obtain scalable, structured data for treatment decisions and research. Manual abstraction is accurate but too expensive and slow; current automated NLP methods are narrow, often synthetic, and usually operate only at document level or for a few variables, failing to reconcile contradictory information across many notes for each patient. There is a need for a scalable, reliable, patient-level extraction method that works on real-world oncology notes and covers a broad set of clinical variables.", "method": "They design an agentic framework where large language models act as reasoning agents that break down oncology data extraction into modular tasks. The system uses context-sensitive retrieval over large collections of unstructured notes and scanned PDFs, and performs iterative synthesis to reconcile and aggregate information at the patient level. The agents are configured to exhaustively extract a wide set of predefined oncology clinical variables into structured form from real-world EHR text. They then evaluate the framework on a large dataset (400k+ notes, 2,250 patients) using standard information extraction metrics and measure its impact when embedded in a data curation workflow.", "result": "On the dataset of 400,000+ notes and scanned reports from 2,250 cancer patients, the system attains an average F1-score of 0.93 across 103 oncology-specific variables, with 100 of them above 0.85 F1. Crucial variables like biomarkers and medications exceed 0.95 F1. When integrated into a real curation pipeline, 94% of model-extracted data are directly approved by human annotators, substantially lowering manual annotation effort and cost.", "conclusion": "The proposed LLM-based agentic framework can reliably and comprehensively extract structured oncology data from large volumes of heterogeneous, real-world EHR notes at the patient level. It outperforms prior narrow or document-level approaches, scales to many clinical variables with high accuracy, and meaningfully reduces the human labor needed for data curation. The work represents, to the authors\u2019 knowledge, the first exhaustive end-to-end deployment of such an agent system for oncology information extraction at scale."}}
{"id": "2512.19960", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19960", "abs": "https://arxiv.org/abs/2512.19960", "authors": ["Luciano Araujo Dourado Filho", "Rodrigo Tripodi Calumby"], "title": "FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification", "comment": null, "summary": "Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \\href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.", "AI": {"tldr": "The paper tackles intra-class variability in fine-grained visual categorization by introducing a hierarchical method that learns fine-grained features through class-wise clustering and pseudo-label classification, achieving state-of-the-art results on PlantNet300k.", "motivation": "In fine-grained visual categorization (FGVC), images within the same class can differ substantially (high intra-class variability), especially when classes are underrepresented. This variability makes it hard for deep learning models to learn discriminative features and can degrade classification performance. The authors aim to reduce the negative impact of intra-class variability while dealing with typically imbalanced, fine-grained datasets like PlantNet300k (plant species).", "method": "The method first performs clustering separately within each class to discover pseudo-labels that represent latent similarity groups inside that class. These pseudo-labels are then used in a hierarchical classification scheme: the model not only predicts the original class label but also learns to predict the corresponding intra-class cluster assignment. This dual objective encourages the network to learn more fine-grained, discriminative visual features that correspond to subtle intra-class differences. The approach is evaluated on the FGVC benchmark PlantNet300k.", "result": "Initial experiments on the PlantNet300k dataset show that the proposed hierarchical clustering-based method reaches state-of-the-art performance despite some components not being fully optimized. The experiments also highlight several aspects that need refinement and further investigation to conclusively establish the method\u2019s advantages.", "conclusion": "The paper concludes that class-wise clustering to create pseudo-labels, combined with hierarchical classification, is a promising strategy to mitigate intra-class variability in FGVC tasks. While the current implementation is not fully optimized and results are still preliminary, the method already attains state-of-the-art performance on PlantNet300k, indicating strong potential and motivating further development and analysis."}}
{"id": "2512.19903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19903", "abs": "https://arxiv.org/abs/2512.19903", "authors": ["Kirk Vanacore", "Rene F. Kizilcec"], "title": "How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse", "comment": null, "summary": "Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.", "AI": {"tldr": "The paper evaluates how well several large language models can, without heavy customization, classify teachers\u2019 instructional moves in real classroom transcripts using different prompting strategies.", "motivation": "As LLMs are rapidly integrated into educational tools for tasks like tutoring and assessment, it is critical to understand their baseline, out-of-the-box ability to interpret real classroom discourse. This sets realistic expectations, informs design choices, and guides benchmarking without assuming costly model fine-tuning.", "method": "The authors benchmark six LLMs on a classification task: labeling instructional moves in authentic classroom transcripts. They systematically vary prompting strategies\u2014zero-shot, one-shot, and few-shot\u2014and compare model predictions with expert human annotations using metrics such as Cohen\u2019s Kappa, along with analyses broken down by instructional move type and error patterns (recall vs false positives).", "result": "Zero-shot performance is only moderate. When given richer few-shot examples, state-of-the-art models improve substantially, with the best configuration achieving Cohen\u2019s Kappa of 0.58 relative to expert coders. However, gains are uneven across different instructional moves, and strategies that boost recall often also increase false positives, indicating notable trade-offs and unreliability in certain categories.", "conclusion": "Current foundation models can meaningfully but imperfectly interpret instructional discourse in classroom transcripts. Careful prompt design, especially few-shot prompting, can reveal and enhance their capabilities but does not fully resolve reliability issues or category-specific weaknesses. Thus, LLMs can support but not yet replace expert judgment in interpreting complex educational interactions, and system designers must account for these limitations."}}
{"id": "2512.19992", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.19992", "abs": "https://arxiv.org/abs/2512.19992", "authors": ["Zhe Sun", "Xueyuan Yang", "Yujie Lu", "Zhenliang Zhang"], "title": "S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test", "comment": "10 pages, 9 figures", "summary": "The integration of embodied agents into human environments demands embodied social intelligence: reasoning over both social norms and physical constraints. However, existing evaluations fail to address this integration, as they are limited to either disembodied social reasoning (e.g., in text) or socially-agnostic physical tasks. Both approaches fail to assess an agent's ability to integrate and trade off both physical and social constraints within a realistic, embodied context. To address this challenge, we introduce Spatially Situated Social Intelligence Test (S$^{3}$IT), a benchmark specifically designed to evaluate embodied social intelligence. It is centered on a novel and challenging seat-ordering task, requiring an agent to arrange seating in a 3D environment for a group of large language model-driven (LLM-driven) NPCs with diverse identities, preferences, and intricate interpersonal relationships. Our procedurally extensible framework generates a vast and diverse scenario space with controllable difficulty, compelling the agent to acquire preferences through active dialogue, perceive the environment via autonomous exploration, and perform multi-objective optimization within a complex constraint network. We evaluate state-of-the-art LLMs on S$^{3}$IT and found that they still struggle with this problem, showing an obvious gap compared with the human baseline. Results imply that LLMs have deficiencies in spatial intelligence, yet simultaneously demonstrate their ability to achieve near human-level competence in resolving conflicts that possess explicit textual cues.", "AI": {"tldr": "Introduces S^3IT, a benchmark to test embodied social intelligence via a complex 3D seat-ordering task with LLM-driven NPCs, revealing a significant performance gap between LLMs and humans, especially in spatial reasoning.", "motivation": "Existing evaluations of social intelligence in AI either focus on text-only social reasoning or on physical tasks that ignore social factors, so they cannot test an agent\u2019s ability to jointly reason over social norms and physical constraints in realistic, embodied scenarios. The authors aim to fill this gap with a benchmark that tightly couples social reasoning, spatial reasoning, and embodiment.", "method": "They design S^3IT, a Spatially Situated Social Intelligence Test, centered on a 3D seating-arrangement task. In procedurally generated environments, an agent must seat LLM-driven NPCs who have varied identities, preferences, and interpersonal relationships. The framework allows controllable difficulty and produces diverse scenarios. Agents must: (1) explore the 3D environment to perceive spatial constraints, (2) interact via dialogue to elicit NPC preferences and relationships, and (3) perform multi-objective optimization to satisfy both social and physical constraints. They then benchmark state-of-the-art LLM-based agents on this task and compare to human performance.", "result": "State-of-the-art LLMs perform poorly on S^3IT relative to human participants, revealing a clear performance gap. The models particularly struggle with tasks that require integrating spatial understanding with social constraints, although they perform much closer to humans on conflict-resolution subproblems where relevant cues are explicitly available in text.", "conclusion": "S^3IT exposes current limitations of LLMs in embodied social intelligence, especially in integrating spatial reasoning with social norms and interpersonal preferences. While LLMs can approach human-level competence when social conflicts are clearly articulated in text, they are weak at handling such conflicts when they are embedded in richer spatial and embodied contexts. The benchmark thus provides a challenging, extensible testbed to drive progress in developing agents with genuinely embodied social intelligence."}}
{"id": "2512.19908", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.19908", "abs": "https://arxiv.org/abs/2512.19908", "authors": ["Jingyi Qiu", "Hong Chen", "Zongyi Li"], "title": "Counterfactual LLM-based Framework for Measuring Rhetorical Style", "comment": null, "summary": "The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.", "AI": {"tldr": "The paper proposes an LLM-based, counterfactual framework to quantitatively measure rhetorical style in ML papers, independently of their substantive content, and shows that visionary rhetoric predicts attention and has increased notably since 2023, largely due to LLM-assisted writing.", "motivation": "There is growing concern that hype and bold rhetoric in AI/ML papers may distort scientific evaluation and incentives, but there has been no reliable way to separate rhetorical style from actual empirical contributions at scale. The authors aim to rigorously measure rhetoric, understand its impact on attention and recognition, and assess how LLM tools are changing scientific writing.", "method": "They construct a counterfactual framework using large language models: (1) define multiple rhetorical personas that differ in writing style; (2) for each paper\u2019s underlying substantive content, use these personas to generate multiple counterfactual versions of the writing; (3) use an LLM judge to perform pairwise comparisons of these versions on rhetorical strength; and (4) aggregate pairwise outcomes using a Bradley\u2013Terry model to produce quantitative scores of rhetorical style. They apply this pipeline to 8,485 ICLR submissions (2017\u20132025), creating over 250,000 counterfactual texts, then analyze correlations between rhetorical style, peer-review scores, citations, media attention, and the adoption of LLM-assisted writing. Robustness is checked by varying personas and comparing LLM-based evaluations with human annotations.", "result": "The framework yields stable, large-scale measurements of rhetorical style for thousands of ICLR submissions. The authors find that visionary or strongly framed rhetoric significantly predicts later attention (citations and media coverage) even after controlling for peer-review evaluations, indicating an independent effect of style. They also find a marked increase in rhetorical strength after 2023 and show empirically that this rise is largely associated with the increased use of LLM-based writing tools. LLM judgments of rhetoric correlate highly with human annotations and are robust to the specific choice of rhetorical personas.", "conclusion": "LLMs can be systematically used as instruments to disentangle and quantify rhetorical style in scientific papers, independent of their substantive content. Visionary rhetoric has a meaningful, independent impact on downstream attention, and LLM-based writing assistance appears to be reshaping the rhetorical landscape of ML research, amplifying overall rhetorical strength. The validated robustness of the framework suggests it could support more nuanced and fair scientific evaluation and meta-science studies."}}
{"id": "2512.20043", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20043", "abs": "https://arxiv.org/abs/2512.20043", "authors": ["Jung Yeon Park", "Yuxuan Chen", "Floor Eijkelboom", "Jan-Willem van de Meent", "Lawson L. S. Wong", "Robin Walters"], "title": "Discovering Lie Groups with Flow Matching", "comment": null, "summary": "Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \\lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.", "AI": {"tldr": "The paper proposes LieFlow, a method to discover symmetries in data by learning probability flows on Lie groups.", "motivation": "Symmetries are crucial both in physics and machine learning: they help explain physical systems and improve model performance and sample efficiency. However, in many settings the relevant symmetries are not known a priori and must be discovered from data. Existing symmetry-discovery approaches are limited in the types of groups they can handle and often rely on strong assumptions. The paper aims to develop a more general, data-driven approach to uncover underlying symmetries.", "method": "The authors cast symmetry discovery as learning a distribution over a hypothesis group that should match the symmetries manifested in data. They use flow matching on Lie groups to construct continuous-time probability flows that transform a base distribution into the target distribution over group elements. Their method, called LieFlow, operates on general Lie groups, including complex-valued groups, enabling the discovery of discrete symmetries like rotations and reflections. They analyze a practical challenge\u2014`last-minute convergence`\u2014where symmetric target modes lead to flows that stay near the initial distribution for most of the trajectory and only move near the end, and propose a new interpolation scheme for flow matching tailored to overcome this in symmetry discovery tasks.", "result": "On synthetic 2D and 3D point cloud datasets, LieFlow successfully recovers various discrete symmetry groups, including those involving reflections, by operating in the complex domain. The experiments show that LieFlow can handle a broader class of groups than previous methods and does so under weaker assumptions, while effectively learning multimodal, symmetric distributions over group elements.", "conclusion": "The paper concludes that learning probability flows on Lie groups is a powerful and flexible framework for symmetry discovery from data. LieFlow can discover a wide range of discrete symmetry groups with fewer structural assumptions than prior work. The authors also highlight the importance of addressing `last-minute convergence` in flow matching for symmetric targets and demonstrate that their proposed interpolation scheme mitigates this issue, improving practical symmetry discovery performance."}}
{"id": "2512.20052", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20052", "abs": "https://arxiv.org/abs/2512.20052", "authors": ["Hung-Chieh Fang", "Kuo-Han Hung", "Chu-Rong Chen", "Po-Jung Chou", "Chun-Kai Yang", "Po-Chen Ko", "Yu-Chiang Wang", "Yueh-Hua Wu", "Min-Hung Chen", "Shao-Hua Sun"], "title": "Learning Skills from Action-Free Videos", "comment": null, "summary": "Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.", "AI": {"tldr": "The paper presents SOF, a framework that learns latent robot skills from large, action-free videos by using optical flow as an intermediate motion representation, enabling high-level planning and better translation to low-level actions.", "motivation": "Current video generative models are visually impressive but hard to convert into concrete robot actions, while latent-action models are more aligned with actions but typically only handle single-step predictions and lack high-level planning. There is a need for a method that can learn reusable, composable skills from raw videos without action labels and still be easily grounded in robot control.", "method": "The authors introduce Skill Abstraction from Optical Flow (SOF), which first computes optical flow from large collections of action-free videos to capture motion. They then learn a flow-based latent skill space that is aligned with both video dynamics and robot actions. This latent space represents temporally extended skills rather than single-step actions. High-level planning is performed in this skill latent space, and learned skills are subsequently translated into low-level robot actions for execution.", "result": "Empirical evaluations show that SOF yields consistent performance gains over baselines in both multitask and long-horizon robotic settings. The method is able to acquire meaningful skills from raw visual data and use them more effectively for complex tasks that require composing multiple skills over time.", "conclusion": "Leveraging optical-flow-based latent skills learned from action-free videos enables robots to perform high-level planning and skill composition while maintaining a clear path to low-level action execution. This approach narrows the gap between powerful video models and practical robotic control, suggesting that large-scale unlabeled video can be an effective resource for building more generalist robots."}}
{"id": "2512.19950", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19950", "abs": "https://arxiv.org/abs/2512.19950", "authors": ["Heet Bodara", "Md Masum Mushfiq", "Isma Farah Siddiqui"], "title": "Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems", "comment": null, "summary": "Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.", "AI": {"tldr": "The paper investigates subtle tone bias in large language model (LLM) based conversational assistants, showing that even when prompted neutrally, LLMs produce systematically skewed tones that can be detected with high accuracy.", "motivation": "As LLMs increasingly power digital personal assistants, their default tone (e.g., overly polite, cheerful, or cautious) can affect users\u2019 perceptions of trust, empathy, and fairness. However, such tone biases are underexplored compared to more obvious content biases. The authors aim to characterize and measure this hidden behavioral trait to inform fair and trustworthy assistant design.", "method": "They synthesize two dialogue datasets with an LLM: one from ostensibly neutral prompts and another where the model is explicitly instructed to adopt positive or negative tones. They then use a pretrained DistilBERT model in a weak-supervision setup to label utterances by tone. Based on these labels, they train multiple tone classifiers, including ensemble models, to detect tone categories and quantify tone bias patterns in the LLM outputs.", "result": "Both datasets\u2014especially the supposedly neutral one\u2014show systematic tonal skew, indicating that the LLM\u2019s default conversational style is biased toward certain tones. Ensemble tone classification models reach macro F1 scores up to 0.92, confirming that these tonal patterns are strong and learnable rather than random noise.", "conclusion": "Tone bias in LLM-based assistants is a consistent, measurable phenomenon that persists even under neutral prompting. This makes tone an important design and governance variable for conversational AI: understanding and controlling it is crucial for building systems perceived as fair, trustworthy, and emotionally appropriate in user interactions."}}
{"id": "2512.20056", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20056", "abs": "https://arxiv.org/abs/2512.20056", "authors": ["Hao Li", "Fabian Deuser", "Wenping Yin", "Steffen Knoblauch", "Wufan Zhao", "Filip Biljecki", "Yong Xue", "Wei Huang"], "title": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach", "comment": null, "summary": "As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC", "AI": {"tldr": "The paper introduces ProbGLC, a probabilistic cross-view geolocalization framework that fuses deterministic and generative models to rapidly and accurately localize disaster scenes from imagery while providing uncertainty estimates for better disaster response.", "motivation": "Climate change is increasing the frequency and severity of extreme weather and disaster events, making rapid and precise localization of affected areas critical for effective response and resource allocation. Existing geolocalization methods often lack both high accuracy and explicit uncertainty quantification, which limits their reliability and interpretability in time-sensitive disaster scenarios.", "method": "The authors design ProbGLC, a unified probabilistic cross-view geolocalization framework that integrates probabilistic (generative) and deterministic models. It operates on cross-view imagery pairs (e.g., ground and overhead/satellite) of disaster scenes, producing both point location estimates and full probabilistic spatial distributions along with a localizability score that reflects confidence. The method is tailored for multiple disaster types and evaluated on two specialized cross-view disaster datasets (MultiIAN and SAGAINDisaster).", "result": "On the MultiIAN and SAGAINDisaster datasets, ProbGLC achieves state-of-the-art cross-view geolocalization performance, reporting Acc@1km of 0.86 and Acc@25km of 0.97. It additionally outputs probabilistic location distributions and localizability scores, offering interpretable uncertainty information alongside high accuracy.", "conclusion": "ProbGLC effectively combines probabilistic and deterministic geolocalization to deliver accurate, explainable cross-view geolocalization for diverse disaster types. Its strong performance and uncertainty-aware outputs suggest substantial potential for improving situational awareness and accelerating decision-making in real-world disaster response operations."}}
{"id": "2512.19995", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19995", "abs": "https://arxiv.org/abs/2512.19995", "authors": ["Ming Li", "Chenrui Fan", "Yize Cheng", "Soheil Feizi", "Tianyi Zhou"], "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models", "comment": null, "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.", "AI": {"tldr": "The paper introduces ThinkARM, a framework that converts raw reasoning traces from language models into high-level reasoning steps to better understand and compare their internal thinking processes.", "motivation": "Although large language models can output chain-of-thought style reasoning traces, it is hard to understand their underlying cognitive structure using only surface-level token statistics. The authors are motivated to find an intermediate-scale, cognitively meaningful representation that can explain and compare how different models reason.", "method": "They adopt Schoenfeld's Episode Theory as a theoretical basis and define a set of functional reasoning step types (e.g., Analysis, Explore, Implement, Verify). They then build ThinkARM, a scalable framework that maps model reasoning traces into sequences of these higher-level steps and apply it to mathematical problem-solving traces from a variety of models, including both reasoning-optimized and standard models.", "result": "The abstraction into episode-level steps reveals stable, reproducible dynamics in models\u2019 thinking processes and uncovers structural differences between reasoning and non-reasoning models that are not captured by token-level analysis. Their case studies show that exploratory steps are key branching points linked to solution correctness, and that methods aimed at making models more efficient tend to remove evaluative/feedback steps instead of just shortening all parts of the reasoning uniformly.", "conclusion": "Episode-level representations of reasoning, as instantiated in ThinkARM, make the structure of language model reasoning explicit and analyzable. This enables systematic study of how reasoning is organized and changed by different training or prompting methods, providing a more informative lens than raw token-level traces."}}
{"id": "2512.20061", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20061", "abs": "https://arxiv.org/abs/2512.20061", "authors": ["Hamed Firooz", "Rui Liu", "Yuchen Lu", "Zhenyu Hou", "Fangzhou Xiong", "Xiaoyang Zhang", "Changshu Jian", "Zhicheng Zhu", "Jiayuan Ma", "Jacob Tao", "Chaitali Gupta", "Xiaochang Peng", "Shike Mei", "Hang Cui", "Yang Qin", "Shuo Tang", "Jason Gaedtke", "Arpit Mittal"], "title": "Scaling Reinforcement Learning for Content Moderation with Large Language Models", "comment": null, "summary": "Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.", "AI": {"tldr": "The paper empirically studies how to use reinforcement learning to turn general-purpose LLMs into accurate, policy-aligned content moderation classifiers, showing strong gains and high data efficiency over supervised fine-tuning.", "motivation": "Content moderation must handle billions of user- and AI-generated items, but achieving expert-level, policy-grounded classification is hard due to scarce labels, evolving policies, and the need for nuanced reasoning that goes beyond keyword matching. Existing LLM-based moderation shows promise but lacks a clear understanding of how to train and scale such systems reliably in real-world settings.", "method": "The authors conduct a systematic empirical study of scaling reinforcement learning for content classification across three real-world moderation tasks. They evaluate multiple RL training setups and reward-shaping strategies, including verifiable rewards (objective checks) and LLM-as-judge schemes, to specialize general-purpose LLMs into policy-aligned classifiers. They vary training data size, number of rollouts, and optimization steps to understand scaling behavior.", "result": "The study finds that RL-based moderation models follow a sigmoid-like scaling curve: performance improves smoothly as training data, rollouts, and optimization steps increase, then gradually saturates. RL significantly boosts performance on tasks that need complex, policy-grounded reasoning and can reach target accuracy with up to 100x less labeled data than supervised fine-tuning. This indicates strong data efficiency and suitability for label-scarce domains.", "conclusion": "Reinforcement learning is an effective way to specialize LLMs into high-accuracy, policy-aligned content moderation classifiers. Its sigmoid scaling behavior offers predictable returns from additional data and computation, and its large data-efficiency advantage over supervised fine-tuning makes it particularly attractive when expert annotations are limited or expensive. The work offers practical guidance for deploying RL-based moderation at industrial scale."}}
{"id": "2512.20092", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20092", "abs": "https://arxiv.org/abs/2512.20092", "authors": ["Yiming Du", "Baojun Wang", "Yifan Xiang", "Zhaowei Wang", "Wenyu Huang", "Boyang Xue", "Bin Liang", "Xingshan Zeng", "Fei Mi", "Haoli Bai", "Lifeng Shang", "Jeff Z. Pan", "Yuxin Jiang", "Kam-Fai Wong"], "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents", "comment": null, "summary": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/", "AI": {"tldr": "The paper proposes Memory-T1, an RL-based, time-aware memory selection framework that significantly improves temporal reasoning over long, noisy multi-session dialogues by learning to select temporally pertinent evidence from dialogue histories.", "motivation": "As multi-session dialogue histories become long and noisy, existing long-context language models struggle to locate temporally relevant information, which severely degrades temporal reasoning performance. There is a need for a method that can robustly select and ground the right evidence in time, especially under very long contexts, to support accurate question answering and reasoning in dialogue agents.", "method": "The authors design Memory-T1, a time-aware memory selection framework that operates in a coarse-to-fine manner. First, it prunes the full dialogue history into a smaller candidate set using temporal and relevance-based filters. Then, a reinforcement learning (RL) agent selects the final evidence sessions from these candidates. The RL process is guided by a multi-level reward function comprising: (i) answer accuracy (how well the selected evidence supports the correct answer), (ii) evidence grounding (encouraging selection of sessions that truly support the answer), and (iii) temporal consistency (encouraging alignment between evidence and the query\u2019s time scope). Temporal consistency is computed both at the session level (proximity in time to the query) and at the utterance level (fidelity to chronological order), providing dense learning signals to resolve subtle chronological ambiguities.", "result": "On the Time-Dialog benchmark, Memory-T1 applied to a 7B model achieves an overall score of 67.0%, setting a new state of the art among open-source models and surpassing a 14B baseline by 10.2 percentage points. Ablation experiments demonstrate that temporal consistency and evidence grounding rewards together yield a 15.0% performance improvement. Memory-T1 also remains robust with dialogue histories up to 128k tokens, where baseline long-context models fail, indicating strong noise resistance and scalability to very long contexts.", "conclusion": "Memory-T1 effectively tackles temporal reasoning over long, noisy multi-session dialogues by learning a time-aware, RL-based memory selection policy. Its coarse-to-fine selection strategy and multi-level reward design, particularly the temporal consistency component, allow it to identify temporally appropriate evidence and maintain strong performance even with extremely long contexts. This leads to state-of-the-art results on Time-Dialog for open-source models and demonstrates a practical path for robust temporal reasoning in conversational agents."}}
{"id": "2512.20074", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20074", "abs": "https://arxiv.org/abs/2512.20074", "authors": ["H M Quamran Hasan", "Housam Khalifa Bashier", "Jiayi Dai", "Mi-Young Kim", "Randy Goebel"], "title": "Reason2Decide: Rationale-Driven Multi-Task Learning", "comment": null, "summary": "Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.", "AI": {"tldr": "Reason2Decide is a two-stage training framework that improves both prediction accuracy and explanation quality for clinical decision support models.", "motivation": "Clinical decision support systems need not only accurate predictions but also reliable, prediction-aligned explanations. Existing self-rationalizing LLM approaches suffer from exposure bias (training on gold labels but testing on model predictions) and poorly integrated handling of prediction and explanation, leading to misaligned or unfaithful rationales. There is also a need to reduce dependence on costly human-authored medical rationales while enabling deployment on smaller models for resource-constrained clinical settings.", "method": "Reason2Decide uses a two-stage training process. Stage-1 trains the model purely on rationale generation, teaching it to produce clinical explanations. Stage-2 jointly trains the model for label prediction and rationale generation. During this joint training, the authors apply scheduled sampling to gradually transition the model\u2019s conditioning from gold labels to its own predicted labels, mitigating exposure bias. The framework is evaluated on three medical datasets (a proprietary triage dataset and two public biomedical QA datasets), with rationales sourced from LLMs and nurses (authoring and post-processing).", "result": "Across model sizes and datasets, Reason2Decide surpasses standard fine-tuning baselines and some zero-shot LLMs in both predictive performance (F1 score) and rationale fidelity (measured by BERTScore, BLEU, and LLM-as-a-Judge metrics). In the triage setting, the method remains robust regardless of whether rationales originate from LLMs, nurses, or nurse-post-processed texts. Notably, even when Stage-1 uses only LLM-generated rationales, Reason2Decide still outperforms other fine-tuning variants, demonstrating the effectiveness of synthetic rationales for pretraining.", "conclusion": "Reason2Decide effectively tackles exposure bias and task separation in self-rationalizing clinical models, yielding better-aligned and more faithful explanations alongside strong predictive performance. The ability to train with LLM-generated rationales reduces reliance on expensive human annotations. Furthermore, because these gains are achieved with models roughly 40x smaller than contemporary foundation models, the framework enables more accessible, explainable clinical decision support in resource-limited environments."}}
{"id": "2512.20097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20097", "abs": "https://arxiv.org/abs/2512.20097", "authors": ["Zuo Wang", "Ye Yuan"], "title": "A Novel Graph-Sequence Learning Model for Inductive Text Classification", "comment": null, "summary": "Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.", "AI": {"tldr": "They propose TextGSL, a model that combines graph neural networks with Transformer-based sequence modeling for inductive text classification, handling diverse word relations and unseen words/edges, and it outperforms strong baselines on benchmark datasets.", "motivation": "Existing GNN-based text classification methods are strong at modeling structural relations but still suffer from two gaps: (1) they underutilize the variety of structural information among word pairs (co-occurrence, syntax, semantic relations); (2) they ignore word order/sequence information within the graph learning module and struggle to classify texts containing new words or relations that were unseen during training. The paper aims to build a model that integrates diverse structural signals and sequence information while remaining inductive (generalizing to new words and relations).", "method": "They introduce TextGSL, a graph-sequence learning model. For each input text, they build a single text-level graph whose nodes are all words in that text. Edges between word pairs are typed to encode different relationships (co-occurrence, syntactic dependencies, semantic similarity, etc.). On top of this multi-edge graph, they design an adaptive multi-edge message passing mechanism that aggregates information across the different edge types to learn richer structural representations. To incorporate word order and contextual sequence information, they augment the graph component with Transformer layers, yielding a joint graph + sequence representation for each text. The framework is inductive in that it builds per-text graphs and operates on them without requiring all words/relations to have been seen during training.", "result": "On multiple benchmark text classification datasets, TextGSL is empirically compared against a range of strong baselines, including conventional neural text classifiers and GNN-based models. Across these datasets, TextGSL consistently achieves higher classification accuracy than the baselines, demonstrating the benefit of combining multi-edge graph modeling with Transformer-based sequence modeling.", "conclusion": "The paper concludes that integrating diverse structural relationships between words via a multi-edge GNN, together with sequence information modeled by Transformers, leads to more discriminative text representations and superior performance on text classification tasks. TextGSL effectively addresses previous limitations of GNN-based text classifiers, particularly their inability to fully exploit heterogeneous structural information and to handle inductive scenarios with new words and relations."}}
{"id": "2512.20082", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20082", "abs": "https://arxiv.org/abs/2512.20082", "authors": ["Chaithra", "Kamesh Kadimisetty", "Biju R Mohan"], "title": "Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches", "comment": "Accepted in CODS 2025", "summary": "Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.", "AI": {"tldr": "They build an adaptive, market-aware financial sentiment analysis framework that combines an instruction-tuned LLaMA model, RAG, and reinforcement learning with real stock return feedback to improve sentiment classification for Indian stocks.", "motivation": "Traditional financial sentiment analysis ignores how actual stock price movements (market feedback) relate to sentiment labels, leading to static models that may not align with real market behavior, especially in a specific context like the Indian stock market. The authors aim to close this gap by directly integrating stock return feedback into the sentiment modeling process, so predictions are both accurate and better aligned with how markets actually react to news.", "method": "1) Fine-tune LLaMA 3.2 3B on the SentiFin dataset using instruction-based learning for financial sentiment classification. 2) Build a retrieval-augmented generation (RAG) pipeline to pull in multi-source contextual information (e.g., news, fundamentals, other data) via cosine similarity on sentence embeddings. 3) Add a feedback-driven module that updates the reliability weights of different information sources by comparing predicted sentiment with realized next-day stock returns. 4) Generalize this adaptive weighting using a reinforcement learning agent trained with Proximal Policy Optimization (PPO), where the agent learns a source-weighting policy that maximizes cumulative reward tied to sentiment\u2013return alignment. 5) Evaluate the framework on NIFTY 50 news headlines from 2024\u20132025 against baseline models and static retrieval approaches.", "result": "On NIFTY 50 news headlines (2024\u20132025), the proposed system outperforms baseline models and static retrieval-based sentiment classifiers in terms of classification accuracy, F1-score, and an explicit metric of market alignment between predicted sentiment and realized returns. This indicates that incorporating dynamic source reweighting, stock-return feedback, and RL-based policy learning improves both predictive performance and consistency with market behavior.", "conclusion": "Integrating an instruction-tuned LLM with a feedback-driven RAG pipeline and an RL-based source-weighting mechanism yields a more robust and market-aware financial sentiment analysis framework. By adapting to real stock return feedback over time, the system better aligns sentiment predictions with actual market reactions, especially in the Indian stock market context. The study suggests that coupling LLMs with dynamic feedback and reinforcement learning is a promising direction for future financial NLP systems."}}
{"id": "2512.20111", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20111", "abs": "https://arxiv.org/abs/2512.20111", "authors": ["Aly Lidayan", "Jakob Bjorner", "Satvik Golechha", "Kartik Goyal", "Alane Suhr"], "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language", "comment": null, "summary": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.", "AI": {"tldr": "They propose ABBEL, a framework where LLM agents keep a concise belief-state summary instead of the full interaction history, and show that RL fine-tuning of belief formation and use can outperform full-context baselines with lower memory use.", "motivation": "Long-horizon sequential decision-making with LLM agents becomes costly or infeasible if the entire interaction history must be kept in the context window. There is a need for a principled way to compress interaction histories into shorter, task-relevant representations while preserving performance and interpretability.", "method": "They introduce ABBEL, where at each step the agent maintains a natural-language belief state summarizing task-relevant unknowns. The agent updates a prior belief with the latest observation to form a posterior belief, and then chooses the next action using only this posterior, not the full history. They evaluate various LLMs under ABBEL in six multi-step environments. To mitigate error propagation and improve performance, they apply reinforcement learning to train models to generate and act on beliefs, using custom rewards including belief-quality grading and penalties on belief length to encourage concise, accurate summaries.", "result": "ABBEL enables interpretable, natural-language belief states and keeps memory usage near-constant over long interactions. However, naive bottlenecking introduces error propagation that can hurt performance relative to full-context baselines. With RL post-training tailored to belief quality and compression, ABBEL-based agents improve their decision quality and can surpass full-context performance while still using less memory than baseline methods.", "conclusion": "Natural-language belief bottlenecks like ABBEL are a viable way to scale LLM agents to long-horizon tasks with constrained memory, offering interpretability and efficiency. Although bottlenecks can initially introduce harmful error accumulation, reinforcement learning that directly optimizes belief formation and usage can not only offset these issues but also yield performance better than full-context agents at lower memory cost."}}
{"id": "2512.20135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20135", "abs": "https://arxiv.org/abs/2512.20135", "authors": ["Zhuo Yang", "Yeyun chen", "Jiaqing Xie", "Ben Gao", "Shuaike Shen", "Wanhao Liu", "Liujia Yang", "Beilun Wang", "Tianfan Fu", "Yuqiang Li"], "title": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization", "comment": null, "summary": "Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.", "AI": {"tldr": "MolAct is an agentic reinforcement learning framework where an LLM learns to perform multi-step molecular editing and optimization via tool-augmented interactions, achieving state-of-the-art validity and strong property-optimization performance.", "motivation": "Molecular design tasks such as editing and optimizing molecules are inherently multi-step and require maintaining chemical validity and structural similarity while improving specific properties. Existing methods often treat these as single-step or weakly interactive problems and lack explicit reasoning and tool integration, limiting reliability and interpretability. The authors aim to formalize molecular design as an agentic RL problem where an LLM can reason, use tools, and iteratively refine molecules in a principled way.", "method": "The authors propose MolAct, an agentic reinforcement learning framework with a two-stage training paradigm. In stage one, an LLM agent is trained to acquire robust molecular editing skills (add, delete, substitute) using tools for chemical validity checks, property evaluation, and similarity control. In stage two, the same agent is further trained for molecular optimization, reusing the learned editing behaviors but now with rewards focused on improving target properties (e.g., LogP, solubility) while maintaining constraints. The framework allows multi-turn interactions where the agent interleaves natural-language reasoning with tool calls, observes feedback, and iteratively refines the molecule. Two instantiations are trained: MolEditAgent for editing tasks and MolOptAgent for optimization tasks, each in different model sizes (e.g., 3B, 7B).", "result": "MolEditAgent-7B achieves very high rates of chemically valid edits\u2014100% for add, 95% for delete, and 98% for substitute\u2014outperforming strong closed-source \u201cthinking\u201d models such as DeepSeek-R1. The smaller MolEditAgent-3B attains performance comparable to much larger open \u201cthinking\u201d models like Qwen3-32B-think. For optimization, MolOptAgent-7B (initialized from MolEditAgent-7B) surpasses the best closed \u201cthinking\u201d baseline (e.g., Claude 3.7) on LogP optimization and remains competitive on solubility, with balanced performance across other objectives. These results suggest that the agentic RL formulation with tool use leads to more accurate and reliable molecular design.", "conclusion": "Treating molecular design as an agentic, multi-step reinforcement learning problem\u2014where an LLM interleaves reasoning and structured tool use for validity, property assessment, and similarity control\u2014yields strong and interpretable performance on both molecular editing and property optimization. The two-stage MolAct training process enables reusable editing skills that transfer effectively to downstream optimization, and empirically outperforms or matches strong \u201cthinking\u201d LLM baselines, indicating that tool-augmented, sequential decision-making is a powerful paradigm for molecular design."}}
{"id": "2512.20136", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20136", "abs": "https://arxiv.org/abs/2512.20136", "authors": ["Hyeongcheol Park", "Jiyoung Seo", "Jaewon Mun", "Hogun Park", "Wonmin Byeon", "Sung June Kim", "Hyeonsoo Im", "JeungSub Lee", "Sangpil Kim"], "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.", "AI": {"tldr": "The paper introduces M^3KG-RAG, a multimodal retrieval-augmented generation framework that builds and queries a multi-hop multimodal knowledge graph to provide more relevant, grounded audio-visual knowledge to MLLMs, improving reasoning and answer faithfulness.", "motivation": "Existing multimodal RAG systems for audio-visual tasks struggle because current multimodal knowledge graphs have poor modality coverage and weak multi-hop connectivity, and because retrieval relies only on similarity in a shared embedding space, which often returns off-topic or redundant information. The authors want a system that can retrieve knowledge that is both tightly grounded in the query and structured for deeper reasoning.", "method": "They propose M^3KG-RAG, which has two main components: (1) a lightweight multi-agent pipeline to construct a multi-hop multimodal knowledge graph (M^3KG) containing context-enriched triplets of multimodal entities, allowing modality-wise retrieval conditioned on the query; and (2) GRASP (Grounded Retrieval And Selective Pruning), which first grounds entities precisely to the query, then scores their relevance to answer support, and finally prunes redundant or off-topic context so that only essential knowledge is fed to the MLLM for generation.", "result": "In experiments over multiple multimodal benchmarks, the system consistently outperforms existing multimodal RAG baselines. It yields better multimodal reasoning and stronger grounding of model answers to the retrieved audio-visual knowledge, indicating improved faithfulness and effectiveness of the proposed retrieval and pruning mechanisms.", "conclusion": "Constructing a multi-hop, modality-rich multimodal knowledge graph and coupling it with a grounded, relevance-aware retrieval and pruning mechanism (GRASP) substantially improves multimodal RAG in audio-visual domains. The approach addresses both structural limitations of existing MMKGs and semantic issues in retrieval, leading to deeper reasoning and more faithful MLLM outputs across benchmarks."}}
{"id": "2512.20140", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20140", "abs": "https://arxiv.org/abs/2512.20140", "authors": ["Xingyou Yin", "Ceyao Zhang", "Min Hu", "Kai Chen"], "title": "Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection", "comment": "9 pages,3 figures", "summary": "Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.", "AI": {"tldr": "The paper shows that adding noise to raw time series before converting them to text greatly stabilizes and improves zero-shot forecasting performance of fully frozen, off-the-shelf LLMs.", "motivation": "Off-the-shelf LLMs can perform time series forecasting in a zero-shot manner, but their performance is highly sensitive to how numerical time series are tokenized into text, since the models cannot be fine-tuned or adapted to distribution shifts. There is a need for a simple, general method that makes such frozen LLM forecasters more robust without modifying model weights or adding specialized adaptation modules, and to test this in settings free from data contamination.", "method": "The authors propose injecting noise into the raw time series prior to tokenization as an inference-time augmentation technique. This noise-perturbed data is then converted into textual form and fed to a fully frozen LLM for zero-shot forecasting. They provide a theoretical analysis explaining why noise encourages the model to rely on stable temporal patterns rather than brittle numeric details, and empirically evaluate the approach on multiple benchmarks. They also construct two new time series datasets that are guaranteed to be out-of-distribution relative to the pre-training data of the used LLMs, and test the method there as well.", "result": "Noise injection before tokenization significantly improves the robustness and accuracy of frozen LLMs in zero-shot time series forecasting across diverse benchmarks. The approach mitigates the brittleness due to specific tokenization choices and distribution shifts. On the two newly introduced contamination-free datasets, the method still yields consistent performance gains, supporting that improvements are not due to hidden pre-training overlap.", "conclusion": "Inference-time noise injection into raw time series, prior to textual tokenization, is an effective and simple strategy for stabilizing and enhancing off-the-shelf LLMs used as zero-shot time series forecasters. It enables the models to focus on underlying temporal structure instead of superficial numerical artifacts, works without any fine-tuning or extra modules, and remains beneficial even on datasets outside the LLMs\u2019 pre-training scope, advancing the practical use of frozen LLMs for time series forecasting."}}
{"id": "2512.20144", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20144", "abs": "https://arxiv.org/abs/2512.20144", "authors": ["Yuxin Wang", "Shicheng Fang", "Bo Wang", "Qi Luo", "Xuanjing Huang", "Yining Zheng", "Xipeng Qiu"], "title": "Multi-hop Reasoning via Early Knowledge Alignment", "comment": "16 pages", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \\href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.", "AI": {"tldr": "The paper proposes Early Knowledge Alignment (EKA), a lightweight module for iterative RAG that first aligns an LLM\u2019s reasoning with the actual retrieval corpus, leading to more precise multi-hop retrieval, fewer cascaded errors, and better performance/efficiency without extra training.", "motivation": "Iterative RAG with reinforcement learning is promising for complex, multi-hop, knowledge-intensive questions, but current systems decompose and plan queries without regard to what the retrieval corpus actually contains. This mismatch causes inefficient retrieval, long and noisy reasoning chains, and cascading errors that degrade final answer quality. The authors aim to better couple planning with the available knowledge so that LLMs explore more efficiently and focus on relevant information during multi-step reasoning.", "method": "They introduce Early Knowledge Alignment (EKA), a module inserted before the planning stage in iterative RAG pipelines. EKA first performs a context-aware retrieval step to expose the LLM to a small set of relevant documents or passages from the corpus. Using this early context, the LLM then plans its decomposition of the question and subsequent retrieval steps. The approach is designed to be training-free and usable as an inference-time strategy across different LLM sizes and RAG frameworks. The authors also analyze EKA from an entropy perspective, showing how early knowledge reduces the search space and unnecessary exploration in reinforcement-learning-based reasoning loops.", "result": "Across six standard RAG benchmarks, EKA substantially boosts retrieval precision, improves answer quality, and shortens reasoning/retrieval chains compared to baseline iterative RAG systems. It also reduces cascading errors through better early decisions and yields efficiency gains (fewer steps or lower computation). Experiments indicate that EKA generalizes across model scales, datasets, and retrieval corpora, and that it consistently outperforms existing iterative RAG configurations without requiring additional training.", "conclusion": "Aligning planning with the retrieval corpus at an early stage materially improves iterative RAG. EKA offers a simple, training-free module that can be plugged into existing RL-augmented RAG systems to obtain better performance, robustness, and efficiency. The work underscores the importance of integrating structured reasoning with corpus-aware exploration, suggesting that early, context-driven alignment is a key ingredient for scalable, multi-hop question answering with large language models."}}
{"id": "2512.20161", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20161", "abs": "https://arxiv.org/abs/2512.20161", "authors": ["Dhivya Dharshini Kannan", "Anupam Trivedi", "Dipti Srinivasan"], "title": "A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers", "comment": "2025 International Joint Conference on Neural Networks (IJCNN), Rome, Italy, 2025, https://ieeexplore.ieee.org/document/11227238", "summary": "Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.", "AI": {"tldr": "The paper develops and optimizes a BiGRU-based model to predict data center PUE from simulated operational data, compares it with a GRU baseline, and evaluates performance via MSE, MAE, and R\u00b2.", "motivation": "Data centers consume large amounts of energy and contribute significantly to global carbon emissions. As edge computing and AI drive rapid growth in storage and computing demand, improving energy efficiency becomes crucial for reducing costs and environmental impact. Power Usage Effectiveness (PUE) is a key metric of data center efficiency, and accurately predicting PUE allows operators to understand how different variables affect energy use and to optimize those variables for better sustainability and competitiveness.", "method": "The authors use EnergyPlus to simulate a Singapore-based data center and generate a dataset of 52,560 samples with 117 features. They apply Recursive Feature Elimination with Cross-Validation (RFECV) to select the most relevant subsets of features under different parameter settings. These selected features are used to search for optimal hyperparameters and train a Bidirectional Gated Recurrent Unit (BiGRU) neural network model for PUE prediction. A standard GRU model is trained as a baseline. Both models are evaluated using mean squared error (MSE), mean absolute error (MAE), and R-squared (R\u00b2) metrics.", "result": "The optimized BiGRU model, trained on RFECV-selected features and tuned hyperparameters, achieves superior predictive performance compared to the GRU baseline across evaluation metrics (MSE, MAE, and R\u00b2), indicating more accurate PUE prediction. The RFECV process successfully reduces the feature set while maintaining or improving prediction quality.", "conclusion": "A BiGRU-based approach, combined with systematic feature selection via RFECV and hyperparameter optimization, can effectively and accurately predict data center PUE from high-dimensional operational data. This improved prediction capability can help data center operators identify influential factors on energy efficiency and guide targeted operational or design changes to reduce energy consumption and environmental impact."}}
{"id": "2512.20145", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20145", "abs": "https://arxiv.org/abs/2512.20145", "authors": ["Xiang Chen", "Yixin Ou", "Quan Feng", "Lei Li", "Piji Li", "Haibo Ye", "Sheng-Jun Huang", "Shuofei Qiao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models", "comment": "IEEE/ACM Transactions on Audio, Speech and Language Processing", "summary": "The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.", "AI": {"tldr": "RetroPrompt is a retrieval-augmented prompting framework for pre-trained foundation models that reduces rote memorization and improves generalization, especially in zero-/few-shot multimodal tasks.", "motivation": "Existing prompt learning for pre-trained foundation models is purely parametric, which can lead to unstable generalization, overfitting to shallow patterns, and underuse of atypical instances when data are limited. The authors want a way to better balance memorization and generalization.", "method": "They propose RetroPrompt, which augments prompting with an external, publicly accessible knowledge base built from the training data. A retrieval mechanism is integrated into input, training, and inference so that the model can fetch relevant contextual information from this corpus instead of relying solely on its parameters. This decouples some knowledge from memorized weights and injects retrieved context into the prompt for multimodal tasks.", "result": "Across multiple NLP and computer vision datasets, RetroPrompt achieves better performance than standard prompting methods in both zero-shot and few-shot settings. Analyses of how models memorize indicate that RetroPrompt changes memorization behavior in a beneficial way.", "conclusion": "By combining prompting with retrieval over a knowledge base derived from training data, RetroPrompt reduces dependence on rote memorization and yields more robust generalization for pre-trained foundation models in multimodal learning, particularly in low-data regimes."}}
{"id": "2512.20162", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20162", "abs": "https://arxiv.org/abs/2512.20162", "authors": ["Arghavan Bazigaran", "Hansem Sohn"], "title": "Concept Generalization in Humans and Large Language Models: Insights from the Number Game", "comment": null, "summary": "We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.", "AI": {"tldr": "The paper compares how humans and large language models (LLMs) generalize concepts in a number-based inference task, finding that humans are more flexible and sample-efficient, while LLMs lean heavily on mathematical rules and need more examples.", "motivation": "To understand whether LLMs generalize concepts in ways similar to humans, especially in tasks involving inferring underlying rules or concepts from numeric examples, and to identify differences in inductive biases and inference strategies.", "method": "The authors use the classic \u201cnumber game\u201d concept inference task and analyze behavior with a Bayesian model as a normative/analytical framework. They compare human participants\u2019 responses with those of LLMs, focusing on how each infers concepts and generalizes from observed examples, and on whether they favor rule-based versus similarity-based reasoning and how many examples they need for reliable generalization.", "result": "The Bayesian model fits human behavior better than it fits LLM behavior. Humans flexibly shift between rule-based and similarity-based concepts, show strong few-shot generalization (even from a single example), and align well with the Bayesian predictions. LLMs, in contrast, show a stronger bias toward mathematical, rule-like concepts and need more examples before they generalize in a human-like way, deviating more from the Bayesian account.", "conclusion": "Human and LLM concept learning in the number game differ in fundamental ways. Humans exhibit flexible inductive biases and strong few-shot capabilities that integrate both rules and similarity, while LLMs prioritize mathematical rules and require more data to generalize. These differences suggest that current LLMs do not yet replicate key aspects of human conceptual inference and may need different architectures or training regimes to better model human-like generalization."}}
{"id": "2512.20156", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.20156", "abs": "https://arxiv.org/abs/2512.20156", "authors": ["Qian Chen", "Luyao Cheng", "Chong Deng", "Xiangang Li", "Jiaqing Liu", "Chao-Hong Tan", "Wen Wang", "Junhao Xu", "Jieping Ye", "Qinglin Zhang", "Qiquan Zhang", "Jingren Zhou"], "title": "Fun-Audio-Chat Technical Report", "comment": "21 pages, https://github.com/FunAudioLLM/Fun-Audio-Chat", "summary": "Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.", "AI": {"tldr": "The paper presents Fun-Audio-Chat, a large audio language model that resolves temporal resolution mismatch and catastrophic forgetting in joint speech-text models using dual-resolution speech representations, core-cocktail training, and multi-task DPO, achieving strong results on multiple speech and audio understanding tasks.", "motivation": "Existing joint speech-text models suffer from temporal resolution mismatches between high-rate speech tokens and low-rate text tokens, leading to diluted semantics, high computation, and loss of text LLM knowledge. There is a need for an audio language model that maintains LLM capabilities while efficiently handling rich speech signals and supporting robust spoken interactions without massive audio-text pretraining.", "method": "The authors propose Fun-Audio-Chat, built on two main innovations from DrVoice: (1) Dual-Resolution Speech Representations (DRSR), where a shared LLM operates on grouped audio tokens at a lower 5Hz rate for efficiency, while a speech-refined head produces higher resolution 25Hz tokens to preserve audio quality; and (2) Core-Cocktail Training, a two-stage fine-tuning and intermediate merging strategy that reduces catastrophic forgetting of text abilities. They further perform multi-stage post-training with Multi-Task DPO across tasks like spoken QA, audio understanding, instruction following, and voice empathy, leveraging pre-trained models rather than large-scale audio-text pretraining.", "result": "Fun-Audio-Chat in 8B and MoE 30B-A3B variants delivers competitive or superior performance on speech-to-text and speech-to-speech benchmarks, particularly ranking among the top models of similar scale on spoken QA tasks. It also shows strong results on audio understanding, speech function calling, instruction following, and voice empathy. A duplex version, Fun-Audio-Chat-Duplex, demonstrates strong performance for spoken QA and full-duplex voice interactions.", "conclusion": "Fun-Audio-Chat demonstrates that combining dual-resolution speech representations, carefully designed training to avoid catastrophic forgetting, and multi-task DPO-based post-training can yield an efficient, powerful large audio language model without extensive audio-text pretraining. The approach preserves core text LLM knowledge while adding strong audio understanding and generation capabilities, and the open-sourced 8B model and demo aim to facilitate research and practical deployment of seamless voice interaction systems."}}
{"id": "2512.20173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20173", "abs": "https://arxiv.org/abs/2512.20173", "authors": ["Ze Gong", "Pradeep Varakantham", "Akshat Kumar"], "title": "Offline Safe Policy Optimization From Heterogeneous Feedback", "comment": "Accepted at AAMAS 2026 (Extended Abstract)", "summary": "Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \\textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \\textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.", "AI": {"tldr": "The paper proposes PreSa, a framework for offline preference-based reinforcement learning that directly learns safe, reward-maximizing policies from human preferences and safety labels, avoiding explicit reward/cost modeling and constrained RL, and shows superior performance on continuous control tasks.", "motivation": "Offline preference-based RL can align policies with human preferences without online interaction, but ensuring safety is difficult, especially in long-horizon continuous control. Existing RLHF-style safe methods learn reward and cost models and then use constrained RL, which works for contextual bandits (e.g., LLMs) but suffers from model errors that accumulate over long horizons, degrading both safety and performance. There is a need for a method that enforces safety and maximizes reward without relying on imperfect learned reward/cost models and heavy constrained RL machinery.", "method": "The authors introduce a framework that operates directly on human feedback about trajectories: pairwise preferences reflecting reward-related desirability and binary labels specifying whether trajectory segments are safe or unsafe. Instead of first learning reward and cost models, they formulate a constrained optimization problem that directly seeks a policy aligned with preferences while satisfying safety constraints. They instantiate this as PreSa (Preference and Safety Alignment), which integrates a preference learning module with safety alignment and solves the resulting constrained optimization in a Lagrangian framework. The Lagrangian multipliers adjust the trade-off between maximizing preference-consistent behavior and meeting safety requirements, yielding a policy that is optimized directly from feedback data in an offline setting.", "result": "On continuous control benchmarks, using both synthetic and real human feedback, PreSa learns policies that achieve high returns while respecting safety constraints. It outperforms state-of-the-art baselines, including other offline safe RL methods and even approaches that have access to ground-truth reward and cost signals, indicating that directly optimizing from preferences and safety labels can be more robust than first learning explicit reward/cost models and then running constrained RL.", "conclusion": "Directly learning safe policies from offline preferences and safety labels, without explicit reward/cost modeling or constrained RL, is an effective strategy for long-horizon continuous control. The proposed PreSa framework successfully aligns policies with human reward preferences while enforcing safety, and empirically surpasses existing safe RL and RLHF-style baselines, suggesting a promising direction for safer, more reliable preference-based RL in complex domains."}}
{"id": "2512.20164", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20164", "abs": "https://arxiv.org/abs/2512.20164", "authors": ["Honglin Mu", "Jinghao Liu", "Kaiyang Wan", "Rui Xing", "Xiuying Chen", "Timothy Baldwin", "Wanxiang Che"], "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications", "comment": null, "summary": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.", "AI": {"tldr": "LLMs used for tasks like resume screening can be tricked by hidden adversarial instructions; the paper benchmarks this risk and proposes defenses, showing training-time defenses are more effective than prompt-only ones.", "motivation": "As LLMs are increasingly deployed for automated decision-making in domains like resume screening, they become targets for adversarial manipulation embedded directly in user-provided text. Existing defenses tend to focus on mature domains like code review, leaving other applications exposed. The paper aims to systematically measure how vulnerable resume-screening LLMs are to such attacks and to explore practical defenses that balance security with decision quality.", "method": "The authors construct a benchmark for adversarial instruction attacks in resume screening scenarios, embedding malicious instructions into resumes to see if LLMs deviate from their screening task. They categorize attack types and measure attack success rates. They then evaluate two defenses: (1) prompt-based, inference-time defenses that try to guide the LLM to ignore foreign instructions, and (2) a training-time defense called FIDS (Foreign Instruction Detection through Separation) implemented via LoRA adaptation, which trains a detector or separation mechanism to identify and filter adversarial instructions. They also test a combined defense pipeline using both methods.", "result": "On the benchmark, some adversarial attack types achieve over 80% success in manipulating the LLM\u2019s behavior. Prompt-based defenses reduce successful attacks by 10.1% but at the cost of a 12.5% increase in false rejections of benign inputs. FIDS with LoRA achieves a larger 15.4% reduction in attack success, with a smaller 10.4% increase in false rejections. When combined, the two defenses provide a 26.3% reduction in attack success, indicating complementary benefits.", "conclusion": "LLMs used in resume screening are highly vulnerable to adversarial instructions hidden in input documents. Simple prompt-based defenses provide only modest protection and introduce notable utility loss. A training-time defense via FIDS (with LoRA) more effectively reduces attack success while maintaining better utility, and combining training-time and prompt-based defenses yields the strongest overall protection. The study underscores that robust deployment of LLMs in screening tasks requires specialized, training-time security measures rather than relying solely on inference-time prompt engineering."}}
{"id": "2512.20206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20206", "abs": "https://arxiv.org/abs/2512.20206", "authors": ["Zhe Sun", "Kunlun Wu", "Chuanjian Fu", "Zeming Song", "Langyong Shi", "Zihe Xue", "Bohan Jing", "Ying Yang", "Xiaomeng Gao", "Aijia Li", "Tianyu Guo", "Huiying Li", "Xueyuan Yang", "Rongkai Liu", "Xinyi He", "Yuxi Wang", "Yue Li", "Mingyuan Liu", "Yujie Lu", "Hongzhao Xie", "Shiyun Zhao", "Bo Dai", "Wei Wang", "Tao Yuan", "Song-Chun Zhu", "Yujia Peng", "Zhenliang Zhang"], "title": "TongSIM: A General Platform for Simulating Intelligent Machines", "comment": null, "summary": "As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.", "AI": {"tldr": "TongSIM is a high-fidelity, general-purpose simulation platform for training and evaluating embodied AI agents across a wide spectrum of tasks, from navigation to complex social and collaborative activities.", "motivation": "Existing embodied AI simulation platforms are typically narrow, designed for specific tasks or limited environments, which restricts progress toward general embodied intelligence and comprehensive evaluation of agent capabilities. There is a need for a unified, versatile, high-fidelity environment that can support diverse tasks, scales, and interaction types, including social and human-AI collaboration scenarios.", "method": "The authors design and build TongSIM, a high-fidelity, general-purpose simulation environment that provides (1) over 100 diverse multi-room indoor scenarios and an open-ended outdoor town; (2) mechanisms for customized scenes and task-adaptive simulation fidelity; (3) support for diverse agent types and rich, dynamic environmental interactions; and (4) a comprehensive evaluation framework and benchmarks targeting perception, cognition, decision-making, cooperation, and spatial/social reasoning in embodied agents.", "result": "TongSIM successfully offers a broad set of realistic indoor and outdoor scenarios, configurable environments, dynamic simulations, and multiple agent types, along with an integrated benchmark suite that enables precise assessment of a wide range of embodied agent skills. It demonstrates that a single, unified platform can practically cover tasks from low-level navigation to high-level social and collaborative activities.", "conclusion": "TongSIM functions as a flexible, scalable, and unified platform that can accelerate research on embodied AI by providing a general-purpose training and evaluation environment. It is positioned as a foundational resource for advancing toward general embodied intelligence through standardized yet versatile simulations and benchmarks."}}
{"id": "2512.20182", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20182", "abs": "https://arxiv.org/abs/2512.20182", "authors": ["Shuzheng Si", "Qingyi Wang", "Haozhe Zhao", "Yuzhuo Bai", "Guanqiao Chen", "Kangyang Luo", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination", "comment": null, "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.", "AI": {"tldr": "FaithLens is a cost-efficient model that detects faithfulness hallucinations in LLM outputs and explains its decisions, outperforming larger models like GPT-4.1 and o3 on many tasks.", "motivation": "LLMs often generate hallucinations that are unfaithful to source information, which is risky in applications such as retrieval-augmented generation and summarization. There is a need for an accurate, efficient detector that can not only flag such hallucinations but also explain why, to improve trust and usability.", "method": "The authors first use powerful LLMs to synthesize labeled training data with natural language explanations, then apply a strict filtering pipeline to ensure correct labels, high-quality explanations, and diverse coverage. They fine-tune a base model on this curated dataset and further improve it with rule-based reinforcement learning, where the reward function jointly incentivizes correct hallucination detection and good explanations. The resulting model outputs both a binary faithfulness label and an explanation.", "result": "On 12 varied benchmarks, the 8B-parameter FaithLens model surpasses advanced proprietary models like GPT-4.1 and o3 in detecting faithfulness hallucinations, while remaining cost-efficient. It also generates strong explanatory rationales accompanying its predictions.", "conclusion": "FaithLens provides an effective balance of accuracy, interpretability, and efficiency for hallucination detection in LLM outputs. By combining synthetic data with explanations and rule-based RL optimization, it becomes a practical tool for real-world LLM applications that require trustworthy, well-justified judgments about output faithfulness."}}
{"id": "2512.20237", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20237", "abs": "https://arxiv.org/abs/2512.20237", "authors": ["Xingbo Du", "Loka Li", "Duzhen Zhang", "Le Song"], "title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents", "comment": "16 pages, 6 figures", "summary": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.", "AI": {"tldr": "The paper introduces MemR^3, a closed-loop memory retrieval controller for LLM agents that autonomously decides when and how to retrieve or reflect on memories, improving RAG performance and transparency over standard retrieve-then-answer pipelines.", "motivation": "Existing memory systems for LLM agents focus on compressing and storing past experiences but pay relatively little attention to explicit, controllable, and transparent retrieval. The authors observe that common pipelines are largely open-loop: they retrieve once and then answer, without adaptively deciding whether more evidence is needed or tracking how evidence supports the final answer. This gap motivates a system that can autonomously control the retrieval process, make its evidence use explicit, and plug into existing memory stores to enhance their effectiveness.", "method": "The authors design MemR^3 as an autonomous controller composed of two main components: (1) a router that dynamically chooses among three actions\u2014retrieve more information from memory, reflect (i.e., reason or synthesize over currently available evidence), or answer the user query\u2014based on optimizing answer quality; and (2) a global evidence-gap tracker that monitors what evidence has been collected, identifies missing pieces needed to answer the question, and makes the reasoning and evidence-collection process transparent. This shifts from a simple retrieve-then-answer flow to a closed-loop control scheme where the agent iteratively decides whether to gather more evidence or proceed to answering. MemR^3 is implemented as a plug-and-play controller on top of existing memory backends such as RAG and Zep, using GPT-4.1-mini as the LLM core, and is evaluated on the LoCoMo benchmark.", "result": "On the LoCoMo benchmark, MemR^3 outperforms strong baseline systems when judged by an LLM-as-a-Judge metric. Specifically, when coupled with existing retrievers, it yields consistent gains across four evaluation categories. Using GPT-4.1-mini as the backend, MemR^3 improves overall performance by +7.29% over standard RAG and +1.94% over Zep, demonstrating that adding a closed-loop retrieval controller can significantly enhance existing memory systems without altering their underlying storage mechanisms.", "conclusion": "MemR^3 demonstrates that treating memory retrieval as an autonomous, closed-loop control problem\u2014rather than a one-shot retrieval step\u2014can substantially improve LLM agent performance and transparency. Its router and evidence-gap tracker enable adaptive decisions about when to retrieve, reflect, or answer, leading to better use of existing memory stores. The plug-and-play nature of the controller means it can be integrated with current RAG and memory backends (e.g., Zep) to boost answer quality with minimal changes to infrastructure, suggesting a promising direction for future memory-augmented LLM agent design."}}
{"id": "2512.20204", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20204", "abs": "https://arxiv.org/abs/2512.20204", "authors": ["Marko \u010cechovi\u010d", "Nat\u00e1lia Komorn\u00edkov\u00e1", "Dominik Mach\u00e1\u010dek", "Ond\u0159ej Bojar"], "title": "Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings", "comment": "12 pages, 2 figures, 6 tables, published as a conference paper in Text, Speech, and Dialogue 28th International Conference, TSD 2025, Erlangen, Germany, August 25-28, 2025, Proceedings, Part II. This version published here on arXiv.org is before review comments and seedings of the TSD conference staff", "summary": "Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.\n  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.", "AI": {"tldr": "The paper introduces a new corpus of cross-lingual dialogues in meetings without a shared language and studies automatic detection of misunderstandings using LLMs.", "motivation": "To realistically evaluate and improve automatic speech translation and cross-lingual meeting support, there is a lack of versatile corpora capturing real interactions between speakers with no common language, as well as a need to understand and detect misunderstandings that arise in such settings.", "method": "The authors collect about 5 hours of cross-lingual dialogue where participants without a shared language communicate via automatic simultaneous speech translation. They provide ASR outputs, gold transcripts in 12 source languages, and both automatic and manually corrected English translations, plus written summaries (minutes). They then manually annotate misunderstandings in the dialogues and evaluate large language models, in particular Google\u2019s Gemini, on automatically detecting misunderstanding spans in the text.", "result": "The resulting corpus covers 12 languages with aligned speech, transcriptions, translations, and meeting summaries. For misunderstanding detection, Gemini achieves 77% recall and 47% precision at identifying textual spans corresponding to misunderstandings, demonstrating partial but imperfect capability.", "conclusion": "The corpus is a valuable resource for research on speech translation, cross-lingual dialogue, and cross-lingual summarization, especially in settings with no shared language. Automatic misunderstanding detection is feasible but remains challenging, as evidenced by Gemini\u2019s moderate recall and low precision; further research is needed to improve models and methods for reliably identifying misunderstandings in cross-lingual meetings."}}
{"id": "2512.20292", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.20292", "abs": "https://arxiv.org/abs/2512.20292", "authors": ["Wenzheng Zeng", "Mingyu Ouyang", "Langyuan Cui", "Hwee Tou Ng"], "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers", "comment": "AAAI 2026 (with appendix)", "summary": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.", "AI": {"tldr": "The paper proposes SlideTailor, an agentic framework that generates presentation slides from research papers customized to a user\u2019s preferred content structure and visual style, using only an example paper\u2013slides pair and a visual template, plus a chain-of-speech mechanism for aligning slides with oral narration, evaluated on a new benchmark with interpretable metrics.", "motivation": "Existing automatic slide generation methods are under-specified: they ignore that different users have distinct, nuanced preferences about slide content density, organization, and visual style. Requiring users to specify these preferences in detailed text is impractical. There is a need for a system that can infer and generalize user preferences from natural artifacts they already have (prior slides and templates), and that also considers how slides will be verbally presented, to yield higher-quality and more usable slide decks.", "method": "The authors define a new task: paper-to-slides generation conditioned on user preferences. They design SlideTailor, a human behavior\u2013inspired agentic framework that incrementally produces editable slides. The system takes as input (1) an example paper\u2013slides pair from the user and (2) a visual template, both of which implicitly encode the user\u2019s content and style preferences. SlideTailor distills these implicit, unlabeled preferences and generalizes them to guide slide creation for new papers. Additionally, they introduce a chain-of-speech mechanism that plans oral narration and uses it to align and refine slide content. They also build a benchmark dataset capturing diverse user preferences and define interpretable evaluation metrics.", "result": "On the constructed benchmark, SlideTailor is empirically shown to generate slides that better match user preferences and exhibit higher quality than baselines. The framework can effectively infer and generalize user-specific content and style preferences from the provided example pair and template, despite these signals being implicit and unlabeled. The chain-of-speech mechanism further improves alignment between slides and intended narration and enables the automatic generation of video presentations. Quantitative metrics and qualitative analyses both support these claims.", "conclusion": "Conditioning paper-to-slides generation on user-specified preferences via natural artifacts is both feasible and beneficial. SlideTailor\u2019s agentic, progressive generation and its chain-of-speech alignment lead to more user-aligned and presentation-ready slides than under-specified, one-size-fits-all approaches. The newly introduced benchmark and metrics provide a basis for further research on personalized slide generation and narration-aware multimodal presentation tools."}}
{"id": "2512.20276", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20276", "abs": "https://arxiv.org/abs/2512.20276", "authors": ["Yuntao Dai", "Hang Gu", "Teng Wang", "Qianyu Cheng", "Yifei Zheng", "Zhiyong Qiu", "Lei Gong", "Wenqi Lou", "Xuehai Zhou"], "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.", "AI": {"tldr": "ActionFlow is a system-level inference framework that accelerates vision-language-action models on edge devices by reorganizing and batching their computation, boosting FPS without retraining.", "motivation": "Vision-Language-Action models enable powerful robotic control but are too slow on resource-constrained edge hardware, running at only 3-5 Hz versus the 20-30 Hz needed for smooth real-time control. Existing acceleration methods often require retraining or reduce accuracy. The paper aims to achieve real-time VLA inference on edge devices without retraining or sacrificing performance.", "method": "The paper proposes ActionFlow, an inference framework that restructures VLA execution as a macro-pipeline of micro-requests via a Cross-Request Pipelining scheduler. This scheduler batches memory-bound decoding phases with compute-bound prefill phases across time steps to better utilize hardware. To support this, the authors introduce a Cross-Request State Packed Forward operator and a Unified KV Ring Buffer, which consolidate fragmented memory operations into dense, efficient computations tailored for edge platforms.", "result": "On the OpenVLA-7B model, ActionFlow achieves a 2.55x increase in frames per second on edge hardware, reaching real-time dynamic manipulation performance, and does so without retraining the model or degrading its accuracy.", "conclusion": "By rethinking VLA inference as a cross-request pipeline and optimizing memory access via packed operators and KV ring buffers, ActionFlow substantially reduces inference latency on edge devices. This enables real-time robotic manipulation with large VLA models without modifying or retraining them, demonstrating that system-level scheduling and memory optimizations can close the performance gap for on-device robotics."}}
{"id": "2512.20293", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20293", "abs": "https://arxiv.org/abs/2512.20293", "authors": ["Jaykumar Kasundra", "Anjaneya Praharaj", "Sourabh Surana", "Lakshmi Sirisha Chodisetty", "Sourav Sharma", "Abhigya Verma", "Abhishek Bhardwaj", "Debasish Kanhar", "Aakash Bhagat", "Khalil Slimi", "Seganrasan Subramanian", "Sathwik Tejaswi Madhusudhan", "Ranga Prasad Chenna", "Srinivas Sunkara"], "title": "AprielGuard", "comment": null, "summary": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.", "AI": {"tldr": "AprielGuard is an 8B-parameter safety model that unifies content safety and adversarial defense for LLMs, trained with diverse data and reasoning traces, and outperforms existing open-source guardrails, especially in complex, multi-step settings.", "motivation": "As LLMs are deployed in interactive and agentic scenarios, they face both safety risks (toxicity, bias) and adversarial attacks (prompt injections, jailbreaks). Existing moderation tools usually handle these as separate problems, leading to fragmented, less robust defenses that do not generalize well to real-world, complex interactions. The authors want a unified, more reliable safeguard that works across different types of risks and settings.", "method": "The authors build AprielGuard, an 8B-parameter safeguard model trained under a unified taxonomy that jointly covers safety risks and adversarial threats. They use a diverse mixture of open and synthetic datasets that include standalone prompts, multi-turn conversations, and agentic (tool-using/workflow) scenarios. Training data are augmented with structured reasoning traces so the model can explain or structure its safety judgments, improving interpretability. They then evaluate AprielGuard on multiple public and proprietary benchmarks against existing open-source guardrail models.", "result": "AprielGuard demonstrates strong performance in detecting both harmful content and adversarial manipulations. It outperforms existing open-source guardrail models such as Llama-Guard and Granite Guardian, with a notable advantage in multi-step and reasoning-heavy scenarios where context and complex attack patterns matter more.", "conclusion": "A single, unified safeguard model that jointly addresses safety risks and adversarial attacks can be more robust and generalizable than tools that treat these problems separately. AprielGuard, with its 8B parameters, diverse training mix, and reasoning-augmented supervision, advances the state of the art in open-source LLM guardrails. By releasing the model, the authors aim to facilitate transparent, reproducible research on reliable LLM safety defenses across conversational and agentic use cases."}}
{"id": "2512.20278", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20278", "abs": "https://arxiv.org/abs/2512.20278", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation", "comment": "7 pages", "summary": "While CodeMem establishes executable code as the optimal representation for agentic procedural memory, the mechanism for autonomously synthesizing this memory from a blank slate remains underexplored. This paper operationalizes the transition of Large Language Models from passive tool-users to active workflow architects. Through a high-fidelity case study of a cross-service orchestration task involving Outlook and OneDrive, we identify and address four structural bottlenecks in automated skill generation: the Discovery Gap involving navigation of large tool registries, the Verification Gap regarding grounding tool response structures, the Decomposition Gap which replaces inefficient search with Linear State Anchoring, and the Scaling Gap focused on concurrency and persistence. We demonstrate that by enforcing a scientific methodology of hypothesize, probe, and code, agents can autonomously write robust, production-grade code skills.", "AI": {"tldr": "The paper presents a method that lets LLM agents autonomously discover, verify, decompose, and scale tool-using behaviors, turning them into robust code-based skills without human-crafted workflows.", "motivation": "CodeMem shows that executable code is a strong format for procedural memory in agents, but it does not explain how an agent can start from scratch and automatically build up this library of code skills. As agents are expected to move from simple API callers to autonomous workflow designers, we need a systematic way for them to explore large tool ecosystems, reliably interpret tool outputs, break complex tasks into manageable subskills, and handle real-world requirements like concurrency and persistence.", "method": "The authors perform a detailed case study on a realistic cross-service orchestration task involving Outlook and OneDrive. From this, they characterize four structural bottlenecks in automated skill generation: (1) a Discovery Gap in searching and selecting from large tool registries, (2) a Verification Gap in grounding and validating tool response schemas, (3) a Decomposition Gap, which they address via a strategy called Linear State Anchoring instead of naive search over plans, and (4) a Scaling Gap covering issues of concurrency and persistence across workflows. They propose a disciplined agent loop modeled on scientific inquiry\u2014\"hypothesize, probe, and code\"\u2014where the LLM forms hypotheses about tools and workflows, probes the environment via tool calls, then consolidates successful patterns into executable, reusable code skills.", "result": "In the studied orchestration task, the proposed methodology enables agents to autonomously discover the right tools, understand their interfaces, decompose the end-to-end workflow, and generate production-grade code that coordinates Outlook and OneDrive. The resulting skills are robust, reusable, and do not rely on manually specified workflows, demonstrating that the agent can effectively bootstrap its own procedural memory library from a blank slate.", "conclusion": "By framing agent behavior as a scientific process of hypothesis, experimentation, and codification, the paper shows that LLM agents can autonomously synthesize high-quality code skills, overcoming key bottlenecks in discovery, verification, decomposition, and scaling. This advances agents from passive tool users toward active workflow architects and provides a path for growing an agent\u2019s procedural memory as executable code without extensive human-engineered scripts."}}
{"id": "2512.20298", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20298", "abs": "https://arxiv.org/abs/2512.20298", "authors": ["Karolina Dro\u017cd\u017c", "Kacper Dudzic", "Anna Sterna", "Marcin Moskalewicz"], "title": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives", "comment": null, "summary": "Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term \"narcissism.\" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.", "AI": {"tldr": "The paper compares state-of-the-art LLMs with human mental health professionals in diagnosing BPD and NPD from Polish first-person autobiographical accounts, finding that LLMs are more accurate overall but heavily underdiagnose narcissistic personality disorder and show distinct explanatory styles and biases.", "motivation": "As LLMs are increasingly used for psychiatric self-assessment and support, it is crucial to know whether they can reliably interpret rich, qualitative patient narratives and how their performance and reasoning compare to trained clinicians, particularly for complex personality disorders like BPD and NPD.", "method": "The authors compiled Polish first-person autobiographical accounts from individuals with Borderline Personality Disorder and Narcissistic Personality Disorder and asked both state-of-the-art Gemini Pro LLMs and human mental health professionals to diagnose each case. They then quantitatively compared diagnostic accuracy and F1 scores for each disorder and qualitatively analyzed the styles and focuses of the justifications provided by models versus clinicians.", "result": "Gemini Pro models outperformed human professionals in overall diagnostic accuracy by about 22 percentage points (65.48% vs. 43.57%). Both LLMs and clinicians identified BPD well (F1 scores 83.4 and 80.0, respectively), but models dramatically underdiagnosed NPD (F1 = 6.7 versus 50.0 for humans) and appeared reluctant to apply the term \"narcissism.\" Qualitative analysis showed models gave confident, elaborate, category-focused explanations, whereas human experts were briefer, more cautious, and centered on patients\u2019 sense of self and temporal experience.", "conclusion": "LLMs can be highly capable at interpreting complex first-person clinical narratives and, in aggregate, may even exceed clinicians\u2019 accuracy for some tasks, but they exhibit serious reliability and bias problems\u2014notably in diagnosing NPD\u2014and adopt explanatory styles that differ markedly from human experts, underscoring the need for caution and further refinement before clinical use."}}
{"id": "2512.20308", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.20308", "abs": "https://arxiv.org/abs/2512.20308", "authors": ["Maxime Poli", "Mahi Luthra", "Youssef Benchekroun", "Yosuke Higuchi", "Martin Gleize", "Jiayi Shen", "Robin Algayres", "Yu-An Chung", "Mido Assran", "Juan Pino", "Emmanuel Dupoux"], "title": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision", "comment": "30 pages, 16 figures", "summary": "The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.", "AI": {"tldr": "SpidR is a self-supervised speech representation model that learns high-quality semantic and phonetic units directly from raw speech, enabling efficient textless spoken language modeling while being faster to pretrain than prior models.", "motivation": "Recent progress in language modeling and speech representation learning suggests language could be modeled directly from speech, bypassing text. To make this feasible, one needs speech representations that capture phonetic and semantic information well, are efficient to train, and whose quality can be reliably assessed. Existing models like wav2vec 2.0, HuBERT, and WavLM are strong but relatively expensive to pretrain and leave open questions about how speech unit quality relates to downstream language modeling performance.", "method": "The authors propose SpidR, a self-supervised model trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. A teacher\u2013student framework is used: intermediate layers of the student predict cluster assignments derived from the teacher\u2019s intermediate layers. This objective stabilizes online clustering and improves the learned codebooks. They then train spoken language models on the learned units and systematically compare layers and models, measuring speech unit quality with ABX and PNMI and correlating these with language modeling benchmarks (sWUGGY, sBLIMP, tSC). They also implement an efficient training codebase to reduce pretraining time.", "result": "SpidR achieves better performance than wav2vec 2.0, HuBERT, WavLM, and DinoSR on spoken language modeling benchmarks such as sWUGGY, sBLIMP, and tSC, indicating that its learned units are more suitable for textless language modeling. The modified objective with self-distillation and online clustering yields higher-quality, more stable codebooks. The empirical study shows a strong correlation between standard unit-quality metrics (ABX, PNMI) and downstream language modeling performance, supporting their use as proxies. In terms of efficiency, SpidR cuts pretraining time from about a week (HuBERT) to about one day on 16 GPUs.", "conclusion": "SpidR provides an effective and efficient way to learn speech representations that are well suited for textless spoken language modeling, outperforming several strong baselines while being substantially faster to pretrain. The demonstrated correlation between unit-quality metrics and language modeling performance offers practical guidance for model development and evaluation. By open-sourcing code and checkpoints, the work facilitates further research on learning language directly from speech without reliance on text transcriptions."}}
{"id": "2512.20344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20344", "abs": "https://arxiv.org/abs/2512.20344", "authors": ["Yaowei Bai", "Ruiheng Zhang", "Yu Lei", "Xuhua Duan", "Jingfeng Yao", "Shuguang Ju", "Chaoyang Wang", "Wei Yao", "Yiwan Guo", "Guilin Zhang", "Chao Wan", "Qian Yuan", "Lei Chen", "Wenjuan Tang", "Biqiang Zhu", "Xinggang Wang", "Tao Sun", "Wei Zhou", "Dacheng Tao", "Yongchao Xu", "Chuansheng Zheng", "Huangxuan Zhao", "Bo Du"], "title": "A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.19493", "summary": "A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.", "AI": {"tldr": "The paper presents Janus-Pro-CXR, a 1B-parameter chest X-ray interpretation system that outperforms larger models and improves radiology workflow in a prospective multicenter clinical trial.", "motivation": "There is a global shortage of radiologists, and chest X-rays generate heavy workloads, especially in primary care. While multimodal large language models could help, most have only been evaluated with automated or retrospective tests, not in rigorous prospective clinical settings. The authors aim to create and clinically validate an efficient, specialized AI system that can reliably interpret chest X-rays and assist radiologists in real-world practice.", "method": "The authors develop Janus-Pro-CXR, a chest X-ray interpretation system built on the DeepSeek Janus-Pro multimodal model with 1B parameters, optimized specifically for radiography. They benchmark its report generation performance against state-of-the-art models, including much larger models such as ChatGPT 4o, using automated metrics and retrospective expert evaluations. They then run a multicenter prospective clinical trial (NCT07117266) where radiologists use the system in real-time reporting to assess its impact on report quality, diagnostic detection of six key findings, interpretation time, and user preference.", "result": "Janus-Pro-CXR outperforms current state-of-the-art X-ray report generation models, achieving higher automated metric scores and more accurate reports than both the base Janus-Pro and ChatGPT 4o despite its smaller size. It reliably detects six clinically critical radiographic findings. In the prospective multicenter deployment, AI assistance improved expert report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by experts in 54.3% of cases.", "conclusion": "A lightweight, domain-optimized multimodal model can surpass larger general-purpose systems for chest X-ray interpretation and deliver tangible clinical benefits. Janus-Pro-CXR enhances diagnostic reliability and efficiency, particularly valuable for resource-limited settings. By open-sourcing the architecture and implementation framework, the work aims to accelerate safe and effective clinical translation of AI-assisted radiology tools."}}
{"id": "2512.20324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20324", "abs": "https://arxiv.org/abs/2512.20324", "authors": ["Nurul Labib Sayeedi", "Md. Faiyaz Abdullah Sayeedi", "Khushnur Binte Jahangir", "Swakkhar Shatabda", "Sarah Masud Preum"], "title": "Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles", "comment": null, "summary": "Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.", "AI": {"tldr": "The paper introduces BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles (4,976 riddle-task instances) to evaluate how well LLMs handle low-resource, figurative, culturally grounded reasoning, showing that current models perform far below humans.", "motivation": "While LLMs perform well on many NLP benchmarks, their reasoning abilities for figurative, culturally grounded, and low-resource languages like Bangla are underexplored. Traditional riddles, which require nuanced cultural and figurative understanding, provide a natural testbed. The authors aim to systematically assess and quantify LLM capabilities and limitations in this domain, and to fill the resource gap for Bangla figurative reasoning benchmarks.", "method": "The authors construct BanglaRiddleEval from 1,244 traditional Bangla riddles and instantiate them into four distinct tasks, yielding 4,976 riddle-task artifacts. Using an LLM-based pipeline, they automatically generate Chain-of-Thought (CoT) explanations, create semantically coherent distractor options for MCQ-style tasks, and annotate each instance with fine-grained ambiguity labels. They then evaluate a range of open-source and closed-source LLMs under various prompting strategies across generative QA, multiple-choice answering, and ambiguity resolution tasks, also assessing explanation quality.", "result": "Across tasks, models show only moderate semantic similarity to gold answers in generative QA but low actual correctness. On MCQ tasks, the best models reach about 56% accuracy, well below an 83% human baseline. For ambiguity resolution, models\u2019 performance varies widely between roughly 26% and 68%, and strong, high-quality Chain-of-Thought explanations only appear in the most capable models. These results quantitatively demonstrate that even advanced LLMs struggle with Bangla riddle reasoning and lag substantially behind humans.", "conclusion": "BanglaRiddleEval is a difficult new benchmark that exposes significant gaps in current LLMs\u2019 ability to handle low-resource, culturally grounded figurative reasoning in Bangla. While models can pick up some relevant cues, their reasoning, accuracy, and ambiguity handling remain far from human-level. The benchmark, along with its publicly released data, code, and evaluation scripts, provides a valuable resource for driving future research on improving LLM performance in figurative, low-resource, and culturally nuanced settings."}}
{"id": "2512.20469", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20469", "abs": "https://arxiv.org/abs/2512.20469", "authors": ["Linfeng Zhang", "Siheng Chen", "Yuzhu Cai", "Jingyi Chai", "Junhan Chang", "Kun Chen", "Zhi X. Chen", "Zhaohan Ding", "Yuwen Du", "Yuanpeng Gao", "Yuan Gao", "Jing Gao", "Zhifeng Gao", "Qiangqiang Gu", "Yanhui Hong", "Yuan Huang", "Xi Fang", "Xiaohong Ji", "Guolin Ke", "Zixing Lei", "Xinyu Li", "Yongge Li", "Ruoxue Liao", "Hang Lin", "Xiaolu Lin", "Yuxiang Liu", "Xinzijian Liu", "Zexi Liu", "Jintan Lu", "Tingjia Miao", "Haohui Que", "Weijie Sun", "Yanfeng Wang", "Bingyang Wu", "Tianju Xue", "Rui Ye", "Jinzhe Zeng", "Duo Zhang", "Jiahui Zhang", "Linfeng Zhang", "Tianhan Zhang", "Wenchang Zhang", "Yuzhi Zhang", "Zezhong Zhang", "Hang Zheng", "Hui Zhou", "Tong Zhu", "Xinyu Zhu", "Qingguo Zhou", "Weinan E"], "title": "Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale", "comment": null, "summary": "AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \\emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.\n  However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.\n  We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \\emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.\n  We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.", "AI": {"tldr": "The paper proposes an infrastructure and orchestration stack (Bohrium+SciMaster) to enable scalable, traceable AI-agent-driven scientific workflows, demonstrating large speedups and rich execution traces from real workloads.", "motivation": "AI agents can now run complex, multi-step scientific workflows by combining reasoning, tools, and verification, but current scientific practice still centers on isolated AI-assisted steps. As AI accelerates scientific output, existing peer-review and publication pipelines are strained, increasing the need for traceability, reproducibility, and credible evaluation. However, current agentic science efforts are hard to scale because workflows are opaque, tools and labs are not agent-ready, execution is hard to trace and govern, and existing AI Scientist systems are bespoke and non-reusable. The paper is motivated by the need for a systematic, reusable infrastructure that makes it easy to build, run, and audit agentic scientific workflows at scale.", "method": "The authors design and implement Bohrium+SciMaster as a layered stack for agentic science. Bohrium is a managed hub for AI-for-Science assets that turns heterogeneous scientific data, software, compute, and laboratory systems into standardized, agent-ready capabilities with traceable interfaces and execution records. SciMaster sits on top as an orchestrator that composes these capabilities into long-horizon scientific workflows that AI agents can execute. Between them, the authors define a \"scientific intelligence substrate\" that organizes models, knowledge, and components into reusable, executable building blocks to support workflow reasoning, composition, and auditing. They then instantiate eleven master agents using this stack and deploy them on real-world scientific workflows to evaluate performance and traceability.", "result": "Using eleven representative master agents on real workflows, the system achieves orders-of-magnitude reductions in end-to-end scientific cycle time, indicating substantial efficiency gains over traditional or less-integrated approaches. The infrastructure also produces execution-grounded signals at multi-million scale, meaning it records large-scale traces and metrics from real workloads, which can be used for monitoring, evaluation, and further improvement of the agents and workflows.", "conclusion": "The paper concludes that scalable agentic science requires not just better models but a full infrastructure-and-ecosystem approach. Bohrium+SciMaster provides such an approach by turning diverse scientific resources into agent-ready, traceable capabilities and orchestrating them into complex workflows via a scientific intelligence substrate. This stack enables composability, auditability, and systematic improvement from real usage, and the demonstrated master agents show that such an approach can dramatically accelerate scientific workflows while generating rich, trustworthy execution traces."}}
{"id": "2512.20404", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20404", "abs": "https://arxiv.org/abs/2512.20404", "authors": ["Junyi Liu", "Stanley Kok"], "title": "Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining", "comment": "WITS 2025 (Workshop on Information Technologies and Systems 2025)", "summary": "With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.", "AI": {"tldr": "The paper proposes a sentiment-aware summarization framework for noisy, short, user-generated texts (e.g., social media), extending both extractive and abstractive models by integrating sentiment signals to produce concise, emotionally informative summaries for IS applications.", "motivation": "Unstructured, noisy, emotion-rich data from social media and similar platforms are increasingly important for Information Systems tasks like brand monitoring and market analysis. Existing summarization methods, mainly designed for well-structured news, perform poorly on such informal content and rarely incorporate sentiment, despite its central role in many IS applications. There is a need for summarization methods that can both handle noisy short texts and explicitly model sentiment information.", "method": "The authors design a sentiment-aware framework that augments two mainstream summarization paradigms: extractive TextRank and abstractive UniLM. For extractive summarization, they embed sentiment signals into the ranking mechanism of TextRank so that sentences or posts are ranked considering both topical relevance and emotional cues. For abstractive summarization, they inject sentiment information into the UniLM generation process so the model can produce summaries that reflect both semantic content and affective tone. The framework thus jointly considers sentiment and content structure in both selection and generation stages.", "result": "The framework yields summaries that better capture emotional nuances and thematic relevance from noisy, short user-generated texts, compared to standard summarization systems optimized for structured news. The resulting summaries are more sentiment-enriched and concise, improving the usefulness of summaries for downstream IS tasks such as brand monitoring and market analysis. (Exact quantitative metrics are not given in the abstract but improvements are claimed qualitatively.)", "conclusion": "Incorporating sentiment signals into both extractive and abstractive summarization processes leads to better summaries of social media-like user-generated text for Information Systems applications. The sentiment-aware framework can support more timely and informed interventions and strategic decisions in dynamic online environments, showing that sentiment modeling is a valuable component of text summarization for noisy, emotion-rich data."}}
{"id": "2512.20520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20520", "abs": "https://arxiv.org/abs/2512.20520", "authors": ["Chehak Malhotra", "Mehak Gopal", "Akshaya Devadiga", "Pradeep Singh", "Ridam Pal", "Ritwik Kashyap", "Tavpritesh Sethi"], "title": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units", "comment": null, "summary": "With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.", "AI": {"tldr": "The paper evaluates whether large language models (LLMs) offer real benefits over smaller language models (SLMs) for predicting shock in ICU patients from clinical notes, and finds that LLMs are not clearly superior.", "motivation": "In clinical NLP, LLMs have revolutionized many text-focused tasks (e.g., classification, NER, summarization), but their usefulness for forward-looking clinical prediction (e.g., predicting shock before it occurs) is underexplored. Early and accurate shock prediction could enable timely interventions and better outcomes in critical care, so it is important to know whether expensive LLMs meaningfully outperform smaller, established models for this kind of prediction.", "method": "The authors used ICU text data from 17,294 ICU stays in MIMIC III and selected stays with length of stay > 24 hours, then labeled patients based on shock index (SI) > 0.7 as abnormal vs. normal SI, resulting in 87 abnormal and 355 normal cases. They compared several LLMs (GatorTron-Base, Llama 8B, Mistral 7B) with SLMs and classical models (BioBERT, DocBERT, BioClinicalBERT, Word2Vec, Doc2Vec). Models were fine-tuned for the binary prediction task (normal vs abnormal SI), with both cross-entropy and focal losses to cope with class imbalance. They evaluated performance using metrics including weighted recall and others.", "result": "GatorTron-Base achieved the best weighted recall (80.5%) among the evaluated models. However, overall performance across metrics showed that LLMs and SLMs performed similarly, with no consistent, clear advantage for LLMs in predicting shock. Class imbalance mitigation (focal and cross-entropy loss) was used, but did not lead to a dominant win for LLM-based approaches.", "conclusion": "LLMs, despite excelling in many NLP tasks, do not automatically provide superior performance for clinical event prediction tasks such as early shock prediction from ICU text. Their added complexity and cost may not be justified for this specific predictive setting. Future LLM development for healthcare should focus explicitly on modeling and predicting clinical trajectories and outcomes, rather than primarily optimizing for relatively simpler text processing tasks such as NER or phenotyping, if the goal is to impact real-world clinical care."}}
{"id": "2512.20491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20491", "abs": "https://arxiv.org/abs/2512.20491", "authors": ["Chen Hu", "Haikuo Du", "Heng Wang", "Lin Lin", "Mingrui Chen", "Peng Liu", "Ruihang Miao", "Tianchi Yue", "Wang You", "Wei Ji", "Wei Yuan", "Wenjin Deng", "Xiaojian Yuan", "Xiaoyun Zhang", "Xiangyu Liu", "Xikai Liu", "Yanming Xu", "Yicheng Cao", "Yifei Zhang", "Yongyao Wang", "Yubo Shu", "Yurong Zhang", "Yuxiang Zhang", "Zheng Gong", "Zhichao Chang", "Binyan Li", "Dan Ma", "Furong Jia", "Hongyuan Wang", "Jiayu Liu", "Jing Bai", "Junlan Liu", "Manjiao Liu", "Na Wang", "Qiuping Wu", "Qinxin Du", "Shiwei Li", "Wen Sun", "Yifeng Gong", "Yonglin Chen", "Yuling Zhao", "Yuxuan Lin", "Ziqi Ren", "Zixuan Wang", "Aihu Zhang", "Brian Li", "Buyun Ma", "Kang An", "Li Xie", "Mingliang Li", "Pan Li", "Shidong Yang", "Xi Chen", "Xiaojia Liu", "Yuchu Luo", "Yuan Song", "YuanHao Ding", "Yuanwei Liang", "Zexi Li", "Zhaoning Zhang", "Zixin Zhang", "Binxing Jiao", "Daxin Jiang", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Yibo Zhu"], "title": "Step-DeepResearch Technical Report", "comment": null, "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.", "AI": {"tldr": "The paper introduces Step-DeepResearch, a medium-sized LLM-based research agent trained with synthesized data and progressive training, plus a new Chinese deep-research benchmark, achieving expert-level performance at lower cost.", "motivation": "Existing academic benchmarks and agents for LLM-based deep research, such as BrowseComp, do not match real-world open-ended research needs, especially in intent understanding, long-horizon planning, and cross-source verification, and there is also a gap in Chinese-domain evaluation.", "method": "They design Step-DeepResearch, an end-to-end research agent, and create a data synthesis strategy based on decomposed atomic capabilities to train planning and report-writing. Training follows a progressive pipeline from agentic mid-training to supervised fine-tuning and reinforcement learning. They further add a checklist-style judging component to improve robustness and build ADR-Bench, a realistic Chinese deep research benchmark, for evaluation.", "result": "Step-DeepResearch with 32B parameters achieves 61.4% on Scale AI Research Rubrics, significantly outperforms comparable models on their new ADR-Bench, and performs competitively with strong proprietary systems such as OpenAI and Gemini DeepResearch.", "conclusion": "Carefully designed data synthesis and progressive agentic training can equip a medium-sized 32B model with expert-level deep-research abilities that are competitive with state-of-the-art closed models, while maintaining better cost-efficiency and providing strong performance in Chinese deep research tasks."}}
{"id": "2512.20548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20548", "abs": "https://arxiv.org/abs/2512.20548", "authors": ["Zhiyi Duan", "Xiangren Wang", "Hongyu Yuan", "Qianli Xing"], "title": "Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model", "comment": null, "summary": "Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.", "AI": {"tldr": "The paper builds the first large-scale multimodal teacher sentiment analysis dataset (T-MED) and proposes a new asymmetric attention-based model (AAM-TSA) that leverages text, audio, video, and instructional information to more accurately and interpretably classify teachers\u2019 emotions, outperforming existing methods.", "motivation": "Teachers\u2019 emotions strongly influence teaching quality, student engagement, and learning outcomes, but prior sentiment analysis work struggles to capture true teacher emotions because teachers often perform or mask emotions, and past models largely ignore instructional context that shapes emotional expression. There is also a lack of large, realistic multimodal datasets focused specifically on teacher sentiment, limiting progress in this domain.", "method": "The authors construct T-MED, a large multimodal dataset of teacher emotions collected from 14,938 annotated instances in 250 real classrooms across 11 subjects from K\u201312 to higher education, including synchronized text, audio, video, and instructional information. They use a human\u2013machine collaborative labeling process to improve annotation accuracy and efficiency. On top of this dataset, they design AAM-TSA, an asymmetric attention-based multimodal teacher sentiment analysis model that employs an asymmetric attention mechanism and a hierarchical gating unit to perform differentiated cross-modal fusion and enhance emotional classification.", "result": "On the T-MED dataset, the proposed AAM-TSA model achieves substantially better performance than existing state-of-the-art sentiment analysis methods, both in classification accuracy and in interpretability of how different modalities and instructional information contribute to emotion predictions.", "conclusion": "By providing the first large-scale multimodal teacher sentiment dataset and a specialized asymmetric attention-based model, the paper advances teacher emotion understanding in real classroom settings, demonstrating that incorporating instructional context and performing nuanced cross-modal fusion significantly improves the accuracy and interpretability of teacher sentiment analysis compared with prior approaches."}}
{"id": "2512.20569", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20569", "abs": "https://arxiv.org/abs/2512.20569", "authors": ["Yanhong Li", "Songlin Yang", "Shawn Tan", "Mayank Mishra", "Rameswar Panda", "Jiawei Zhou", "Yoon Kim"], "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection", "comment": null, "summary": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.", "AI": {"tldr": "They propose a simple, data-driven way to choose which layers of a pretrained softmax-attention Transformer to convert into linear-attention layers, then distill the model into a hybrid (softmax + linear) architecture for faster inference.", "motivation": "Hybrid softmax + linear attention architectures can make large language models more inference-efficient, but converting a pretrained Transformer raises the question: which specific layers should be turned into linear attention layers? Existing methods use naive heuristics (e.g., every other layer) or costly procedures with specialized diagnostic datasets. There is a need for a simple, effective, and cheap strategy for selecting layers to convert, so that we can systematically build efficient hybrid models from existing LLMs without full retraining.", "method": "1) Start from a pretrained softmax-attention Transformer. 2) Train it briefly on generic text data while computing an importance score per layer. 3) Use these scores to decide which layers to convert from softmax attention to linear attention. 4) Apply the RADLADS distillation pipeline: (a) transfer attention weights, (b) align hidden states, (c) perform KL-based distribution matching between teacher and student, and (d) run a short finetuning stage. The final model interleaves softmax and linear attention layers according to the learned importance ranking.", "result": "The proposed layer-importance-based selection recipe yields hybrid models that outperform baselines on downstream evaluations. Specifically, they do better than: (1) simple uniform schemes that interleave linear layers at a fixed ratio across depth, and (2) more complex strategies that depend on specialized diagnostic datasets. The method achieves these gains while using only a small amount of generic text data and a relatively lightweight training pass to obtain layer scores.", "conclusion": "A simple, inexpensive layer-importance scoring procedure, combined with an existing distillation pipeline, provides an effective way to convert pretrained softmax Transformers into efficient hybrid softmax+linear attention models. This data-driven layer selection outperforms both heuristic and more elaborate prior approaches, offering a practical tool for building faster LLMs from existing checkpoints without pretraining from scratch."}}
{"id": "2512.20586", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20586", "abs": "https://arxiv.org/abs/2512.20586", "authors": ["Humza Nusrat", "Luke Francisco", "Bing Luo", "Hassan Bagher-Ebadian", "Joshua Kim", "Karen Chin-Snyder", "Salim Siddiqui", "Mira Shah", "Eric Mellon", "Mohammad Ghassemi", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent", "comment": null, "summary": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.", "AI": {"tldr": "The paper develops and evaluates an LLM-based agent, SAGE, for automated stereotactic radiosurgery planning, showing that a reasoning (chain-of-thought) variant can match human plan quality while improving dose to critical structures and providing transparent, auditable optimization logs.", "motivation": "Stereotactic radiosurgery requires highly precise dose distributions near critical brain structures, and manual planning is labor-intensive and expertise-dependent. Existing AI planning tools are often black-box systems, limiting clinical trust and adoption because clinicians cannot easily see or audit how decisions are made. The authors aim to determine whether chain-of-thought reasoning in a large language model can enable more transparent, agentic, and clinically acceptable SRS treatment planning.", "method": "The authors built SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent that interacts with a treatment planning system to design SRS plans. In a retrospective cohort of 41 brain metastasis patients treated with 18 Gy single-fraction SRS, they generated two AI plans per case: one using a standard non-reasoning LLM and one using a reasoning/chain-of-thought LLM. They compared AI plans against human clinical plans on dosimetric endpoints (PTV coverage, max dose, conformity index, gradient index, organ-at-risk doses). They also analyzed the internal text-based optimization traces to quantify behaviors such as constraint verification, trade-off deliberation, and causal explanation to assess transparency and reasoning patterns.", "result": "The reasoning SAGE agent produced plans with dosimetric quality comparable to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all non-significant differences, p > 0.21). Notably, it reduced cochlear dose below human baselines with statistical significance (p = 0.022). Behaviorally, the reasoning model frequently engaged in prospective constraint verification (457 documented instances) and explicit trade-off deliberation (609 instances) when optimizing plans, whereas the non-reasoning model showed virtually none of these behaviors (0 and 7 instances, respectively). Content analysis localized most constraint-checking and causal explanation to the reasoning agent, demonstrating richer, more structured internal decision processes.", "conclusion": "An LLM-based reasoning agent, SAGE, can automate SRS treatment planning with plan quality on par with human experts while improving certain organ-at-risk sparing (e.g., cochlea). Incorporating chain-of-thought reasoning yields systematic constraint verification and trade-off analysis, producing optimization traces that function as auditable logs. This suggests that explicit reasoning can address transparency concerns and support safer, more trustworthy deployment of AI in radiotherapy planning."}}
{"id": "2512.20578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20578", "abs": "https://arxiv.org/abs/2512.20578", "authors": ["Amirhosein Ghasemabadi", "Di Niu"], "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits", "comment": null, "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.", "AI": {"tldr": "The paper proposes Gnosis, a lightweight mechanism that lets frozen LLMs predict whether their own generated answers are correct by reading internal hidden states and attention patterns, achieving better accuracy and calibration than existing self- or externally-judged methods at negligible extra cost.", "motivation": "LLMs often produce fluent yet incorrect or hallucinated outputs and struggle to recognize their own mistakes. Existing reliability solutions depend on external judges, multiple samples, or text-based self-critique, which are computationally expensive and only weakly correlated with ground-truth correctness. The authors are motivated to find an intrinsic, low-cost way for LLMs to assess their own reliability using internal signals already present during inference.", "method": "The authors introduce Gnosis, an add-on self-awareness module for frozen LLMs. During inference, Gnosis passively observes internal traces such as hidden states and attention patterns, then compresses these high-dimensional signals into fixed-size descriptors that are independent of sequence length. A small additional network (~5M parameters) is trained to map these descriptors to a correctness prediction. Gnosis operates without modifying the base LLM, incurs negligible extra inference cost, and can be applied across different backbones and tasks. It can also be used on partial generations to monitor ongoing reasoning trajectories.", "result": "Across multiple benchmark classes\u2014math reasoning, open-domain QA, and academic knowledge tasks\u2014and for backbones ranging from 1.7B to 20B parameters, Gnosis yields correctness predictions that surpass strong internal baselines and large external judge models in both accuracy and calibration. It offers reliable early-warning signals on partial generations and supports compute-aware control by detecting failing reasoning paths before completion. Its performance generalizes across tasks and model scales while maintaining low overhead.", "conclusion": "The study concludes that strong, actionable correctness signals are intrinsically encoded in the internal dynamics of LLM generation and can be extracted efficiently via a small add-on module. Gnosis demonstrates that frozen LLMs can gain effective self-verification capabilities\u2014outperforming heavier external-judging approaches\u2014by decoding information from hidden states and attention patterns. This opens up a path toward more trustworthy and compute-efficient LLM deployments without requiring model fine-tuning or expensive multi-sample procedures."}}
{"id": "2512.20618", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.20618", "abs": "https://arxiv.org/abs/2512.20618", "authors": ["Runtao Liu", "Ziyi Liu", "Jiaqi Tang", "Yue Ma", "Renjie Pi", "Jipeng Zhang", "Qifeng Chen"], "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "comment": null, "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "AI": {"tldr": "A multi-agent, reinforcement-learned framework for long-video question answering that improves temporal grounding and visual reasoning over hour-long TV episodes.", "motivation": "Existing long-video QA methods struggle with hour-long content because they either compress videos into lossy summaries or use limited tools. This harms temporal grounding and causes models to miss fine-grained visual and narrative cues important for answering detailed questions about long TV episodes.", "method": "They design a multi-agent system with three roles: (1) a master LLM that plans and coordinates; (2) a grounding agent that localizes video segments relevant to the question; and (3) a vision agent that extracts targeted textual observations from selected clips, complementing subtitles with visual details. The master agent has a step limit and is trained with reinforcement learning to optimize concise, correct, and efficient collaboration among agents, yielding interpretable reasoning trajectories.", "result": "On two new episode-level benchmarks, LongTVQA and LongTVQA+ (aggregated from TVQA/TVQA+ for hour-long episodes), their multi-agent system significantly outperforms strong non-agent baselines on long-video QA. Additional experiments show that reinforcement learning further boosts the master agent\u2019s reasoning quality and planning efficiency compared with non-RL variants.", "conclusion": "A coordinated, RL-trained multi-agent framework with explicit grounding and visual observation agents is more effective for long-video question answering than monolithic or summary-based approaches. It improves temporal grounding, captures fine-grained visual cues, and provides interpretable reasoning traces, advancing QA over hour-long TV episodes."}}
{"id": "2512.20595", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20595", "abs": "https://arxiv.org/abs/2512.20595", "authors": ["Dhruv Anand", "Ehsan Shareghi"], "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs", "comment": "27 pages, 5 figures, 9 tables. Cube available at https://github.com/dana-23/cube-bench", "summary": "We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.", "AI": {"tldr": "The paper presents Cube Bench, a Rubik\u2019s-cube-based benchmark to test multimodal large language models\u2019 spatial and sequential reasoning, revealing sharp performance degradation with task difficulty and a strong gap between closed- and open-weight models.", "motivation": "Existing benchmarks for multimodal large language models insufficiently isolate and stress-test spatial and sequential reasoning abilities, especially in tightly constrained, goal-directed settings. Rubik\u2019s cubes provide a compact, well-defined environment where perception, planning, and error recovery can be precisely measured, enabling more diagnostic evaluation than generic vision-language tasks.", "method": "The authors design Cube Bench, which uses a common set of scrambled Rubik\u2019s cube states at varying depths and evaluates models on five component skills: reconstructing cube faces from multimodal inputs, selecting the optimal next move, predicting the result of candidate moves, following multi-step solution plans with recovery from mistakes, and detecting/correcting their own errors. All models receive identical prompts and parsing rules, and performance is quantified via a uniform distance-to-solved metric to enable side-by-side comparison across seven recent MLLMs, including both closed- and open-weight systems.", "result": "Across the evaluated models, accuracy declines steeply as scramble depth increases. Models frequently fail to recover once their solution trajectories stall or diverge, indicating weak robustness in long-horizon control. High performance on perception-oriented face reconstruction does not translate into strong action selection or reliable multi-step execution. Closed-source models considerably outperform open-weight ones on both single-step and multi-step tasks, while most open models perform near chance on the hardest configurations. A simple reflective self-correction scheme yields modest improvements but sometimes harms performance by inducing overthinking and unnecessary revisions.", "conclusion": "Cube Bench serves as a focused, reproducible benchmark for probing sequential spatial reasoning in multimodal LLMs. The study highlights that current models, especially open-weight ones, struggle with deeper cube configurations and robust multi-step control, and that perception proficiency alone is an unreliable proxy for planning competence. The findings suggest significant headroom for future architectures and training strategies aimed at strengthening spatial reasoning, long-horizon planning, and effective self-correction in MLLMs."}}
{"id": "2512.20604", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20604", "abs": "https://arxiv.org/abs/2512.20604", "authors": ["Alexandros Christoforos", "Chadbourne Davis"], "title": "MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts", "comment": "Under submission", "summary": "We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.", "AI": {"tldr": "MoE-DiffuSeq is a diffusion-based text generation framework that uses mixture-of-experts and sparse attention to efficiently generate high-quality long documents.", "motivation": "Existing diffusion-based text generators like DiffuSeq become computationally expensive and memory-intensive for long sequences, limiting their practicality for tasks such as scientific articles, code repositories, or long dialogues. There is a need for methods that scale diffusion models to long documents without sacrificing quality.", "method": "The authors design MoE-DiffuSeq, which combines a mixture-of-experts (MoE) architecture with a customized sparse attention mechanism tailored for diffusion-based sequence modeling. Sparse attention reduces the quadratic cost of full attention, while MoE routes tokens to specialized expert subnetworks, improving efficiency and capacity. They also introduce a soft absorbing state in the diffusion process to speed up sequence reconstruction and increase generation precision.", "result": "Experiments on long document generation tasks show that MoE-DiffuSeq yields faster training and sampling than prior diffusion-based text models, while maintaining or improving text quality and coherence. Benchmarks indicate gains in efficiency, speed, accuracy, and expressiveness, especially on long document settings like scientific articles, code modeling, and long-form dialogue.", "conclusion": "MoE-DiffuSeq successfully scales diffusion models to long-form text generation by integrating sparse attention, mixture-of-experts, and a soft absorbing state. The framework enhances both computational efficiency and generation quality, making diffusion models more practical for real-world long document applications."}}
