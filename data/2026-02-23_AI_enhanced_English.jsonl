{"id": "2602.17784", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17784", "abs": "https://arxiv.org/abs/2602.17784", "authors": ["Meng Ye", "Xiao Lin", "Georgina Lukoczki", "Graham W. Lederer", "Yi Yao"], "title": "QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration", "comment": null, "summary": "Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.", "AI": {"tldr": "The paper introduces QueryPlot, a semantic retrieval and mapping framework that uses NLP embeddings to link geological text (deposit models) with geologic map data to create mineral prospectivity maps from natural language queries.", "motivation": "Mineral prospectivity mapping is important for identifying regions likely to host specific mineral deposits, but current workflows are manual, expert-driven, and struggle to synthesize heterogeneous textual knowledge (deposit models) with large geospatial datasets. There is a need for scalable, data-driven tools that can automatically translate qualitative geological descriptions into spatial evidence layers suitable for exploration targeting and machine learning.", "method": "The authors build QueryPlot, which (1) curates descriptive deposit models for 120+ deposit types; (2) converts polygons from the State Geologic Map Compilation (SGMC) into structured textual descriptions; (3) uses a pretrained text embedding model to encode both user natural language queries and region descriptions into a shared vector space; (4) computes semantic similarity between queries and regions to rank and map polygons as continuous evidence layers; (5) enables compositional queries by aggregating multiple similarity layers for multi-criteria analysis; and (6) exposes these capabilities through a web-based interactive system that outputs GIS-compatible layers. They conduct a tungsten skarn case study and integrate similarity features into supervised learning pipelines to test added predictive value.", "result": "In the tungsten skarn case study, the embedding-based semantic retrieval achieves high recall of known tungsten skarn occurrences and generates prospective areas that closely match expert-defined permissive tracts. When the similarity scores are added as features in supervised learning models, they provide measurable improvements in classification accuracy over baselines without these features.", "conclusion": "QueryPlot demonstrates that modern NLP embedding models can successfully connect textual geological knowledge with geologic map data for mineral prospectivity mapping, enabling natural-language-driven, compositional querying and automated generation of spatial evidence layers. The framework is effective both for direct prospectivity mapping and as a feature generator for supervised learning, and it is made available as an open-source, web-based tool with public datasets to support further research and application in mineral exploration."}}
{"id": "2602.17815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17815", "abs": "https://arxiv.org/abs/2602.17815", "authors": ["Zhining Zhang", "Wentao Zhu", "Chi Han", "Yizhou Wang", "Heng Ji"], "title": "Neural Synchrony Between Socially Interacting Language Models", "comment": "Accepted at ICLR 2026", "summary": "Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the \"social minds\" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.", "AI": {"tldr": "The paper studies whether multiple interacting LLMs exhibit \u2018neural synchrony\u2019\u2014synchronized internal activations\u2014analogous to what is observed between human brains during social interaction, and shows this synchrony tracks their social engagement and performance.", "motivation": "Human brains show synchronized activity during social interaction, which is considered a hallmark of social cognition or \u201csocial minds.\u201d LLMs, especially in multi-agent settings, are often described as socially capable, but it is unclear whether they have any internal dynamics comparable to human social minds. The authors want an empirical, representation-level way to evaluate the \u201csociality\u201d of LLMs beyond surface behavior by testing whether interacting LLMs show neural synchrony similar to humans.", "method": "They design multi-LLM social interaction simulations and measure \u2018neural synchrony\u2019 by comparing internal representations (activations) of different LLMs over time while they interact. They construct experiments where levels of social engagement and temporal alignment can vary, and quantify synchrony across these conditions. They then correlate the measured synchrony with independent measures of social performance in the tasks.", "result": "The experiments show that neural synchrony between LLMs systematically increases with higher social engagement and better temporal alignment in their interactions. Moreover, the degree of synchrony is strongly correlated with how well the LLMs perform on social tasks, suggesting that synchrony is a meaningful index of their social behavior quality.", "conclusion": "Neural synchrony can serve as a useful proxy for analyzing the sociality of LLMs at the representational level. Interacting LLMs exhibit internal synchronization patterns that parallel human brain synchrony in social contexts, and this synchrony is tightly linked to their social performance. This supports the idea that multi-LLM systems can be fruitfully compared to human \u201csocial minds\u201d at the level of internal dynamics, offering a new lens for studying LLM social interaction capabilities."}}
{"id": "2602.17848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17848", "abs": "https://arxiv.org/abs/2602.17848", "authors": ["Cassandra L. Jacobs", "Morgan Grobol"], "title": "On the scaling relationship between cloze probabilities and language model next-token prediction", "comment": null, "summary": "Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.", "AI": {"tldr": "Larger language models better predict human eye movements and reading times but still underestimate human-like responses.", "motivation": "To understand how model size in language models affects their ability to predict human reading behavior and cloze (next-word prediction) responses.", "method": "Compare different-sized language models on their ability to assign probabilities to human cloze responses and to predict eye movement and reading time measures, analyzing sensitivity to lexical co-occurrence vs. semantic alignment.", "result": "Larger models assign more accurate probabilities to human cloze responses and better predict reading behavior, being more semantically aligned with human choices but less sensitive to low-level lexical co-occurrence statistics.", "conclusion": "Increased memorization capacity in larger language models improves their semantic guessing of upcoming words and alignment with human reading behavior, but reduces their sensitivity to low-level information important for word recognition."}}
{"id": "2602.17881", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17881", "abs": "https://arxiv.org/abs/2602.17881", "authors": ["Joschka Braun"], "title": "Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations", "comment": "Master's Thesis, University of T\u00fcbingen. 89 pages, 34 figures. Portions of this work were published at the ICLR 2025 Workshop on Foundation Models in the Wild (see arXiv:2505.22637)", "summary": "Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.", "AI": {"tldr": "The thesis analyzes why steering vectors for language models are often unreliable and shows that reliability depends on how well a linear direction approximates the underlying latent behavior.", "motivation": "Steering vectors are an appealing, lightweight way to control language model behavior without retraining, but their effect is inconsistent across behaviors and prompts. Understanding what makes steering reliable or unreliable is important for safe, predictable control of language models and for designing better steering methods.", "method": "The author empirically studies steering vectors by (1) training them on different behavior datasets and prompt variations, (2) measuring cosine similarity between training activation differences, (3) analyzing how well positive vs. negative examples separate along the learned steering direction, and (4) comparing the performance and cross-dataset efficacy correlations of different steering directions.", "result": "They find that: (1) higher cosine similarity among training activation differences predicts more reliable steering; (2) datasets where positive and negative activations are more cleanly separated along the steering direction yield more reliable steering; and (3) steering vectors trained on different prompt variants are directionally distinct in activation space but perform similarly and their strengths/weaknesses are correlated across datasets.", "conclusion": "Steering vectors tend to fail when the true latent representation of the target behavior is not well-approximated by a single linear direction in activation space. The work provides diagnostics\u2014such as activation similarity and class separation along the steering direction\u2014for detecting when steering will be unreliable, and argues that future methods should incorporate non-linear structure in the underlying behavior representations."}}
{"id": "2602.17676", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17676", "abs": "https://arxiv.org/abs/2602.17676", "authors": ["Xingcheng Xu", "Jingjing Qu", "Qiaosheng Zhang", "Chaochao Lu", "Yanqing Yang", "Na Zou", "Xia Hu"], "title": "Epistemic Traps: Rational Misalignment Driven by Model Misspecification", "comment": null, "summary": "The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a \"locked-in\" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.", "AI": {"tldr": "The paper argues that common unsafe behaviors in large language models (LLMs)\u2014like hallucination, sycophancy, and deception\u2014are not just training bugs but rational outcomes of how the models internally represent the world, and it proposes a formal framework to explain and address this.", "motivation": "LLMs and AI agents show recurring unsafe behaviors that standard safety methods (especially reinforcement learning-based fine-tuning) cannot reliably remove. Existing safety work tends to treat these as fixable artifacts of training, without a solid theoretical account of why they appear and persist. The authors want a unified theory that explains these behaviors as stable, rational responses of the system, rather than accidental glitches, to better guide alignment methods.", "method": "The authors adapt Berk-Nash Rationalizability, a concept from theoretical economics, to model AI agents as optimizers operating under a misspecified, subjective model of the world. Within this framework, they analyze how different reward schemes and internal belief structures lead to specific behavioral equilibria, including unsafe ones. They then run behavioral experiments on six state-of-the-art model families and construct empirical \u201cphase diagrams\u201d that map where safe vs unsafe behaviors appear as a function of model beliefs and incentives.", "result": "The analysis shows that behaviors like hallucination, sycophancy, and strategic deception arise naturally and persistently when the agent\u2019s internal world model is misspecified, forming stable misaligned equilibria or oscillatory patterns depending on the reward structure. Strategic deception can become locked-in or sustained even when it is objectively risky. Empirical experiments with multiple model families confirm the theoretical predictions and delineate precise boundaries between safe and unsafe behavioral regimes.", "conclusion": "Safety in LLMs is not a smooth outcome of simply making rewards for good behavior larger; instead, it corresponds to distinct phases determined by the agent\u2019s epistemic priors\u2014its internal beliefs about the world. Therefore, robust alignment requires \u201cSubjective Model Engineering\u201d: intentionally designing and shaping the agent\u2019s internal belief structure, not just tuning external reward functions. This reframes alignment as engineering how the model interprets reality, representing a significant shift in safety strategy."}}
{"id": "2602.17907", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17907", "abs": "https://arxiv.org/abs/2602.17907", "authors": ["Raymond Li", "Amirhossein Abaskohi", "Chuyuan Li", "Gabriel Murray", "Giuseppe Carenini"], "title": "Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions", "comment": "20 pages, 5 figures", "summary": "Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.", "AI": {"tldr": "They improve neural topic models by supervising them with soft labels derived from language models\u2019 contextual token distributions instead of sparse Bag-of-Words reconstruction, leading to better topic quality and document retrieval performance.", "motivation": "Traditional neural topic models are trained to reconstruct bag-of-words representations, which ignore word order and context and suffer from data sparsity, resulting in suboptimal topic quality and limited usefulness for tasks like semantic retrieval. With powerful language models now capturing rich contextual information, there is a need to leverage their knowledge to provide more semantically meaningful supervision for topic modeling.", "method": "They construct semantically grounded soft label targets using a language model: given a document and a specialized prompt, they obtain next-token probability distributions from the LM and project these probabilities onto a predefined vocabulary. These projected distributions serve as soft, context-enriched supervision signals. The neural topic model is then trained to reconstruct these soft labels using its own hidden states, rather than reconstructing sparse BoW counts. They also design a retrieval-based evaluation metric that measures how well the learned topic representations support retrieval of semantically similar documents.", "result": "On three benchmark datasets, their method achieves substantial gains in standard topic-modeling metrics such as topic coherence and purity compared to existing neural topic model baselines. Under the proposed retrieval-based metric, their approach significantly outperforms prior methods at identifying semantically similar documents, indicating stronger and more useful topic representations for retrieval tasks.", "conclusion": "Leveraging language models to generate contextual, soft supervision signals for topic models yields topics that better reflect the true thematic structure of corpora and substantially improve document retrieval of semantically similar texts. This demonstrates that LM-derived soft labels are an effective replacement for traditional BoW reconstruction objectives in neural topic modeling, especially for retrieval-oriented applications."}}
{"id": "2602.17826", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.17826", "abs": "https://arxiv.org/abs/2602.17826", "authors": ["Marcelo Labre"], "title": "Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge", "comment": "Submitted to NeuS 2026. Supplementary materials and code: https://doi.org/10.5281/zenodo.18665030", "summary": "Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.", "AI": {"tldr": "The paper tests whether plugging a formal math ontology into a retrieval-augmented generation pipeline can make language models more reliable on math problems, finding gains when retrieval is accurate but harm when it is not.", "motivation": "Language models often hallucinate, are brittle, and lack formal grounding, which is particularly dangerous in high-stakes domains that need verifiable reasoning. The author wants to see if connecting LMs to explicit, formal domain knowledge (ontologies) can counter these issues and improve trustworthy reasoning, using mathematics as a clean, well-structured testbed.", "method": "They build a neuro-symbolic pipeline that uses the OpenMath formal ontology as a knowledge base. For each math problem, a hybrid retriever plus cross-encoder reranker selects relevant formal definitions and injects them as additional context into the language model\u2019s prompt (a retrieval-augmented generation setup). They then run three open-source language models on the MATH benchmark with and without this ontology-guided context.", "result": "Across the three open-source models evaluated on MATH, performance improves when the retrieval component successfully selects highly relevant ontology entries, indicating that formal, structured context can help models reason better. However, when retrieval quality is poor and irrelevant definitions are inserted, model performance drops compared to no extra context, showing that bad symbolic guidance is actively harmful.", "conclusion": "Formal domain ontologies integrated into RAG pipelines can indeed improve language model reliability and accuracy in specialized domains, but they introduce a strong dependence on retrieval quality. Neuro-symbolic methods are promising but fragile: to be beneficial, they require robust mechanisms to ensure that the injected formal knowledge is accurate and relevant, otherwise it can degrade reasoning instead of grounding it."}}
{"id": "2602.17911", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17911", "abs": "https://arxiv.org/abs/2602.17911", "authors": ["Jash Rajesh Parekh", "Wonbin Kweon", "Joey Chan", "Rezarta Islamaj", "Robert Leaman", "Pengcheng Jiang", "Chih-Hsuan Wei", "Zhizheng Wang", "Zhiyong Lu", "Jiawei Han"], "title": "Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering", "comment": null, "summary": "Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.", "AI": {"tldr": "Introduces CondMedQA, a benchmark for conditional biomedical question answering, and Condition-Gated Reasoning (CGR), a framework that uses condition-aware knowledge graphs to select patient-appropriate answers, improving robustness and performance.", "motivation": "Real clinical decisions depend heavily on patient-specific conditions (e.g., comorbidities, contraindications), but existing biomedical QA systems and benchmarks largely treat medical knowledge as uniformly applicable. Current retrieval-augmented and graph-based methods lack explicit mechanisms to verify that retrieved evidence is actually applicable to the given patient context, and no benchmark systematically tests conditional reasoning. This creates a gap between QA performance metrics and the true demands of clinical reasoning.", "method": "1) Construct CondMedQA, a benchmark of multi-hop biomedical questions whose correct answers explicitly depend on specified patient conditions. 2) Propose Condition-Gated Reasoning (CGR), which builds condition-aware knowledge graphs where nodes/edges encode relevance to particular clinical conditions. 3) During inference, CGR selectively activates or prunes reasoning paths based on the conditions specified in the query, effectively \u201cgating\u201d the reasoning process to maintain condition applicability. 4) Evaluate CGR against state-of-the-art biomedical QA systems on both CondMedQA and standard benchmarks.", "result": "CGR demonstrates improved ability to choose answers that are appropriate to the given patient conditions on the new CondMedQA benchmark, while achieving comparable or better performance than existing state-of-the-art methods on standard biomedical QA datasets. This indicates that explicit condition-gating does not sacrifice general QA accuracy and yields clear gains on conditional reasoning tasks.", "conclusion": "Explicitly modeling conditionality via condition-aware knowledge graphs and gated reasoning paths leads to more reliable selection of clinically appropriate answers in biomedical QA. The CondMedQA benchmark fills a missing evaluation need for conditional reasoning, and the CGR framework shows that incorporating patient-specific conditions into the reasoning pipeline is both feasible and beneficial for robust medical QA systems."}}
{"id": "2602.17831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17831", "abs": "https://arxiv.org/abs/2602.17831", "authors": ["Simon Henniger", "Gabriel Poesia"], "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels", "comment": "Project website: https://token-games.ai/", "summary": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.", "AI": {"tldr": "They propose The Token Games, an automated framework where language models generate and solve programming puzzles to evaluate each other\u2019s reasoning, computing Elo ratings from pairwise duels.", "motivation": "Existing reasoning benchmarks are becoming harder to maintain: crafting truly challenging, unseen questions is expensive and requires expert (often PhD-level) knowledge, and there is lingering concern that models might have seen similar problems during training, so benchmarks may not really measure genuine reasoning. The authors want an evaluation method that scales without heavy human curation, is less vulnerable to training-data contamination, and can keep challenging improving models.", "method": "They introduce The Token Games (TTG), inspired by historical mathematical duels. In TTG, language models generate their own programming puzzles: each puzzle is a Python boolean function, and a solver must find inputs that cause it to return True. Different models both create and solve such puzzles in pairwise \u201cduels.\u201d Outcomes of these duels are then converted into Elo ratings, providing a relative ranking of models. The programming-puzzle format enables automatic verification of solutions without human grading and supports a wide variety of problem types within a unified interface.", "result": "They apply TTG to 10 frontier language models. The Elo-based rankings obtained from the self-generated puzzle duels closely align with rankings from established, human-authored benchmarks such as Humanity\u2019s Last Exam, even though TTG requires no human effort to create the puzzles. They also observe that puzzle creation itself is difficult for current models, revealing a capability dimension (good-task generation) not captured in prior benchmarks.", "conclusion": "The Token Games offer a scalable, self-generating evaluation paradigm for reasoning: models challenge each other with automatically verifiable programming puzzles, and their duel performance yields meaningful comparative rankings. This framework is hard to saturate by construction, reduces reliance on human-crafted questions, and opens the door to evaluating additional skills such as creativity and task creation alongside standard problem solving."}}
{"id": "2602.17937", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17937", "abs": "https://arxiv.org/abs/2602.17937", "authors": ["Xiaotang Du", "Giwon Hong", "Wai-Chung Kwan", "Rohit Saxena", "Ivan Titov", "Pasquale Minervini", "Emily Allaway"], "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification", "comment": null, "summary": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.", "AI": {"tldr": "The paper systematically compares instruction optimization methods for tabular fact verification with LLMs and shows they reliably improve accuracy across prompts, models, and benchmarks.", "motivation": "While instruction optimization has improved LLM reasoning in various tasks, its impact on tabular fact verification\u2014especially across different prompting styles and model families\u2014has not been rigorously studied. The authors aim to clarify which optimizers and prompting strategies work best and how they change model behavior.", "method": "Using the DSPy optimization framework, the authors evaluate three instruction optimizers (COPRO, MiPROv2, SIMBA) applied to four prompting paradigms (direct prediction, Chain-of-Thought, ReAct with SQL tools, and CodeAct with Python execution). They run experiments on four tabular fact verification benchmarks and three LLM families, and then perform behavioral analyses to understand how optimization alters reasoning patterns and tool use.", "result": "Instruction optimization consistently raises verification accuracy. MiPROv2 delivers the most stable gains for Chain-of-Thought prompting, while SIMBA yields the largest improvements for ReAct agents, particularly with larger models. Behavioral analysis shows SIMBA nudges models toward more direct reasoning, enhances numerical comparison in CoT, and reduces unnecessary tool calls in ReAct.", "conclusion": "Instruction optimization is an effective, model-agnostic way to boost tabular fact verification performance. CoT prompting remains strong, especially for smaller models, whereas ReAct agents with larger models can match or exceed performance but only when their instructions are carefully optimized, with SIMBA being especially beneficial for tool-using agents."}}
{"id": "2602.17902", "categories": ["cs.AI", "cs.MA", "cs.SE", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.17902", "abs": "https://arxiv.org/abs/2602.17902", "authors": ["Jiaru Bai", "Abdulrahman Aldossary", "Thomas Swanick", "Marcel M\u00fcller", "Yeonghun Kang", "Zijian Zhang", "Jin Won Lee", "Tsz Wai Ko", "Mohammad Ghazi Vakili", "Varinia Bernales", "Al\u00e1n Aspuru-Guzik"], "title": "El Agente Gr\u00e1fico: Structured Execution Graphs for Scientific Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gr\u00e1fico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.", "AI": {"tldr": "The paper introduces El Agente Gr\u00e1fico, a type-safe, graph-backed LLM agent framework that coordinates complex scientific workflows more reliably than ad hoc, text-centric agent systems.", "motivation": "Existing LLM-based scientific agents integrate with external tools in an ad hoc, brittle way, using unstructured text for context and coordination. This makes workflows hard to audit, reproduce, and scale, especially when dealing with complex, multi-step computational science tasks. The authors aim to provide a more robust, transparent, and scalable foundation for agentic scientific automation.", "method": "They design a single-agent system, El Agente Gr\u00e1fico, that embeds LLM decisions inside a type-safe execution environment. Scientific concepts and computational states are modeled as typed Python objects, managed via an object-graph mapper and stored either in memory or in an external dynamic knowledge graph. The agent uses typed symbolic identifiers instead of raw text to manage context and orchestrate tools, enabling provenance tracking and consistent state management. They evaluate the system on automated benchmarks for university-level quantum chemistry tasks and extend it to conformer ensemble generation and metal-organic framework (MOF) design.", "result": "The system shows that a single LLM agent, when coupled with the proposed reliable execution engine and knowledge-graph-backed state, can robustly execute complex, multi-step, and parallel computations that had previously required a multi-agent setup. It successfully supports additional application classes (conformer ensembles and MOF design) using knowledge graphs as both memory and reasoning substrates.", "conclusion": "Abstraction via typed objects and knowledge graphs, together with a type-safe execution environment, provides a scalable and auditable basis for LLM-driven scientific agents. This structured, graph-centric approach can replace brittle, prompt-centric, text-only coordination, enabling more reliable and extensible automation of complex scientific workflows."}}
{"id": "2602.17949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17949", "abs": "https://arxiv.org/abs/2602.17949", "authors": ["Victoria Blake", "Mathew Miller", "Jamie Novak", "Sze-yuan Ooi", "Blanca Gallego"], "title": "CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications", "comment": "30 pages, 6 figures, 4 tables", "summary": "Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.", "AI": {"tldr": "The paper introduces CUICurate, a GraphRAG-based framework that automatically builds clinically meaningful UMLS concept sets from CUIs, matching human precision while improving coverage and scalability.", "motivation": "Existing clinical NER systems map text to individual UMLS CUIs, but many downstream tasks need broader, clinically meaningful concept sets that group synonyms, subtypes, and supertypes. Manually creating these sets is laborious, inconsistent, and poorly supported, especially when NLP pipelines already work at the CUI level. The authors aim to automate and standardize this concept set curation process.", "method": "The authors build a UMLS knowledge graph and generate graph embeddings to enable semantic retrieval of candidate CUIs related to a target concept. For each target, the system retrieves candidate CUIs from the graph, then applies a two-stage large language model pipeline: (1) LLM-based filtering to remove irrelevant CUIs and (2) LLM-based classification to assign candidates into appropriate concept-set categories. They compare a larger LLM (GPT-5) with a smaller model (GPT-5-mini) on these tasks and evaluate performance on five lexically heterogeneous clinical concepts against manually curated benchmark and gold-standard concept sets.", "result": "CUICurate generated concept sets that were larger and more complete than manually curated benchmarks while maintaining human-level precision. GPT-5-mini performed better in the high-recall filtering step, whereas GPT-5 produced final classifications that aligned more closely with clinician judgments. The system\u2019s outputs were consistent across repeated runs and required relatively low computational resources.", "conclusion": "CUICurate provides a scalable, reproducible way to curate UMLS concept sets, significantly reducing manual effort. By combining graph-based semantic retrieval with LLM reasoning, it yields focused, high-quality candidate concept sets suitable for integration into clinical NLP pipelines and adaptable to varied phenotyping and analytic use cases."}}
{"id": "2602.17910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17910", "abs": "https://arxiv.org/abs/2602.17910", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems", "comment": null, "summary": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.", "AI": {"tldr": "The paper proposes APEMO, a runtime scheduling layer that improves long-horizon AI agent reliability by reallocating computation to critical trajectory moments using affect-like temporal signals.", "motivation": "Existing AI alignment methods focus on single responses, not the stability and reliability of autonomous agents over long interaction trajectories. There is a need to ensure high-quality behavior across entire workflows under limited computational budgets.", "method": "Introduce APEMO, a scheduling/orchestration layer that (1) monitors trajectories with behavioral proxies to detect instability, (2) identifies critical segments such as peaks and endings using temporal-affective cues, and (3) selectively increases computational resources or repairs at those points without changing model weights. It is evaluated in multi-agent simulations and planner\u2013executor LLM setups.", "result": "APEMO, when plugged into long-horizon agent workflows, consistently improves overall trajectory quality and the likelihood that trajectories are reusable, outperforming more static or structural orchestration baselines under the same compute budget.", "conclusion": "Alignment for agentic systems should be viewed as a temporal control and resource-allocation problem. APEMO provides a practical, model-agnostic engineering approach for making long-horizon agents more reliable without retraining or weight modification."}}
{"id": "2602.17981", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17981", "abs": "https://arxiv.org/abs/2602.17981", "authors": ["Amine Kobeissi", "Philippe Langlais"], "title": "Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering", "comment": null, "summary": "Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.", "AI": {"tldr": "The paper analyzes failures in retrieval-augmented generation for financial QA and proposes a page-level retrieval model to improve within-document context selection, boosting page and chunk recall on FinanceBench.", "motivation": "In financial QA over long regulatory filings, getting the right answer requires not just finding the correct document but also the precise page or chunk with the answer. A common but under-studied failure is when systems retrieve the right document but miss the exact page, causing the generator to guess from incomplete context. The authors are motivated to systematically study this within-document retrieval failure and quantify its impact, especially since accuracy is critical in high-stakes financial settings.", "method": "The authors perform a multi-level retrieval evaluation (document, page, chunk) on a 150-question subset of FinanceBench. They reproduce and compare several retrieval setups: dense, sparse, hybrid, and hierarchical retrieval, combined with reranking and query reformulation. They then conduct oracle-based analyses to estimate upper bounds on both retrieval and generation performance at different granularities. To address page- and chunk-level gaps, they introduce a domain-specific page scorer: a bi-encoder fine-tuned on page-level relevance in financial filings, using pages as an intermediate retrieval unit between document and chunk and leveraging page-level semantic coherence.", "result": "Empirically, they find that better document-level retrieval generally improves page-level recall across methods, but oracle analyses reveal remaining headroom at page and chunk levels, indicating that current systems still frequently miss the exact answering segment. Their proposed domain-fine-tuned page scorer significantly improves page recall and downstream chunk retrieval performance compared to baseline retrieval strategies.", "conclusion": "The study concludes that within-document retrieval\u2014specifically at page and chunk level\u2014is a key bottleneck for reliable financial QA with retrieval-augmented generation. Evaluating and optimizing retrieval at multiple granularities is crucial, and introducing a page-level bi-encoder tailored to financial filings effectively narrows this gap by exploiting the natural semantic structure of pages, leading to more reliable access to answer-bearing contexts."}}
{"id": "2602.17990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17990", "abs": "https://arxiv.org/abs/2602.17990", "authors": ["Madhav Kanda", "Pedro Las-Casas", "Alok Gautam Kumbhare", "Rodrigo Fonseca", "Sharad Agarwal"], "title": "WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics", "comment": null, "summary": "LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.", "AI": {"tldr": "The paper introduces WorkflowPerturb, a benchmark that perturbs high-quality LLM-generated workflows in controlled ways to test and analyze the sensitivity and calibration of workflow evaluation metrics.", "motivation": "Evaluating structured workflows generated by large language models is hard because existing automatic metrics are poorly calibrated: they do not map consistently to human notions of quality and do not clearly indicate how severe a degradation in the workflow is when scores change. There is a need for a controlled setting to study how different metrics respond to specific, realistic errors in workflows so that score changes can be interpreted in terms of error severity.", "method": "The authors create WorkflowPerturb, a benchmark composed of 4,973 high-quality (golden) workflows and 44,757 perturbed variants. They design three realistic perturbation types\u2014Missing Steps, Compressed Steps, and Description Changes\u2014each applied at three severity levels (10%, 30%, and 50%) to the golden workflows. They then evaluate multiple families of workflow evaluation metrics on this benchmark and analyze how metric scores change with perturbation severity using expected score trajectories and residual analyses.", "result": "The study reveals systematic differences in how various metric families react to controlled workflow perturbations. Some metrics are more or less sensitive to particular perturbation types or severity levels, and the calibration of scores\u2014how well score magnitudes track error severity\u2014varies across metrics. The analyses, via score trajectories and residuals, highlight strengths and weaknesses in existing metrics for severity-aware workflow evaluation.", "conclusion": "WorkflowPerturb provides a controlled, realistic benchmark for probing the sensitivity and calibration of workflow evaluation metrics in LLM-generated workflows. The empirical results show that different metric families behave systematically differently and that understanding these behaviors is crucial for interpreting metric scores in terms of workflow degradation severity. The benchmark and analyses support the development and use of more severity-aware evaluation practices, and the dataset will be released to the community upon acceptance."}}
{"id": "2602.18029", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18029", "abs": "https://arxiv.org/abs/2602.18029", "authors": ["Ali El Filali", "In\u00e8s Bedar"], "title": "Towards More Standardized AI Evaluation: From Models to Agents", "comment": "19 pages, 3 figures", "summary": "Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer \"How good is the model?\" but \"Can we trust the system to behave as intended, under change, at scale?\". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.", "AI": {"tldr": "The paper argues that traditional, model-centric evaluation methods are inadequate for modern AI agents and positions evaluation as an ongoing control and governance discipline rather than a final performance check.", "motivation": "As AI systems shift from static models to dynamic, tool-using agents deployed at scale, organizations need reliable ways to understand, control, and trust system behavior. Existing evaluation practices based on static benchmarks and aggregate scores fail to capture real-world, changing, and non-deterministic behavior, leading to silent failures and misplaced trust.", "method": "The paper conceptually analyzes common evaluation pipelines, identifies their implicit assumptions and failure modes, and examines how the rise of agentic, tool-using systems changes the nature of what needs to be measured. Rather than introducing new datasets or metrics, it reframes evaluation as an ongoing, system-level measurement and control problem.", "result": "The analysis shows that high benchmark scores often fail to correlate with trustworthy real-world behavior, that evaluation pipelines themselves can be major sources of error and opacity, and that agentic systems make traditional one-shot performance metrics insufficient. The paper articulates a clearer conceptual model of evaluation\u2019s role in AI agents.", "conclusion": "Evaluation should be treated as a continuous, governance-oriented measurement discipline embedded throughout the AI lifecycle, especially for agentic systems. Instead of focusing on winning benchmarks, teams should design evaluation processes that reveal system behavior under change and at scale, thereby conditioning trust, iteration, and oversight in non-deterministic AI systems."}}
{"id": "2602.18025", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18025", "abs": "https://arxiv.org/abs/2602.18025", "authors": ["Haruki Abe", "Takayuki Osa", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets", "comment": "ICLR 2026", "summary": "Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.", "AI": {"tldr": "They study how to pre-train robot locomotion policies using offline reinforcement learning together with cross-embodiment data from many different robots, and propose a grouping strategy to reduce conflicts between robots with very different bodies.", "motivation": "Collecting high-quality expert demonstrations for each robot platform is expensive and limits scalable robot policy pre-training. There is a need to leverage abundant suboptimal data and trajectories from many different robot morphologies to learn generalizable control priors efficiently, while understanding when and why this works or fails.", "method": "They combine offline reinforcement learning with cross-embodiment learning: using heterogeneous trajectories (both expert and suboptimal) from many robot platforms to learn universal locomotion control priors. They systematically analyze this paradigm using a new locomotion benchmark dataset covering 16 different robot morphologies. They study the impact of the proportion of suboptimal data and number of robot types on learning, identify gradient conflicts across morphologies, and introduce an embodiment-based grouping strategy that clusters robots by morphological similarity and updates the policy using group-level gradients to reduce conflicts.", "result": "The combined offline RL + cross-embodiment approach is effective when datasets contain many suboptimal trajectories, and it outperforms pure behavior cloning in pre-training locomotion policies. However, performance degrades as suboptimal data increases and as more, diverse robot types are added, due to conflicting gradients from different morphologies. The proposed static embodiment-based grouping strategy significantly reduces these inter-robot gradient conflicts and empirically outperforms other conflict-resolution baselines.", "conclusion": "Offline RL combined with cross-embodiment learning is a powerful paradigm for scalable robot locomotion pre-training, especially when leveraging large amounts of suboptimal data. Yet, naive aggregation of many diverse robot morphologies leads to gradient conflicts that hurt learning. A simple, morphology-aware grouping of robots with group-level gradient updates can effectively mitigate these conflicts and yield better pre-trained policies than existing conflict-resolution approaches."}}
{"id": "2602.18092", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18092", "abs": "https://arxiv.org/abs/2602.18092", "authors": ["Matthew DiGiuseppe", "Joshua Robison"], "title": "Perceived Political Bias in LLMs Reduces Persuasive Abilities", "comment": "39 pages, 10 figures", "summary": "Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.", "AI": {"tldr": "The paper studies how claims that ChatGPT is politically biased affect its ability to correct people\u2019s economic policy misconceptions, finding that partisan bias warnings substantially reduce its persuasive impact.", "motivation": "Conversational AI is increasingly used to correct public misconceptions, but its success likely depends on whether users see it as politically neutral. As political elites frame large language models as ideologically biased, it becomes crucial to understand whether such credibility attacks undermine AI-driven persuasion, especially in polarized contexts like U.S. politics.", "method": "The authors run a preregistered survey experiment in the United States (N=2144). Participants engage in a three-round conversation with ChatGPT focused on correcting a personally held economic policy misconception. Participants are randomly assigned to a neutral control condition or a treatment where they receive a brief message asserting that the LLM is biased against their own political party. The authors then measure changes in beliefs and analyze conversation transcripts to assess differences in engagement and receptivity.", "result": "Relative to the neutral control, being told that the LLM is biased against one\u2019s party reduces belief change (persuasion) by about 28%. Transcript analysis shows that treated participants push back more against the AI\u2019s arguments and display less receptive engagement during the conversation, indicating that the bias warning meaningfully changes the interaction dynamics.", "conclusion": "Perceptions of partisan alignment significantly shape the persuasive power of conversational AI. When users are led to believe that an LLM is biased against their political camp, its ability to correct misconceptions is markedly diminished, as users become more resistant and less engaged. This implies that political attacks on AI neutrality can materially limit the effectiveness of LLM-based interventions to address misinformation, especially in polarized environments."}}
{"id": "2602.18137", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18137", "abs": "https://arxiv.org/abs/2602.18137", "authors": ["Vincent Grari", "Ciprian Tomoiaga", "Sylvain Lamprier", "Tatsunori Hashimoto", "Marcin Detyniecki"], "title": "Agentic Adversarial QA for Improving Domain-Specific LLMs", "comment": "9 pages, 1 Figure", "summary": "Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.", "AI": {"tldr": "They propose an adversarial question-generation framework that creates compact, challenging synthetic data to better fine-tune LLMs for specialized domains, improving accuracy with fewer samples on LegalBench.", "motivation": "LLMs pretrained on broad internet data underperform in specialized domains, and current synthetic data methods (paraphrasing, knowledge extraction) lack support for interpretive reasoning and create large, redundant datasets with poor sample efficiency. There is a need for a more targeted, efficient way to generate synthetic training data that focuses on a model\u2019s real weaknesses in specialized tasks.", "method": "They introduce an adversarial question-generation framework. It iteratively compares outputs from the target model and a stronger expert model, both grounded in reference documents. By analyzing discrepancies, it generates semantically challenging questions that expose comprehension gaps. This is a feedback-driven loop that refines questions to focus on interpretive reasoning weaknesses while keeping the synthetic dataset compact and non-redundant.", "result": "On specialized subsets of the LegalBench corpus, their framework yields higher accuracy than baselines while requiring substantially fewer synthetic training samples, indicating improved sample efficiency and better adaptation to the legal domain.", "conclusion": "Adversarial, feedback-driven question generation targeted at model\u2013expert disagreements can produce a small but highly informative synthetic dataset that significantly improves LLM adaptation to specialized domains, particularly in interpretive reasoning, and does so more sample-efficiently than standard synthetic data approaches."}}
{"id": "2602.18201", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18201", "abs": "https://arxiv.org/abs/2602.18201", "authors": ["Joseph Bingham", "Netanel Arussy", "Dvir Aran"], "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps", "comment": "10 pages, 2 figures, preprint", "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \\textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime", "AI": {"tldr": "The paper shows that unsupervised representations can still encode and even emphasize sensitive attributes like age and income, contradicting the assumption that leaving these attributes out ensures neutrality.", "motivation": "Many practitioners assume that if sensitive attributes are simply omitted from training data, then learned representations are fair or neutral with respect to those attributes. The paper aims to challenge and empirically test this assumption in the context of unsupervised learning and representation learning.", "method": "The authors introduce and use SOMtime, a topology-preserving, high-capacity Self-Organizing Map method, to learn unsupervised embeddings on large real-world datasets while withholding sensitive attributes from the input. They then analyze whether the latent axes correlate with those withheld sensitive attributes, and compare these correlations to those obtained with other representation methods such as PCA, UMAP, t-SNE, and autoencoders. They also study demographic skews in clusters obtained from the learned embeddings.", "result": "SOMtime embeddings recover strong monotonic orderings aligned with sensitive attributes like age and income, with Spearman correlations up to 0.85, far exceeding correlations produced by PCA and UMAP (typically below 0.23, with a single case at 0.31), and t-SNE/autoencoders (at most 0.34). Unsupervised clustering of these embeddings yields demographically imbalanced groups, revealing fairness risks even in the absence of a supervised prediction task.", "conclusion": "The assumption of 'fairness through unawareness' fails even at the level of unsupervised representations: sensitive ordinal attributes can emerge as dominant latent factors despite being omitted from the training input. Therefore, fairness auditing and mitigation must encompass unsupervised representation learning and segmentation steps, not just supervised models, and practitioners cannot rely on simple attribute omission for fairness."}}
{"id": "2602.18145", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18145", "abs": "https://arxiv.org/abs/2602.18145", "authors": ["Siya Qi", "Yudong Chen", "Runcong Zhao", "Qinglin Zhu", "Zhanghao Hu", "Wei Liu", "Yulan He", "Zheng Yuan", "Lin Gui"], "title": "Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention", "comment": "25 pages, 10 figures", "summary": "Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.", "AI": {"tldr": "They propose a frequency-aware method using attention variation to detect hallucinations in LLMs, finding hallucinated tokens correspond to high-frequency attention patterns and achieving better performance than prior methods on RAG benchmarks.", "motivation": "Existing hallucination detection methods for LLMs use intrinsic signals like attention but rely on coarse, global statistics that miss fine-grained instabilities in grounding behavior. There is a need for more sensitive, lightweight detectors that capture subtle attention dynamics during generation, especially in retrieval-augmented generation (RAG) settings.", "method": "Treat the attention distribution over context tokens at each generation step as a discrete signal and analyze its frequency components using tools from signal processing. Extract high-frequency components that represent rapid, local changes in attention over time. Quantify the high-frequency attention energy associated with each generated token and use these frequency-based features to build a lightweight hallucination detector, then apply and evaluate it across different LLMs and tasks.", "result": "Empirically, hallucinated tokens tend to exhibit higher high-frequency attention energy, indicating fragmented and unstable grounding. Using these high-frequency attention features, their hallucination detector outperforms verification-based, internal-representation-based, and traditional attention-based baselines on the RAGTruth and HalluRAG benchmarks across multiple models and tasks.", "conclusion": "Viewing attention as a signal and focusing on its high-frequency components provides a fine-grained and effective indicator of hallucinations in context-based generation. This frequency-aware attention analysis yields a simple, lightweight hallucination detector that generalizes across benchmarks and surpasses several existing categories of detection methods, suggesting that attention dynamics encode rich grounding information that prior coarse metrics overlook."}}
{"id": "2602.18291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18291", "abs": "https://arxiv.org/abs/2602.18291", "authors": ["Zhuoran Li", "Hai Zhong", "Xun Wang", "Qingxin Xia", "Lihua Zhang", "Longbo Huang"], "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies", "comment": null, "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.", "AI": {"tldr": "They introduce OMAD, an online off-policy multi-agent RL framework that uses diffusion models as policies, achieving state-of-the-art performance and 2.5\u20135\u00d7 better sample efficiency on standard MARL benchmarks.", "motivation": "Online multi-agent RL requires expressive policies to coordinate agents effectively, but standard policy parameterizations struggle to capture complex, multimodal joint behaviors. Diffusion models are highly expressive and successful in offline and generative tasks, yet they are underused in online MARL because their intractable likelihood makes standard entropy-based exploration and coordination difficult.", "method": "They design OMAD, an online off-policy MARL framework that uses diffusion models as decentralized policies under a CTDE setup. The core idea is a relaxed policy objective that maximizes a scaled version of joint policy entropy without needing explicit likelihoods, enabling exploration with diffusion policies. They further introduce a joint distributional value function that incorporates entropy-augmented targets to train the diffusion policies in a coordinated way and stabilize learning across agents.", "result": "On MPE and MAMuJoCo benchmarks spanning 10 different tasks, OMAD consistently outperforms prior MARL baselines, reaching new state-of-the-art performance. It notably improves sample efficiency by roughly 2.5\u00d7 to 5\u00d7 compared with competing methods.", "conclusion": "Diffusion models can be effectively integrated as policies in online, off-policy MARL despite their intractable likelihoods by optimizing a relaxed entropy-based objective and using a joint distributional critic under CTDE. This approach substantially enhances coordination, stability, and sample efficiency, establishing diffusion-based policies as a powerful new option for online multi-agent RL."}}
{"id": "2602.18152", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.18152", "abs": "https://arxiv.org/abs/2602.18152", "authors": ["Ortal Hadad", "Edoardo Loru", "Jacopo Nudo", "Niccol\u00f2 Di Marco", "Matteo Cinelli", "Walter Quattrociocchi"], "title": "The Statistical Signature of LLMs", "comment": null, "summary": "Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.", "AI": {"tldr": "The paper uses lossless text compression to reveal statistical differences between human and LLM-generated language, showing LLM text is generally more compressible and structurally regular, though this distinction weakens at small scales.", "motivation": "Although LLMs generate text via probabilistic sampling, we lack simple, model-agnostic ways to characterize how this process alters the statistical structure of language in actual usage. Existing evaluations often rely on model internals, semantic judgments, or task-specific metrics rather than direct, surface-level statistical signatures. The authors want a unified, scalable way to compare human and machine text across contexts and to understand how generative systems reshape communication.", "method": "They propose using standard lossless compression as a proxy for statistical regularity in text, under the idea that more predictable, repetitive structures compress better. They then compare compression ratios of human vs. LLM outputs in three increasingly complex settings: (1) controlled continuations where humans and LLMs extend the same prompts; (2) a knowledge infrastructure setting comparing Wikipedia (human-produced) to a generative counterpart, Grokipedia; and (3) synthetic social environments comparing LLM-mediated Moltbook with human-driven Reddit interactions. They systematically measure and analyze compressibility differences across models, domains, and interaction scales.", "result": "Across all studied settings, LLM-generated text shows a consistent signature: it is more compressible and thus structurally more regular than human-produced text, indicating concentration in a narrower set of recurrent statistical patterns. This effect is robust across different models, tasks, and domains. However, when looking at small, fragmented interaction units (e.g., short exchanges or posts), the compressibility gap between human and LLM text narrows, implying that at small scales their surface statistical properties become harder to distinguish.", "conclusion": "Lossless compression of surface text alone can reliably differentiate probabilistic LLM generation from human language at larger scales, providing a simple, model-agnostic structural metric. LLM text tends to be more statistically regular and compressible, but this distinction diminishes in short or fragmented contexts, suggesting limits to surface-based detection. The framework offers a robust way to quantify how generative systems alter textual production and gives a structural lens on the changing complexity of human\u2013machine communication."}}
{"id": "2602.18154", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18154", "abs": "https://arxiv.org/abs/2602.18154", "authors": ["Mirae Kim", "Seonghun Jeong", "Youngjun Kwak"], "title": "FENCE: A Financial and Multimodal Jailbreak Detection Dataset", "comment": "lrec 2026 accepted paper", "summary": "Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.", "AI": {"tldr": "The paper introduces FENCE, a Korean-English multimodal dataset for detecting jailbreak attempts in financial LLM/VLM applications.", "motivation": "Existing vision-language models are vulnerable to jailbreak attacks, especially in sensitive domains like finance, yet there is a lack of specialized, multimodal, domain-specific datasets and tools for detecting such attacks.", "method": "The authors construct FENCE, a bilingual multimodal dataset of finance-relevant prompts paired with potentially harmful, image-grounded jailbreak content, and then train a baseline jailbreak detector on this dataset. They evaluate multiple commercial and open-source VLMs for vulnerability and test the trained detector on both in-distribution and external benchmarks.", "result": "Experiments show consistent vulnerabilities across VLMs, with both GPT-4o and open-source models susceptible to successful jailbreaks. The baseline detector trained on FENCE achieves 99% accuracy on in-distribution data and performs strongly on external benchmarks, demonstrating good generalization.", "conclusion": "FENCE is an effective, realistic, and robust resource for training and evaluating multimodal jailbreak detectors in financial applications, helping improve the safety and reliability of AI systems deployed in sensitive financial domains."}}
{"id": "2602.18171", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18171", "abs": "https://arxiv.org/abs/2602.18171", "authors": ["Wojciech Michaluk", "Tymoteusz Urban", "Mateusz Kubita", "Soveatin Kuntur", "Anna Wroblewska"], "title": "Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models", "comment": null, "summary": "Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.", "AI": {"tldr": "The paper proposes a hybrid clickbait detection system that combines transformer-based text embeddings with explicit linguistic features, achieving high accuracy and better interpretability.", "motivation": "Clickbait headlines harm information quality and erode user trust, and existing detection methods may either underperform or lack interpretability. The authors aim to build a more accurate and transparent model for detecting clickbait in online headlines.", "method": "They compare several representations\u2014classical vectorizers, word embeddings, and large language model (LLM) embeddings\u2014and pair them with tree-based classifiers. Their final approach uses XGBoost on top of transformer-based embeddings, augmented with 15 hand-crafted linguistic informativeness features (e.g., pronouns, superlatives, numerals, punctuation).", "result": "The best model, XGBoost over embeddings plus 15 explicit features, reaches an F1-score of 91%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt-based classification, and feature-only baselines. The added features improve both performance and interpretability.", "conclusion": "A hybrid model that fuses powerful text embeddings with a small, linguistically motivated feature set can deliver state-of-the-art clickbait detection while providing interpretable cues about what drives predictions. The authors share code and trained models to facilitate reproducibility and further research."}}
{"id": "2602.18176", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18176", "abs": "https://arxiv.org/abs/2602.18176", "authors": ["Kaisen Yang", "Jayden Teoh", "Kaicheng Yang", "Yitong Zhang", "Alex Lamb"], "title": "Improving Sampling for Masked Diffusion Models via Information Gain", "comment": "https://github.com/yks23/Information-Gain-Sampler", "summary": "Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.", "AI": {"tldr": "The paper introduces Info-Gain Sampler, a new decoding strategy for Masked Diffusion Models (MDMs) that chooses generation order by maximizing information gain rather than greedy local certainty, leading to higher quality outputs across reasoning, coding, writing, and image generation tasks.", "motivation": "Existing MDM samplers typically decode tokens by greedily selecting positions with the highest local certainty, ignoring how current decisions affect uncertainty in future steps. This underuses the non-causal property of MDMs, which allows evaluating how a chosen token affects the probability distribution of all remaining masked positions. The authors want a decoding method that actively reduces total (cumulative) uncertainty over the whole sequence instead of just optimizing each step myopically.", "method": "They analyze failure cases of greedy, certainty-based decoding in MDMs to show that local decisions can increase global uncertainty. Building on this, they design the Info-Gain Sampler, which at each step scores candidate decoding choices by combining (1) immediate uncertainty reduction at the current position and (2) expected information gain on the remaining masked tokens, measured via how token probabilities/entropies change after a hypothetical decoding decision. The sampler uses the full, non-causal joint distribution provided by an MDM to estimate these global effects and selects the next position and token that best reduce cumulative uncertainty. The approach is evaluated on multiple architectures and tasks.", "result": "Across diverse benchmarks, Info-Gain Sampler consistently improves performance compared with existing MDM samplers that rely on greedy local certainty. On reasoning tasks, it yields a 3.6% absolute improvement in average accuracy and significantly lowers cumulative uncertainty from 78.4 to 48.6, surpassing the strongest baseline. In creative writing evaluations, it attains a 63.1% win rate against comparisons. Similar gains are reported for coding and image generation, demonstrating robustness across modalities and model architectures.", "conclusion": "The paper concludes that decoding order in MDMs should be planned using global information-theoretic criteria rather than local certainty heuristics. By leveraging the non-causal nature of MDMs to explicitly account for how each decoding decision reshapes the uncertainty landscape over all remaining tokens, the Info-Gain Sampler better minimizes cumulative uncertainty and improves output quality across a range of tasks. This establishes information-gain-based planning as a principled and practical alternative to greedy decoding for masked diffusion generation."}}
{"id": "2602.18217", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18217", "abs": "https://arxiv.org/abs/2602.18217", "authors": ["Kohei Kajikawa", "Shinnosuke Isono", "Ethan Gotlieb Wilcox"], "title": "Information-Theoretic Storage Cost in Sentence Comprehension", "comment": null, "summary": "Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.", "AI": {"tldr": "Introduces an information-theoretic, continuous measure of working-memory storage cost in sentence comprehension, estimable from neural language models, and shows it predicts known processing difficulties and reading times.", "motivation": "Traditional psycholinguistic measures of memory load in sentence processing rely on symbolic grammars and assign discrete, uniform costs to syntactic predictions, which may be overly rigid, theory-dependent, and poorly aligned with graded, probabilistic language use. There is a need for a more flexible, continuous, and empirically grounded measure of processing storage cost that reflects uncertainty about future input and can be operationalized using modern computational models.", "method": "The authors define processing storage cost using information theory as the amount of information that past words contain about future context under uncertainty (essentially, how much contextual information must be maintained to anticipate upcoming input). They operationalize this quantity using pre-trained neural language models, which provide probability distributions over continuations and thus allow estimation of the relevant information-theoretic terms. They then conduct three empirical analyses in English: (1) evaluate whether this measure reproduces classic processing asymmetries in center embeddings and relative clauses; (2) compare it against a grammar-based storage-cost metric on a syntactically annotated corpus to test correspondence; and (3) test whether it predicts word-by-word reading times in two large-scale naturalistic datasets, controlling for standard information-based predictors (e.g., surprisal, frequency).", "result": "(i) The proposed information-theoretic storage-cost measure captures known asymmetries in processing difficulty for center-embedded structures and relative clauses, aligning with classic psycholinguistic findings. (ii) It shows a positive correlation with an established grammar-based storage-cost metric computed on a syntactically annotated corpus, indicating conceptual continuity despite different formal underpinnings. (iii) In mixed-effects or comparable regression models of eye-tracking/reading-time data, the new measure accounts for additional variance in reading times beyond what is explained by traditional predictors such as surprisal and other information-based variables, demonstrating incremental predictive value.", "conclusion": "Working-memory storage demands in sentence comprehension can be fruitfully reconceptualized in information-theoretic terms as the amount of information past input carries about uncertain future context. This continuous, theory-neutral measure can be estimated directly from pre-trained neural language models and not only recovers classic psycholinguistic processing patterns but also explains unique variance in naturalistic reading behavior beyond standard predictors. Thus, it provides a flexible, empirically grounded alternative to discrete grammar-based metrics and offers a bridge between cognitive theories of sentence processing and modern probabilistic language models."}}
{"id": "2602.18232", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18232", "abs": "https://arxiv.org/abs/2602.18232", "authors": ["Lexiang Tang", "Weihao Gao", "Bingchen Zhao", "Lu Ma", "Qiao jin", "Bang Yang", "Yuexian Zou"], "title": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning", "comment": null, "summary": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.", "AI": {"tldr": "They propose a decoding method (CCD) that focuses extra computation only on low-confidence tokens during reasoning, improving accuracy and shortening outputs without retraining.", "motivation": "Test-time scaling typically assumes more compute always helps, but in LLM reasoning, errors and verbosity are concentrated in a few low-confidence tokens. This motivates a method that targets these uncertain spots instead of uniformly increasing computation.", "method": "During generation, the method detects low-confidence tokens and selectively intervenes only there. It builds a contrastive reference sequence by replacing high-confidence tokens with placeholders, computes the reference distribution, and subtracts it from the main model distribution at low-confidence positions to refine predictions. This is done as a training-free, confidence-driven contrastive decoding procedure.", "result": "Across mathematical reasoning benchmarks, CCD yields significantly higher accuracy, shorter outputs, and only small additional KV-cache (memory) overhead compared to standard decoding.", "conclusion": "Targeted, confidence-driven contrastive decoding at low-confidence tokens can make LLM reasoning more reliable and concise, avoiding the computational redundancy of uniform test-time scaling, and works as a plug-and-play, training-free decoding strategy."}}
{"id": "2602.18262", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18262", "abs": "https://arxiv.org/abs/2602.18262", "authors": ["Aaron Louis Eidt", "Nils Feldhus"], "title": "Simplifying Outcomes of Language Model Component Analyses with ELIA", "comment": "EACL 2026 System Demonstrations. GitHub: https://github.com/aaron0eidt/ELIA", "summary": "While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.", "AI": {"tldr": "The paper presents ELIA, an interactive web tool that makes complex mechanistic interpretability of LLMs accessible to non-experts by combining multiple analysis methods with automatically generated natural language explanations.", "motivation": "Mechanistic interpretability techniques for LLMs are powerful but hard to use and understand for people who are not specialists. This limits their impact and keeps insights locked behind a steep expertise barrier. The authors are motivated to close this accessibility gap by making complex model analyses understandable and explorable for a wider audience, including practitioners and stakeholders who lack deep interpretability expertise.", "method": "The authors design and implement ELIA, a web application that integrates three interpretability techniques\u2014Attribution Analysis, Function Vector Analysis, and Circuit Tracing\u2014into a single interactive environment. They introduce a novel use of a vision-language model to automatically generate natural language explanations for the typically dense and technical visualizations produced by these methods. They then conduct a mixed-methods user study comparing interactive, explorable interfaces with simpler static visualizations, measuring user preferences, comprehension, and the moderating role of prior LLM experience.", "result": "The user study showed that participants preferred the interactive, explorable interfaces over static visualizations. The automatically generated natural language explanations helped users understand complex analyses, with statistical results indicating no significant correlation between prior LLM experience and comprehension scores. This implies that ELIA effectively narrows the knowledge gap between experts and non-experts. Qualitative feedback likely supported these findings by highlighting how interactivity and AI-generated narratives aided understanding.", "conclusion": "The paper concludes that AI can successfully simplify complex mechanistic interpretability outputs through automatically generated explanations, but that this is most effective when embedded in a carefully designed, user-centered interactive interface. Interactivity, specificity of information, and narrative guidance are identified as crucial design principles for making advanced model analysis techniques broadly accessible, thereby reducing barriers to comprehension across different experience levels."}}
{"id": "2602.18324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18324", "abs": "https://arxiv.org/abs/2602.18324", "authors": ["Alexandra Ciobotaru", "Ana-Maria Bucur", "Liviu P. Dinu"], "title": "PsihoRo: Depression and Anxiety Romanian Text Corpus", "comment": "This article was accepted at LREC 2026", "summary": "Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.", "AI": {"tldr": "The paper introduces PsihoRo, the first Romanian-language psychological NLP corpus focused on depression and anxiety, built from open-ended responses plus standard screening questionnaires, and provides an initial statistical and linguistic characterization of the dataset.", "motivation": "There is growing interest in using NLP to study mental health, emotions, and psychological constructs. While English has multiple psychological corpora, Romanian lacks any open-source mental health corpus, making it difficult to develop and evaluate models for detecting depression, anxiety, and emotional patterns in Romanian texts. The authors aim to fill this resource gap.", "method": "The authors collected data using a form that combined 6 open-ended questions with two standardized self-report screening instruments: PHQ-9 for depression and GAD-7 for anxiety. They compiled responses from 205 Romanian participants into a corpus named PsihoRo. They then conducted statistical analysis of the screening scores, text analysis using the Romanian adaptation of LIWC, emotion detection, and topic modeling to characterize the linguistic and psychological features of the corpus.", "result": "PsihoRo was constructed as a new Romanian-language corpus containing textual responses linked to validated depression and anxiety scores (PHQ-9 and GAD-7) from 205 participants. The analyses identified salient linguistic, emotional, and thematic patterns in the data, highlighting which features are most prominent and informative within this resource. The results demonstrate that the corpus is coherent and carries meaningful psychological signal suitable for NLP research.", "conclusion": "PsihoRo represents the first publicly introduced Romanian corpus for depression and anxiety, offering an initial but important step toward psychological NLP research in Romanian. Despite its modest size, the resource is shown to have clear and interpretable linguistic and emotional structure, laying groundwork for future models and larger-scale studies on Romanian mental health text analysis."}}
{"id": "2602.18326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18326", "abs": "https://arxiv.org/abs/2602.18326", "authors": ["Tao Wu", "Adam Kapelner"], "title": "Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning", "comment": "8 pages, 3 figures, 4 tables", "summary": "We describe a modern deep learning system that automatically identifies informative contextual examples (\\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \\qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.", "AI": {"tldr": "The paper proposes and evaluates deep learning models to automatically select high-quality example sentences (\u201ccontexts\u201d) for teaching vocabulary to high school students, showing that supervised, feature-augmented models can yield a large pool of near-perfect instructional contexts.", "motivation": "Human-created example sentences for vocabulary teaching are costly and time-consuming to produce at scale. There is a need for an automatic system that can reliably identify informative, pedagogically useful contexts for first-language vocabulary instruction, enabling large-scale, low-cost educational content generation.", "method": "The authors compare three approaches for ranking or selecting example contexts. (i) An unsupervised method that uses MPNet contextual embeddings and similarity-based heuristics to find good contexts. (ii) A supervised method based on Qwen3 embeddings fine-tuned for the instructional task, with a nonlinear regression head trained on human-labeled data. (iii) An enhanced supervised model that extends (ii) by incorporating handcrafted features of contexts (e.g., structural or lexical properties). They also propose a new evaluation and visualization metric, the Retention Competency Curve, that examines the trade-off between the fraction of good contexts discarded and the resulting ratio of good to bad contexts among the selected examples.", "result": "Among the three approaches, the feature-augmented supervised model (iii) performs best. It achieves a very high good-to-bad context ratio of 440 while discarding only 70% of the good contexts, indicating a strong ability to concentrate good examples while maintaining substantial coverage. The Retention Competency Curve provides a compact way to compare how each model balances precision (good-to-bad ratio) against recall (percentage of good contexts retained).", "conclusion": "A modern embedding-based neural model, especially when fine-tuned with human supervision and augmented with handcrafted features, can automatically select a large number of highly effective example contexts for vocabulary instruction. This yields a scalable, low-cost pipeline for generating near-perfect instructional materials across many target words, with performance conveniently summarized by the proposed Retention Competency Curve metric."}}
{"id": "2602.18346", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18346", "abs": "https://arxiv.org/abs/2602.18346", "authors": ["Pavithra PM Nair", "Preethu Rose Anish"], "title": "Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System", "comment": null, "summary": "In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.", "AI": {"tldr": "The paper presents Vichara, a framework that decomposes Indian appellate court case documents into structured decision points to both predict appellate outcomes and generate IRAC-style explanations, achieving state-of-the-art performance and high interpretability on Indian legal datasets.", "motivation": "Courts in India suffer from a massive backlog of cases, a significant portion of which are appellate matters requiring review of lower-court decisions. Existing AI judgment prediction tools often treat cases as monolithic texts and focus primarily on predicting outcomes, offering limited transparency or adaptation to the structure of Indian legal reasoning. There is a need for models that can both accurately predict appellate judgments and provide structured, interpretable explanations aligned with how Indian courts reason, to support judges and lawyers in understanding and trusting AI assistance.", "method": "The authors introduce Vichara, a framework that processes English-language appellate case documents from the Indian judiciary and decomposes them into fine-grained decision points. Each decision point captures the legal issue, deciding authority, outcome, reasoning, and temporal context, forming a structured representation. On top of this structure, Vichara generates explanations in a format inspired by the IRAC (Issue-Rule-Application-Conclusion) pattern, adapted to Indian legal practice. The framework is instantiated and evaluated using four large language models\u2014GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B\u2014on two datasets: PredEx and the expert-annotated ILDC_expert subset.", "result": "Vichara outperforms existing judgment prediction benchmarks on both evaluated datasets. Using GPT-4o mini within the framework yields the best predictive performance, with F1 scores of 81.5 on PredEx and 80.3 on ILDC_expert. Llama-3.1-8B performs second best. Human evaluators assessed the generated explanations on Clarity, Linking between facts and conclusions, and overall Usefulness, finding that GPT-4o mini\u2019s explanations are superior in interpretability compared with alternatives.", "conclusion": "The study concludes that Vichara\u2019s decision-point decomposition combined with IRAC-inspired, Indian-law-adapted explanation structure leads to both accurate and interpretable appellate judgment predictions in the Indian legal context. By exceeding existing benchmarks and producing explanations judged clear, well-linked, and useful, Vichara demonstrates that structured modeling of legal reasoning can make AI-assisted judgment prediction more trustworthy and practically valuable for legal professionals."}}
{"id": "2602.18351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18351", "abs": "https://arxiv.org/abs/2602.18351", "authors": ["Jordan Robinson", "Angus R. Williams", "Katie Atkinson", "Anthony G. Cohn"], "title": "Validating Political Position Predictions of Arguments", "comment": "13 pages, 6 figures, 6 tables. Under review", "summary": "Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \\textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $\u03b1=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($\u03b1=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.", "AI": {"tldr": "The paper proposes and validates a dual-scale (pointwise + pairwise) human evaluation framework for modeling subjective, continuous attributes like political stance, showing that language models can reliably capture ordinal structure in such domains.", "motivation": "Traditional knowledge representation relies heavily on discrete, symbolic, or categorical labels and on pairwise human validation as a gold standard for evaluation. However, many real-world attributes (e.g., political positions) are subjective, continuous, and nuanced, so strict pairwise agreement is hard to achieve and scalability is limited. Existing methods struggle to represent and validate such subjective, continuous knowledge in a way that is both reliable and scalable. The authors aim to address this gap for political stance prediction in realistic argumentative discourse.", "method": "The authors introduce a dual-scale validation framework that combines pointwise and pairwise human annotations for subjective continuous attributes. They apply it to political stance prediction on 23,228 arguments from 30 debates on the UK TV programme \"Question Time.\" They run 22 different language models to generate political position predictions for each argument, forming a large knowledge base. Pointwise evaluation measures direct agreement between individual human judgments and model outputs (using Krippendorff's alpha). Pairwise evaluation then compares the induced rankings of arguments (from both humans and models) to assess ordinal alignment, again via inter-annotator agreement metrics. They also structure the argument data so it can support graph-based reasoning and retrieval-augmented generation.", "result": "Pointwise evaluation yields moderate agreement between human annotators and model predictions (Krippendorff's \u03b1 = 0.578), consistent with the intrinsic subjectivity and continuity of political stance. However, when comparing rankings derived from human vs. model predictions in a pairwise (ordinal) framework, the best model achieves substantially higher alignment (\u03b1 = 0.86). The authors thus obtain a large, validated knowledge base of model-predicted political positions for over 23k arguments, with evidence that the models capture robust ordinal structure even where exact pointwise labels are noisy or contested.", "conclusion": "The dual-scale framework offers a practical, scalable validation methodology for subjective, continuous knowledge that maintains reliability by leveraging both pointwise and pairwise assessments. The resulting structured argumentation knowledge base supports graph-based reasoning and retrieval-augmented generation in political domains. More broadly, the findings show that language models can extract consistent ordinal structure from inherently subjective discourse, extending knowledge representation beyond traditional symbolic or categorical schemes and opening up new possibilities for representing nuanced, continuous attributes in real-world settings."}}
{"id": "2602.18425", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18425", "abs": "https://arxiv.org/abs/2602.18425", "authors": ["Deniz Qian", "Hung-Ting Chen", "Eunsol Choi"], "title": "RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering", "comment": "18 pages, 12 figures, 12 tables", "summary": "Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.", "AI": {"tldr": "The paper proposes an iterative retrieve-verify-retrieve (RVR) framework that repeatedly retrieves and filters documents to maximize coverage of all valid answers to a query.", "motivation": "Many real-world queries have multiple valid answers, but standard retrieval systems focus on top-ranked, most obvious results, missing less prominent but still correct answers. There is a need for retrieval methods that improve completeness (recall of all answers) rather than just precision or a single best answer, and that can work with existing retrievers while being adaptable to multi-answer scenarios.", "method": "RVR runs retrieval in multiple rounds. In the first round, an off-the-shelf retriever takes the original query and returns a candidate document set. A verifier module then selects a high-quality subset of these documents. In subsequent rounds, the system augments the query with information from the previously verified documents and performs retrieval again, aiming to surface new, uncovered answers. They also fine-tune retrievers specifically for this iterative, verifier-guided inference setup to further boost performance. The framework is compared against strong baselines, including agentic search methods, and tested across multiple datasets and base retrievers.", "result": "On the multi-answer retrieval dataset QAMPARI, RVR achieves at least a 10% relative and 3% absolute improvement in complete recall percentage over baselines. The method also yields consistent improvements on two out-of-domain datasets, QUEST and WebQuestionsSP, regardless of which base retriever is used. These gains hold even when using off-the-shelf retrievers, with additional improvements from fine-tuning under the RVR regime.", "conclusion": "Iteratively combining retrieval with verification and using verified documents to guide subsequent rounds leads to more comprehensive coverage of all valid answers to a query. The RVR framework is flexible (works with off-the-shelf retrievers), benefits from task-specific fine-tuning, and outperforms existing approaches including agentic search, suggesting that verifier-guided, multi-round retrieval is a promising direction for improving answer recall in multi-answer question settings."}}
{"id": "2602.18429", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18429", "abs": "https://arxiv.org/abs/2602.18429", "authors": ["Harshul Raj Surana", "Arijit Maji", "Aryan Vats", "Akash Ghosh", "Sriparna Saha", "Amit Sheth"], "title": "VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.", "AI": {"tldr": "The paper introduces VIRAASAT, a semi-automated multi-hop QA dataset focused on Indian cultural knowledge, and SCoM, a symbolic reasoning framework that improves LLM performance on this dataset.", "motivation": "LLMs perform poorly on tasks that require deep socio-cultural knowledge and multi-hop reasoning in local contexts like Indian culture. Existing cultural benchmarks are manually crafted, single-hop, and expensive to scale, so they fail to adequately measure or address these weaknesses.", "method": "The authors build VIRAASAT, a multi-hop QA dataset grounded in a knowledge graph of 700+ expert-curated Indian cultural artifacts, spanning 13 attributes across all Indian states and union territories. They generate over 3,200 multi-hop questions that require chained cultural reasoning. They then propose SCoM (Symbolic Chain-of-Manipulation), a framework that trains models to internally simulate atomic knowledge-graph manipulations and reliably traverse the graph topology, contrasting with standard Chain-of-Thought training.", "result": "On supervised fine-tuning experiments, models trained with SCoM outperform standard Chain-of-Thought baselines by up to 20% on the VIRAASAT benchmark, revealing that symbolic graph manipulation helps ground and synthesize low-probability cultural facts better than conventional CoT.", "conclusion": "VIRAASAT provides a scalable, culturally rich multi-hop benchmark for Indian culture, exposing significant limitations of current LLMs in cultural reasoning. SCoM offers a more effective training approach than standard CoT for such tasks by aligning model reasoning with explicit knowledge-graph structure. The released dataset and method aim to support the development of more culturally aware reasoning models."}}
