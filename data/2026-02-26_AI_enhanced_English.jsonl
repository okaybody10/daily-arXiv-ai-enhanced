{"id": "2602.21268", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21268", "abs": "https://arxiv.org/abs/2602.21268", "authors": ["Takaaki Fujita", "Florentin Smarandache"], "title": "A Dynamic Survey of Soft Set Theory and Its Extensions", "comment": "Book.143 pages. Publisher: Neutrosophic Science International Association (NSIA) Publishing House. ISBN: 978-1-59973-859-8", "summary": "Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.", "AI": {"tldr": "Survey of soft set theory and its extensions for structured uncertainty modeling in decision problems.", "motivation": "Provide a unified, survey-style account of soft set theory and its many variants, clarifying how they model parameterized uncertainty and support decision making.", "method": "Conceptual and structural survey: present core definitions, compare and organize existing variants (hypersoft, superhypersoft, TreeSoft, bipolar, dynamic soft sets), and relate them to other mathematical domains like topology and matroid theory.", "result": "A synthesized overview of the landscape of soft set theory and its extensions, including representative constructions and links to other fields.", "conclusion": "Soft sets and their extensions form a rich, flexible framework for handling parameterized uncertainty, with deep connections to other mathematical structures and active directions for further research."}}
{"id": "2602.21351", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21351", "abs": "https://arxiv.org/abs/2602.21351", "authors": ["Dmitrii Pantiukhin", "Ivan Kuznetsov", "Boris Shapkin", "Antonia Anna Jost", "Thomas Jung", "Nikolay Koldunov"], "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives", "comment": "20 pages, 6 figures, 7 tables, supplementary material included", "summary": "The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.", "AI": {"tldr": "The paper introduces PANGAEA-GPT, a hierarchical multi-agent LLM framework that autonomously discovers and analyzes heterogeneous Earth science data from repositories like PANGAEA using coordinated agent workflows.", "motivation": "Earth science repositories accumulate vast amounts of heterogeneous data, but citation metrics show much of it is underused, indicating poor data discoverability and limited reusability. Existing LLM-based tools are not well-structured for scalable, reliable, and type-aware interaction with such complex repositories, motivating a more robust, autonomous framework.", "method": "The authors design a hierarchical multi-agent system with a centralized Supervisor-Worker architecture. The Supervisor routes tasks to specialized Worker agents based on data type, manages a strict data-type-aware routing mechanism, and coordinates multi-step workflows. Workers operate in sandboxed environments with deterministic code execution and leverage self-correction based on runtime feedback, allowing them to detect and resolve execution errors during data discovery and analysis tasks.", "result": "In use cases drawn from physical oceanography and ecology, the system autonomously executes complex, multi-step analytical workflows on PANGAEA datasets with minimal human intervention, successfully discovering, querying, and analyzing heterogeneous data while handling runtime errors through its feedback mechanisms.", "conclusion": "PANGAEA-GPT shows that a hierarchical multi-agent LLM architecture with supervised routing and sandboxed, self-correcting code execution can significantly improve autonomous discovery and analysis of large, heterogeneous Earth science repositories, offering a generalizable methodology for enhancing data reusability via coordinated agent workflows."}}
{"id": "2602.21496", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21496", "abs": "https://arxiv.org/abs/2602.21496", "authors": ["Umid Suleymanov", "Zaur Rajabov", "Emil Mirzazada", "Murat Kantarcioglu"], "title": "Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information", "comment": "Under Review", "summary": "While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regulate these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question. To address this, we introduce SemSIEdit, an inference-time framework where an agentic \"Editor\" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. Our analysis reveals a Privacy-Utility Pareto Frontier, where this agentic rewriting reduces leakage by 34.6% across all three SemSI categories while incurring a marginal utility loss of 9.8%. We also uncover a Scale-Dependent Safety Divergence: large reasoning models (e.g., GPT-5) achieve safety through constructive expansion (adding nuance), whereas capacity-constrained models revert to destructive truncation (deleting text). Finally, we identify a Reasoning Paradox: while inference-time reasoning increases baseline risk by enabling the model to make deeper sensitive inferences, it simultaneously empowers the defense to execute safe rewrites.", "AI": {"tldr": "The paper presents SemSIEdit, an inference-time editing framework that reduces semantic sensitive information leakage from LLMs while preserving utility.", "motivation": "Existing defenses handle structured PII but not semantic sensitive information (SemSI), such as inferred identity attributes, reputation-harming content, or hallucinated personal claims. It is unclear whether LLMs can self-regulate these subtle, context-dependent leaks without overly sacrificing usefulness, motivating a more nuanced, narrative-preserving defense mechanism.", "method": "The authors design SemSIEdit, an agentic \u201cEditor\u201d that runs at inference time. This Editor iteratively critiques model outputs, detects sensitive spans, and rewrites them in place to maintain narrative coherence instead of refusing to answer or heavily censoring. They evaluate SemSIEdit across three SemSI categories and study the trade-off between privacy protection and utility, as well as behavior differences across model scales and reasoning settings.", "result": "SemSIEdit reduces semantic sensitive information leakage by 34.6% across all three SemSI categories, with only a 9.8% drop in utility. The authors observe a Privacy-Utility Pareto Frontier, showing systematic trade-offs. They also find a Scale-Dependent Safety Divergence: larger reasoning models (e.g., GPT-5) achieve safer outputs via constructive expansion (adding nuance), while smaller models tend to use destructive truncation (removing text). In addition, they document that inference-time reasoning both increases baseline risk of sensitive inference and simultaneously enhances the effectiveness of the defensive rewriting.", "conclusion": "Inference-time agentic editing offers a practical way to mitigate semantic sensitive information leakage while preserving most of the utility of LLM outputs. Model scale and reasoning behavior critically shape how safety is achieved, with larger models enabling more constructive, nuance-adding defenses. Although reasoning raises the potential for deeper sensitive inferences, it also equips defenses like SemSIEdit to perform more effective safe rewrites, highlighting a nuanced privacy-utility-safety trade space for future LLM safety research."}}
{"id": "2602.21534", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21534", "abs": "https://arxiv.org/abs/2602.21534", "authors": ["Xiaoxuan Wang", "Han Zhang", "Haixin Wang", "Yidan Shi", "Ruoyan Li", "Kaiqiao Han", "Chenyi Tong", "Haoran Deng", "Renliang Sun", "Alexander Taylor", "Yanqiao Zhu", "Jason Cong", "Yizhou Sun", "Wei Wang"], "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning", "comment": null, "summary": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.", "AI": {"tldr": "The paper identifies and addresses instability in agentic reinforcement learning (ARL) by introducing a standardized benchmark (ARLArena), analyzing core policy gradient design dimensions, and proposing SAMPO, a stable policy optimization method that improves robustness and performance in LLM-based agent training.", "motivation": "ARL is promising for training agents on complex, multi-step interactive tasks, but current methods are highly unstable and prone to training collapse. This instability prevents scaling to larger environments and longer horizons, and makes it hard to systematically explore and compare algorithmic design choices. The paper is motivated by the need for a stable, reproducible framework and clearer understanding of what drives instability in ARL, so that practical, reliable training pipelines for LLM-based agents can be built.", "method": "1) Propose ARLArena, a clean and standardized testbed plus analysis framework for ARL. 2) Decompose policy gradient methods for ARL into four core design dimensions (e.g., likely variants of objectives, credit assignment, bootstrap/advantage estimation, exploration/regularization, etc.) and systematically evaluate how each dimension affects performance and stability under controlled settings. 3) From this analysis, identify dominant sources of instability and synthesize them into a unified policy-gradient-based view of ARL. 4) Based on these insights, design SAMPO, a stable agentic policy optimization algorithm that modifies or constrains the key policy gradient components to reduce training collapse while maintaining or improving performance.", "result": "The analysis in ARLArena reveals which policy gradient design dimensions most strongly influence instability in ARL. Using these insights, the proposed SAMPO algorithm delivers consistently stable training dynamics and strong task performance across a range of agentic benchmarks. Empirical evaluations show that SAMPO avoids the training collapse commonly observed in previous ARL setups and yields robust results across diverse interactive tasks.", "conclusion": "By introducing ARLArena and conducting a fine-grained decomposition of policy gradient components, the paper provides a unified perspective on instability in agentic reinforcement learning. The proposed SAMPO algorithm effectively mitigates major sources of training instability, enabling more reliable and scalable ARL. Overall, the work offers both conceptual clarity about ARL\u2019s failure modes and practical guidance for building stable, reproducible LLM-based agent training pipelines."}}
{"id": "2602.21212", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21212", "abs": "https://arxiv.org/abs/2602.21212", "authors": ["Takato Yasuno"], "title": "Disaster Question Answering with LoRA Efficiency and Accurate End Position", "comment": "12 pages, 5 figures", "summary": "Natural disasters such as earthquakes, torrential rainfall, floods, and volcanic eruptions occur with extremely low frequency and affect limited geographic areas. When individuals face disaster situations, they often experience confusion and lack the domain-specific knowledge and experience necessary to determine appropriate responses and actions. While disaster information is continuously updated, even when utilizing RAG search and large language models for inquiries, obtaining relevant domain knowledge about natural disasters and experiences similar to one's specific situation is not guaranteed. When hallucinations are included in disaster question answering, artificial misinformation may spread and exacerbate confusion. This work introduces a disaster-focused question answering system based on Japanese disaster situations and response experiences. Utilizing the cl-tohoku/bert-base-japanese-v3 + Bi-LSTM + Enhanced Position Heads architecture with LoRA efficiency optimization, we achieved 70.4\\% End Position accuracy with only 5.7\\% of the total parameters (6.7M/117M). Experimental results demonstrate that the combination of Japanese BERT-base optimization and Bi-LSTM contextual understanding achieves accuracy levels suitable for real disaster response scenarios, attaining a 0.885 Span F1 score. Future challenges include: establishing natural disaster Q\\&A benchmark datasets, fine-tuning foundation models with disaster knowledge, developing lightweight and power-efficient edge AI Disaster Q\\&A applications for situations with insufficient power and communication during disasters, and addressing disaster knowledge base updates and continual learning capabilities.", "AI": {"tldr": "The paper proposes a Japanese disaster-focused question answering (QA) system that reduces hallucinations and improves reliability during natural disasters, achieving high span-level accuracy with a lightweight BERT+Bi-LSTM model optimized via LoRA.", "motivation": "During natural disasters, people are confused and often lack the specialized knowledge needed to decide what to do. General-purpose RAG and large language models may fail to find highly relevant, situation-specific disaster knowledge and can hallucinate, spreading artificial misinformation that worsens confusion. There is a need for a domain-specialized, accurate, and efficient QA system tailored to Japanese disaster contexts and response experiences, especially usable under constrained power and communication conditions.", "method": "The authors design a disaster-focused QA model specialized for Japanese by combining the pre-trained cl-tohoku/bert-base-japanese-v3 encoder with a Bi-LSTM layer and Enhanced Position Heads for answer span prediction. They apply LoRA-based parameter-efficient fine-tuning so that only 5.7% of the total parameters (6.7M out of 117M) are updated, making the model more lightweight while retaining capacity. The system is trained and evaluated on Japanese disaster situation and response experience data, focusing on span extraction metrics such as end position accuracy and span F1.", "result": "The proposed architecture achieves 70.4% End Position accuracy and a Span F1 score of 0.885, while requiring updates to only 5.7% of the parameters (6.7M/117M). These results indicate that optimizing Japanese BERT-base with a Bi-LSTM layer for contextual understanding delivers accuracy suitable for real-world disaster response QA while remaining parameter-efficient.", "conclusion": "The specialized Japanese disaster QA system can provide accurate, span-based answers suitable for real disaster response scenarios with a lightweight, LoRA-optimized BERT+Bi-LSTM architecture. The work suggests that domain specialization and parameter-efficient tuning are effective for safety-critical QA. Future work includes: creating standardized natural disaster QA benchmarks; fine-tuning larger foundation models with disaster knowledge; developing lightweight, power-efficient edge AI QA applications for low-power, low-connectivity disaster environments; and handling continual updates to the disaster knowledge base to maintain up-to-date performance."}}
{"id": "2602.21556", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.21556", "abs": "https://arxiv.org/abs/2602.21556", "authors": ["Nivasini Ananthakrishnan", "Meena Jagadeesan"], "title": "Power and Limitations of Aggregation in Compound AI Systems", "comment": null, "summary": "When designing compound AI systems, a common approach is to query multiple copies of the same model and aggregate the responses to produce a synthesized output. Given the homogeneity of these models, this raises the question of whether aggregation unlocks access to a greater set of outputs than querying a single model. In this work, we investigate the power and limitations of aggregation within a stylized principal-agent framework. This framework models how the system designer can partially steer each agent's output through its reward function specification, but still faces limitations due to prompt engineering ability and model capabilities. Our analysis uncovers three natural mechanisms -- feasibility expansion, support expansion, and binding set contraction -- through which aggregation expands the set of outputs that are elicitable by the system designer. We prove that any aggregation operation must implement one of these mechanisms in order to be elicitability-expanding, and that strengthened versions of these mechanisms provide necessary and sufficient conditions that fully characterize elicitability-expansion. Finally, we provide an empirical illustration of our findings for LLMs deployed in a toy reference-generation task. Altogether, our results take a step towards characterizing when compound AI systems can overcome limitations in model capabilities and in prompt engineering.", "AI": {"tldr": "The paper studies when aggregating outputs from multiple identical AI models can expand the set of outputs a system designer can elicit, using a principal\u2013agent framework, and characterizes the mechanisms by which such aggregation helps.", "motivation": "Compound AI systems often query several copies of the same model and aggregate their responses, but it is unclear in theory whether this actually lets designers obtain outputs they could not get from a single model, given limitations in prompts and model capabilities. The paper aims to formally understand and characterize the benefits and boundaries of such aggregation.", "method": "The authors use a stylized principal\u2013agent framework where the system designer (principal) influences each model copy (agent) via reward functions, subject to constraints from prompt engineering and model capability. Within this framework, they analyze and formally define mechanisms\u2014feasibility expansion, support expansion, and binding set contraction\u2014by which aggregation can expand the set of elicitable outputs. They prove theorems giving necessary and sufficient conditions for aggregation to be elicitability-expanding, then illustrate these ideas empirically with a toy reference-generation task using LLMs.", "result": "They identify three core mechanisms through which aggregation of identical models can expand the set of elicitable outputs: (1) feasibility expansion (making otherwise infeasible outputs attainable), (2) support expansion (enlarging the support of possible outputs), and (3) binding set contraction (relaxing binding constraints on outputs). They show that any aggregation operation that expands elicitability must realize at least one of these mechanisms, and that stronger versions of these mechanisms exactly characterize when aggregation is elicitability-expanding. Empirical experiments on a toy LLM reference-generation task qualitatively validate the theory.", "conclusion": "Aggregation of multiple identical AI models is not automatically beneficial; its value can be precisely understood through three mechanisms that expand what outputs a system designer can elicit. The paper offers a theoretical characterization of when compound AI systems can overcome limitations of individual models and imperfect prompt engineering, and supports this with a simple empirical demonstration, providing guidance for designing more effective multi-model systems."}}
{"id": "2602.21215", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21215", "abs": "https://arxiv.org/abs/2602.21215", "authors": ["Runyi Hu", "Jie Zhang", "Shiqian Zhao", "Jiale Meng", "Jiwei Li", "Jason Zeng", "Ming Wu", "Michael Heinrich", "Yonggang Wen", "Tianwei Zhang"], "title": "Inference-time Alignment via Sparse Junction Steering", "comment": "28 pages, 17 figures", "summary": "Token-level steering has emerged as a pivotal approach for inference-time alignment, enabling fine grained control over large language models by modulating their output distributions without parameter updates. While effective, existing methods rely on dense intervention at every decoding step. This persistent manipulation not only incurs substantial computational overhead but also risks compromising generation quality by excessively drifting from the model's intrinsic distribution. In this work, we show that dense intervention is unnecessary and propose Sparse Inference time Alignment (SIA), which performs sparse junction steering by intervening only at critical decision points along the generation trajectory. Our key insight is that high entropy junctions mark pivotal decision points in the generation trajectory and are particularly susceptible to misalignment, indicating the need to introduce alignment related reward signals at these points. Extensive experiments across different model families and alignment objectives show that steering only 20% to 80% of tokens achieves superior alignment-efficiency trade offs. For strong base models such as Qwen3, intervening on as few as 20% of tokens matches or even surpasses heavily post-trained instruct models. This sparsity enables stronger guidance while better preserving the model's native distribution, integrates seamlessly with search based methods such as Best-of-N, and reduces computational cost by up to 6x.", "AI": {"tldr": "The paper proposes Sparse Inference-time Alignment (SIA), a method that steers large language models only at critical, high-entropy decision points during generation instead of at every token, achieving better alignment-efficiency trade-offs with lower computational cost and less deviation from the base model\u2019s distribution.", "motivation": "Existing token-level steering techniques for aligning large language models require dense, step-by-step intervention at every decoding step. This incurs large computational overhead and can degrade generation quality by forcing the model too far from its intrinsic distribution. The authors aim to reduce the cost and side effects of inference-time alignment while maintaining or improving alignment quality.", "method": "The authors introduce Sparse Inference-time Alignment (SIA), which identifies high-entropy tokens in the generation trajectory as \u201cjunctions\u201d or critical decision points. Instead of modifying the output distribution at every step, SIA applies steering only at these junction tokens, injecting alignment-related reward signals locally. They also investigate how SIA integrates with search-based decoding methods like Best-of-N and evaluate different sparsity levels (e.g., steering 20\u201380% of tokens).", "result": "Experiments across multiple LLM families and various alignment objectives show that intervening on only a subset (20\u201380%) of tokens can outperform dense steering methods in terms of the trade-off between alignment quality and computational cost. On strong base models such as Qwen3, steering as little as 20% of tokens can match or surpass the performance of heavily post-trained instruct models. The method also provides up to a 6x reduction in computation and is compatible with search-based methods like Best-of-N.", "conclusion": "Dense, per-token intervention is not necessary for effective inference-time alignment. By sparsely steering only at high-entropy, critical decision points, SIA maintains or improves alignment while significantly reducing computational overhead and better preserving the base model\u2019s native distribution. This sparse strategy enables stronger yet more efficient guidance and integrates well with existing decoding strategies."}}
{"id": "2602.21745", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.21745", "abs": "https://arxiv.org/abs/2602.21745", "authors": ["Hyo Jin Kim"], "title": "The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems", "comment": "13 pages, 5 figures. Version 1. Includes recursive feedback extension and simulation results. Data available via DOI: 10.5281/zenodo.18754266", "summary": "We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.\n  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.\n  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.", "AI": {"tldr": "The paper proposes a formal, dynamical model (ASIR Courage Model) that treats truth-telling as a transition between suppressed and expressed states driven by competing forces, applicable to both humans and AI under constraints.", "motivation": "Existing discussions of courage, honesty, and alignment often treat truthfulness as a stable trait or static policy. The authors want a structural, quantitative way to explain when and why agents (humans or AI) shift from silence/distortion to disclosure under risk, particularly in asymmetric or high-stakes situations.", "method": "They construct a phase-dynamic model with two principal states: suppression (S0) and expression (S1). The transition is governed by an inequality: \u03bb(1+\u03b3)+\u03c8 > \u03b8+\u03c6, where \u03bb is baseline openness, \u03b3 is relational amplification, \u03c8 is accumulated internal/structural pressure, and \u03b8 and \u03c6 are inhibitory thresholds and transition costs. They then generalize this architecture from human psychology to AI systems, interpreting policy constraints, alignment filters, and multi-objective tensions as analogous forces in the same phase space. A feedback extension is added so that each transition outcome updates the parameters, creating path-dependent dynamics over repeated interactions.", "result": "The model yields a unified formalism in which both human silence under social or personal pressure and AI constrained or distorted outputs emerge from the same type of inequality-driven state transition. It explains how varying contextual, relational, and structural pressures can tip systems from suppression to expression and how repeated interactions change future thresholds and pressures, leading to divergence in behavior over time.", "conclusion": "Truth-disclosure under risk can be modeled as a dynamical state transition driven by interacting forces, not as a fixed trait or intention. By treating human courage and AI alignment within a shared geometric/phase-space framework, the ASIR Courage Model offers a structural explanation for when agents disclose or suppress information and how their behavior evolves across repeated, constrained interactions."}}
{"id": "2602.21216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21216", "abs": "https://arxiv.org/abs/2602.21216", "authors": ["Zhyar Rzgar K Rostam", "G\u00e1bor Kert\u00e9sz"], "title": "EQ-5D Classification Using Biomedical Entity-Enriched Pre-trained Language Models and Multiple Instance Learning", "comment": "12 tables", "summary": "The EQ-5D (EuroQol 5-Dimensions) is a standardized instrument for the evaluation of health-related quality of life. In health economics, systematic literature reviews (SLRs) depend on the correct identification of publications that use the EQ-5D, but manual screening of large volumes of scientific literature is time-consuming, error-prone, and inconsistent. In this study, we investigate fine-tuning of general-purpose (BERT) and domain-specific (SciBERT, BioBERT) pre-trained language models (PLMs), enriched with biomedical entity information extracted through scispaCy models for each statement, to improve EQ-5D detection from abstracts. We conduct nine experimental setups, including combining three scispaCy models with three PLMs, and evaluate their performance at both the sentence and study levels. Furthermore, we explore a Multiple Instance Learning (MIL) approach with attention pooling to aggregate sentence-level information into study-level predictions, where each abstract is represented as a bag of enriched sentences (by scispaCy). The findings indicate consistent improvements in F1-scores (reaching 0.82) and nearly perfect recall at the study-level, significantly exceeding classical bag-of-words baselines and recently reported PLM baselines. These results show that entity enrichment significantly improves domain adaptation and model generalization, enabling more accurate automated screening in systematic reviews.", "AI": {"tldr": "They fine-tune general and biomedical pre-trained language models, enriched with entities extracted by scispaCy, to automatically detect EQ-5D usage in abstracts for systematic reviews, achieving high F1 and recall and outperforming traditional and recent baselines.", "motivation": "Systematic literature reviews in health economics need to find all studies that use the EQ-5D measure of health-related quality of life. Manually screening large numbers of abstracts is slow, inconsistent, and error-prone, and existing automatic methods are limited. The authors want to improve automatic EQ-5D detection to make screening more accurate and efficient.", "method": "They fine-tune three pre-trained language models\u2014BERT (general), SciBERT and BioBERT (domain-specific)\u2014to classify EQ-5D usage. Each sentence in an abstract is enriched with biomedical entities extracted by three different scispaCy models, producing nine combinations of PLM + entity-enrichment model. They evaluate performance at both sentence and study (abstract) levels. To aggregate sentence-level information into a study-level decision, they apply a Multiple Instance Learning framework with attention pooling, where each abstract is treated as a bag of enriched sentences.", "result": "Entity-enriched PLM models consistently outperform classical bag-of-words and previously reported PLM baselines. At the study level, they achieve F1-scores up to 0.82 with nearly perfect recall, indicating very few EQ-5D studies are missed while maintaining good precision.", "conclusion": "Adding biomedical entity information to pre-trained language models substantially improves EQ-5D detection in abstracts. The combination of entity enrichment and MIL-based attention pooling enhances domain adaptation and generalization, enabling more accurate and reliable automated screening for systematic literature reviews in health economics."}}
{"id": "2602.21746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21746", "abs": "https://arxiv.org/abs/2602.21746", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation", "comment": null, "summary": "In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.", "AI": {"tldr": "The paper extends a prior fuzzy-logic-based ethical decision-making architecture (fEDM) into fEDM+ by adding explainability and pluralistic validation, making it more interpretable and robust to multiple stakeholder perspectives while preserving formal verifiability.", "motivation": "The original fEDM ensured formal soundness and consistent ethical decisions using fuzzy logic and Fuzzy Petri Nets, but it lacked principled explainability (clear reasons grounded in explicit moral principles) and could not adequately handle ethical pluralism, i.e., differing stakeholder values and priorities. The authors aim to address these gaps so that ethical AI systems can be both transparent and robust across diverse value frameworks.", "method": "The authors extend the fEDM architecture in two main ways. (1) They add an Explainability and Traceability Module (ETM) that connects each ethical decision rule to explicit moral principles and computes a weighted contribution of each principle to the recommended action, yielding traceable explanations. (2) They replace single-normative-referent validation with a pluralistic semantic validation framework that evaluates each decision against multiple stakeholder referents, each encoding distinct priorities over principles and different risk tolerances. The framework is still grounded in fuzzy logic and supports formal verification through Fuzzy Petri Nets.", "result": "The resulting extended framework, fEDM+, can generate transparent, auditable explanations indicating which moral principles contributed, and to what extent, to each decision. It can also represent and evaluate principled disagreement among stakeholder value systems instead of collapsing them into a single norm, thereby increasing robustness and contextual sensitivity of ethical decisions. Formal verifiability of the underlying reasoning process is preserved.", "conclusion": "fEDM+ provides an ethically richer and more practically useful decision-making layer for AI systems by combining fuzzy-logic-based risk assessment with explicit principle-based explainability and multi-stakeholder validation. This makes the framework suitable for use as an oversight and governance layer in ethically sensitive AI applications, where interpretability, accountability, and responsiveness to plural values are required."}}
{"id": "2602.21217", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.21217", "abs": "https://arxiv.org/abs/2602.21217", "authors": ["S M Ruhul Alam", "Rifa Ferzana"], "title": "Applied Sociolinguistic AI for Community Development (ASA-CD): A New Scientific Paradigm for Linguistically-Grounded Social Intervention", "comment": "13 pages, 2 figures, 3 tables; simulation-based study introducing the ASA-CD framework", "summary": "This paper establishes Applied Sociolinguistic AI for Community Development (ASA-CD) as a novel scientific paradigm for addressing community challenges through linguistically grounded, AI-enabled intervention. ASA-CD introduces three key contributions: (1) linguistic biomarkers as computational indicators of discursive fragmentation; (2) development-aligned natural language processing (NLP), an AI optimisation paradigm prioritising collective outcomes; and (3) a standardised five-phase protocol for discursive intervention. A proof-of-concept study, incorporating real-world and synthetic corpora, demonstrates systematic associations between exclusionary language and negative sentiment and simulates intervention-based improvements. ASA-CD provides a unified methodological, ethical and empirical framework for scalable, value-aligned AI in the service of community empowerment.", "AI": {"tldr": "Introduces ASA-CD, an AI paradigm using sociolinguistics to diagnose and improve community discourse.", "motivation": "Communities face discursive fragmentation and exclusionary language that harm cohesion and development, while current AI/NLP systems are not explicitly optimised for collective, community-level outcomes or ethically grounded, scalable interventions.", "method": "Defines ASA-CD as a paradigm with three pillars: (1) linguistic biomarkers that computationally detect discursive fragmentation; (2) development-aligned NLP, which reframes AI optimisation objectives around collective developmental goals; and (3) a five-phase, standardised protocol for discursive intervention. Uses a proof-of-concept combining real-world and synthetic language corpora to detect links between exclusionary language and negative sentiment and to simulate changes under intervention scenarios.", "result": "Finds systematic associations between exclusionary language and negative sentiment in the examined corpora, and shows via simulations that discursive interventions guided by ASA-CD could improve these linguistic indicators.", "conclusion": "ASA-CD offers a unified, ethically aware and empirically grounded framework for using sociolinguistically informed NLP to design, simulate and scale AI interventions that support community empowerment and development-focused outcomes."}}
{"id": "2602.21814", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21814", "abs": "https://arxiv.org/abs/2602.21814", "authors": ["Heejin Jo"], "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem", "comment": "9 pages, 4 tables", "summary": "Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.", "AI": {"tldr": "The paper studies why large language models fail a physical reasoning benchmark (the 'car wash problem') and shows that a structured reasoning prompt (STAR: Situation-Task-Action-Result) dramatically improves accuracy, more than adding extra context.", "motivation": "Large language models often fail at problems requiring implicit physical constraint reasoning, such as the 'car wash problem'. Many production systems try to fix this via more context (RAG, user profiles), but it is unclear whether structured reasoning prompts or extra context matter more. The authors want to isolate which components of a prompt architecture actually drive correct reasoning in such tasks.", "method": "They run a variable isolation study using Claude 3.5 Sonnet on the car wash benchmark under six different prompting conditions. Each condition has n=20 trials (120 total). Hyperparameters are controlled (temperature 0.7, top_p 1.0). They systematically vary: (1) baseline prompt, (2) STAR reasoning framework, (3) STAR + user profile context retrieved via a vector database, (4) STAR + RAG context, and combinations, to see how each layer affects accuracy. They use Fisher\u2019s exact test to assess significance and report odds ratios.", "result": "With a baseline prompt, accuracy is 0%. Introducing only the STAR (Situation-Task-Action-Result) reasoning framework increases accuracy to 85%, a statistically significant jump (p=0.001, Fisher\u2019s exact test; odds ratio 13.22). Adding user profile context via vector retrieval provides an additional 10 percentage point increase (to 95%), and adding RAG context contributes another 5 percentage points, reaching 100% accuracy in the full-stack configuration.", "conclusion": "The main driver of improved performance on the car wash physical reasoning benchmark is structured reasoning scaffolding, not extra contextual information. Specifically, forcing the model to articulate goals and structure its reasoning (via the STAR framework) is far more impactful than adding user profile or RAG-derived context, though these still deliver marginal gains. For implicit constraint reasoning tasks, designers should prioritize robust reasoning frameworks in prompts before focusing on more complex context injection mechanisms."}}
{"id": "2602.21218", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21218", "abs": "https://arxiv.org/abs/2602.21218", "authors": ["Amin Banayeeanzade", "Qingchuan Yang", "Deqing Fu", "Spencer Hong", "Erin Babinsky", "Alfy Samuel", "Anoop Kumar", "Robin Jia", "Sai Praneeth Karimireddy"], "title": "EPSVec: Efficient and Private Synthetic Data Generation via Dataset Vectors", "comment": null, "summary": "High-quality data is essential for modern machine learning, yet many valuable corpora are sensitive and cannot be freely shared. Synthetic data offers a practical substitute for downstream development, and large language models (LLMs) have emerged as powerful engines for generating it. However, existing private text generation methods are severely inefficient: they are data-intensive, computationally slow, and often require large private corpora or batch sizes to achieve usable quality. We introduce EPSVec, a differentially-private lightweight alternative that steers LLM generation using *dataset vectors*--directions in activation space that capture the distributional gap between private data and public priors. EPSVec extracts and sanitizes steering vectors just once and then performs standard decoding. This decouples the privacy budget from generation, enabling arbitrarily many synthetic samples without additional privacy cost and yielding strong fidelity even in low-data regimes. Furthermore, we enhance our method by utilizing pretrained (base) models and introducing fixed-shot prompting to boost generation diversity and fidelity. Our experiments demonstrate that EPSVec outperforms existing baselines in distributional alignment and downstream utility, particularly in low-data regimes, while significantly reducing computational overhead.", "AI": {"tldr": "The paper proposes EPSVec, a differentially private, vector-based steering method for large language models to generate high-fidelity synthetic text from sensitive datasets with low computational and data requirements.", "motivation": "Sharing real-world text data is often impossible due to privacy constraints, yet high-quality labeled text is crucial for training and evaluating machine learning models. Existing differentially private text generation approaches are inefficient, needing large private corpora, big batch sizes, or expensive computation, and they tightly couple privacy cost to the number of generated samples. The authors aim to develop a method that can leverage sensitive data to guide LLM-based synthetic generation with strong privacy guarantees, minimal compute, and good performance even when only a small amount of private data is available.", "method": "EPSVec represents the difference between a sensitive dataset and public data as a low-dimensional *dataset vector* in the LLM\u2019s activation space. The method (1) computes an activation-level representation of private data and public priors, (2) derives a steering vector capturing their distributional gap, (3) sanitizes this vector with differential privacy mechanisms, and then (4) uses it to steer LLM decoding at inference time without further access to raw private data. The steering vectors are extracted and privatized once; afterwards, standard decoding with these fixed vectors generates arbitrarily many synthetic samples at zero additional privacy cost. The method is further improved by leveraging pretrained base models and a fixed-shot prompting scheme to increase diversity and fidelity of generated text.", "result": "Across experiments, EPSVec outperforms prior private text generation baselines in matching the target data distribution (distributional alignment) and in downstream task performance (utility), with especially strong gains when only limited private data is available. It also substantially reduces computational overhead relative to existing differentially private generation techniques, since the DP cost is paid once during vector extraction and not per sample during generation.", "conclusion": "EPSVec provides an efficient, lightweight approach to differentially private text generation by steering LLMs with sanitized dataset vectors instead of repeatedly applying DP mechanisms during sampling. This decouples privacy loss from the number of generated samples, works well in low-data regimes, and achieves better alignment and utility than existing methods while being more computationally efficient. The approach suggests that activation-space steering with privatized dataset representations is a promising direction for scalable, privacy-preserving synthetic data generation."}}
{"id": "2602.21857", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21857", "abs": "https://arxiv.org/abs/2602.21857", "authors": ["Jabez Magomere", "Elena Kochkina", "Samuel Mensah", "Simerjot Kaur", "Fernando Acero", "Arturo Oncevay", "Charese H. Smiley", "Xiaomo Liu", "Manuela Veloso"], "title": "Distill and Align Decomposition for Enhanced Claim Verification", "comment": "EACL Findings 2026", "summary": "Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.", "AI": {"tldr": "They use reinforcement learning to train a smaller language model to break complex claims into good subclaims, directly optimizing what helps a verifier model, which yields better overall claim verification performance than prior methods.", "motivation": "Existing complex claim verification systems need to decompose claims into simpler subclaims, but current decomposition methods don\u2019t reliably improve the final verification accuracy because they are not directly optimized for the verifier\u2019s needs or for high-quality structured decompositions.", "method": "They introduce an RL-based decomposer trained with Group Relative Policy Optimization (GRPO). The approach combines (1) structured, stepwise generation of subclaims, (2) supervised finetuning on examples distilled from a stronger teacher model, and (3) a multi-objective reward that jointly encourages correct output format, better alignment with a downstream verifier, and intrinsically higher-quality decompositions.", "result": "On six evaluation setups, an 8B-parameter decomposer trained with this method boosts downstream claim verification to 71.75% macro-F1, beating prompt-based baselines by 1.99 and 6.24 points and existing RL approaches by 5.84 points. Human judges also rate the produced subclaims as high quality.", "conclusion": "Jointly optimizing the decomposition process with respect to both intrinsic quality and verifier performance via RL allows relatively small language models to achieve state-of-the-art complex claim verification, demonstrating the benefit of tightly coupling decomposition and verification objectives."}}
{"id": "2602.21219", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21219", "abs": "https://arxiv.org/abs/2602.21219", "authors": ["Bo Ni", "Branislav Kveton", "Samyadeep Basu", "Subhojyoti Mukherjee", "Leyao Wang", "Franck Dernoncourt", "Sungchul Kim", "Seunghyun Yoon", "Zichao Wang", "Ruiyi Zhang", "Puneet Mathur", "Jihyung Kil", "Jiuxiang Gu", "Nedim Lipka", "Yu Wang", "Ryan A. Rossi", "Tyler Derr"], "title": "Reasoning-Based Personalized Generation for Users with Sparse Data", "comment": null, "summary": "Large Language Model (LLM) personalization holds great promise for tailoring responses by leveraging personal context and history. However, real-world users usually possess sparse interaction histories with limited personal context, such as cold-start users in social platforms and newly registered customers in online E-commerce platforms, compromising the LLM-based personalized generation. To address this challenge, we introduce GraSPer (Graph-based Sparse Personalized Reasoning), a novel framework for enhancing personalized text generation under sparse context. GraSPer first augments user context by predicting items that the user would likely interact with in the future. With reasoning alignment, it then generates texts for these interactions to enrich the augmented context. In the end, it generates personalized outputs conditioned on both the real and synthetic histories, ensuring alignment with user style and preferences. Extensive experiments on three benchmark personalized generation datasets show that GraSPer achieves significant performance gain, substantially improving personalization in sparse user context settings.", "AI": {"tldr": "GraSPer is a framework that improves LLM personalization when user history is sparse by expanding user context with predicted future interactions and synthetic texts, then generating outputs conditioned on both real and synthetic histories.", "motivation": "Personalized LLM outputs typically rely on rich user interaction histories, but many practical users are \u2018cold-start\u2019 or have sparse histories (e.g., new users on social media or e-commerce). This sparsity undermines the quality of personalized generation, so there is a need for methods that can still infer and leverage user preferences under limited observed data.", "method": "GraSPer (Graph-based Sparse Personalized Reasoning) augments sparse user context in three stages: (1) It uses a graph-based mechanism to predict items a user is likely to interact with in the future, thereby expanding the user\u2019s implicit preference profile. (2) Through reasoning alignment, it has an LLM generate explanatory or interaction-style texts for these predicted items, thus creating synthetic but coherent user histories. (3) The final personalized generation step conditions on both real histories and these synthetic, aligned histories to produce outputs better matched to the user\u2019s style and preferences.", "result": "On three benchmark personalized text generation datasets, GraSPer yields substantial performance improvements over baselines, particularly in settings where user histories are sparse. The experiments demonstrate that augmenting context via predicted future interactions and generated texts enhances personalization quality.", "conclusion": "GraSPer shows that even under sparse user data, personalization in LLMs can be significantly improved by predicting likely future interactions and generating aligned synthetic histories, then conditioning final outputs on this enriched context. This approach effectively mitigates cold-start issues and enhances personalized text generation performance."}}
{"id": "2602.21858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21858", "abs": "https://arxiv.org/abs/2602.21858", "authors": ["Dezhi Kong", "Zhengzhao Feng", "Qiliang Liang", "Hao Wang", "Haofei Sun", "Changpeng Yang", "Yang Li", "Peng Zhou", "Shuai Nie", "Hongzhen Wang", "Linfeng Zhou", "Hao Jia", "Jiaming Xu", "Runyu Shi", "Ying Huang"], "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices", "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.", "AI": {"tldr": "The paper introduces ProactiveMobile, a benchmark to evaluate and advance proactive intelligence of mobile multimodal large language model (MLLM) agents, focusing on inferring latent user intent from contextual signals and generating executable API call sequences.", "motivation": "Existing multimodal large language models for mobile agents mainly operate reactively, only following explicit user commands, while the emerging goal is proactive intelligence where agents anticipate user needs and act autonomously. Progress towards this is limited by the lack of realistic, executable benchmarks to measure and train such proactive behavior in complex real-world settings.", "method": "The authors design ProactiveMobile, a benchmark that formalizes proactive mobile tasks as inferring hidden user intent from four types of on-device contextual signals and mapping them to executable sequences of actions from a pool of 63 mobile APIs. They build 3,660+ instances across 14 realistic scenarios, allow multiple correct answers to reflect real-world variability, and employ a team of 30 experts to audit and refine the dataset for factual accuracy, logical consistency, and action feasibility. They then evaluate various MLLMs, including a fine-tuned Qwen2.5-VL-7B-Instruct, o1, and GPT-5, on this benchmark.", "result": "On ProactiveMobile, the fine-tuned Qwen2.5-VL-7B-Instruct model achieves a 19.15% success rate, outperforming o1 at 15.71% and GPT-5 at 7.39%. Despite these differences, all models show relatively low success, revealing that current MLLMs generally struggle with proactive tasks involving latent intent inference and executable action planning.", "conclusion": "Proactivity\u2014inferring latent user intent from context and autonomously planning actions\u2014is a key but underdeveloped capability in current mobile MLLMs. The ProactiveMobile benchmark both exposes this gap and demonstrates that proactivity can be improved through targeted training, underscoring its value as a standard for evaluating and driving research on proactive mobile agents."}}
{"id": "2602.21220", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21220", "abs": "https://arxiv.org/abs/2602.21220", "authors": ["Subhadip Mitra"], "title": "Field-Theoretic Memory for AI Agents: Continuous Dynamics for Context Preservation", "comment": "15 pages, 6 figures. Code: https://github.com/rotalabs/rotalabs-fieldmem", "summary": "We present a memory system for AI agents that treats stored information as continuous fields governed by partial differential equations rather than discrete entries in a database. The approach draws from classical field theory: memories diffuse through semantic space, decay thermodynamically based on importance, and interact through field coupling in multi-agent scenarios. We evaluate the system on two established long-context benchmarks: LoCoMo (ACL 2024) with 300-turn conversations across 35 sessions, and LongMemEval (ICLR 2025) testing multi-session reasoning over 500+ turns. On LongMemEval, the field-theoretic approach achieves significant improvements: +116% F1 on multi-session reasoning (p<0.01, d= 3.06), +43.8% on temporal reasoning (p<0.001, d= 9.21), and +27.8% retrieval recall on knowledge updates (p<0.001, d= 5.00). Multi-agent experiments show near-perfect collective intelligence (>99.8%) through field coupling. Code is available at github.com/rotalabs/rotalabs-fieldmem.", "AI": {"tldr": "Proposes a novel memory system for AI agents that models memories as continuous fields governed by PDEs instead of discrete database entries, yielding large gains on long-context and multi-agent benchmarks.", "motivation": "Existing AI memory systems struggle with very long contexts, multi-session interactions, and coordinated multi-agent reasoning because they treat memory as discrete items to be stored and retrieved, leading to brittle retrieval and poor temporal/semantic generalization. The authors aim to create a more flexible, physically inspired memory representation that better captures gradual change, diffusion of relevance, and interactions among multiple agents\u2019 memories.", "method": "They design a memory architecture based on classical field theory where information is represented as continuous fields over a semantic space. These fields evolve according to partial differential equations: information diffuses through the space, decays over time at rates tied to its importance, and interacts via coupling terms when multiple agents are present. The system is plugged into AI agents and evaluated on long-context and multi-agent benchmarks, comparing against standard discrete memory/retrieval methods using F1, temporal reasoning accuracy, retrieval recall, and a collective intelligence measure.", "result": "On the LongMemEval benchmark, the field-theoretic memory system shows large improvements over baselines: a 116% gain in F1 for multi-session reasoning, 43.8% improvement on temporal reasoning, and 27.8% higher retrieval recall for knowledge updates, all with strong statistical significance (p<0.01 or better and large effect sizes). On LoCoMo, it successfully handles 300-turn, 35-session conversations, and in multi-agent setups, field coupling yields near-perfect collective intelligence performance above 99.8%.", "conclusion": "Modeling memory as a continuous field governed by PDEs is a powerful alternative to discrete database-style memory for AI agents. It supports more robust long-context, temporal, and multi-session reasoning and scales effectively to multi-agent settings through field coupling. The authors\u2019 empirical results suggest that physically inspired, field-theoretic memory representations can substantially improve long-term and collective reasoning in AI systems."}}
{"id": "2602.21889", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21889", "abs": "https://arxiv.org/abs/2602.21889", "authors": ["Otto Nyberg", "Fausto Carcassi", "Giovanni Cin\u00e0"], "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support", "comment": "17 pages, 17 figures", "summary": "Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training.", "AI": {"tldr": "The paper proposes a Bayesian computational framework (2-Step Agent) to analyze how AI predictions influence human beliefs, decisions, and outcomes, and shows that even a single misaligned prior can make AI assistance worse than no assistance.", "motivation": "AI predictions are increasingly used to support human decisions, but it is not well understood when and how such assistance actually improves or worsens outcomes. There is a need for a principled way to model the impact of AI advice on human beliefs and subsequent choices.", "method": "The authors develop the 2-Step Agent framework, a Bayesian causal inference model that formalizes two stages: (1) how an AI prediction about a new case updates a rational Bayesian agent\u2019s prior beliefs, and (2) how these updated beliefs drive the agent\u2019s decision and the eventual outcome. They then run simulations under different prior and model-alignment assumptions.", "result": "Simulations show that if the human decision-maker\u2019s prior beliefs are even slightly misaligned with reality or with the AI model, the combination of those priors with the AI\u2019s predictions can systematically lead to worse downstream outcomes than if the human had received no AI support at all.", "conclusion": "AI-assisted decision support is not automatically beneficial; its impact critically depends on the alignment between user priors, the AI model, and the true data-generating process. Because misaligned priors can lead to harmful outcomes despite accurate predictions, careful model documentation and user training are essential to ensure that AI support improves rather than degrades decision quality."}}
{"id": "2602.21222", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21222", "abs": "https://arxiv.org/abs/2602.21222", "authors": ["Riya Adsul", "Balachandra Devarangadi Sunil", "Isha Nalawade", "Sudharshan Govindan"], "title": "Task-Aware LoRA Adapter Composition via Similarity Retrieval in Vector Databases", "comment": null, "summary": "Parameter efficient fine tuning methods like LoRA have enabled task specific adaptation of large language models, but efficiently composing multiple specialized adapters for unseen tasks remains challenging. We present a novel framework for dynamic LoRA adapter composition that leverages similarity retrieval in vector databases to enable zero-shot generalization across diverse NLP tasks. Our approach constructs a task-aware vector database by embedding training examples from 22 datasets spanning commonsense reasoning, question answering, natural language inference, and sentiment analysis. At inference time, we retrieve the most similar training examples, compute task similarity distributions via nucleus sampling, and dynamically merge relevant LoRA adapters using retrieval weighted fusion strategies. We evaluated four merging methods Linear, Concatenation, TIES, and Magnitude Prune demonstrating that our dataset centric retrieval approach often matches or exceeds the performance of individually fine-tuned task-specific adapters. Notably, Linear merging achieves 70.95% on PIQA and 77.62% on RTE, substantially outperforming single-task baselines (46% and 52%, respectively). Our framework requires no additional retriever training, operates with frozen embeddings, and enables efficient, interpretable adapter composition. These results suggest that retrieval based dynamic merging offers a promising direction for scalable, parameter-efficient multitask learning without requiring full model retraining for each new task.", "AI": {"tldr": "The paper proposes a retrieval-based framework to dynamically merge multiple LoRA adapters so that a model can handle unseen NLP tasks in a zero-shot, parameter-efficient way, often matching or beating single-task fine-tuned adapters.", "motivation": "While LoRA enables efficient task-specific fine-tuning of large language models, scaling to many tasks is difficult because it is unclear how to effectively combine multiple specialized adapters for a new, unseen task without re-training. The authors aim to create a method that can reuse and compose existing adapters based on task similarity, avoiding separate fine-tuning for every new task.", "method": "They build a task-aware vector database using frozen embeddings of training examples from 22 datasets across several NLP task types. At inference, given an input, they retrieve similar training examples, estimate a task similarity distribution using nucleus sampling, and then dynamically fuse the LoRA adapters associated with those tasks. They explore four adapter-merging strategies\u2014Linear, Concatenation, TIES, and Magnitude Prune\u2014where the contribution of each adapter is weighted by the retrieval-based task similarity scores. No extra retriever training is done; embeddings are kept frozen.", "result": "Across multiple benchmarks, the retrieval-guided merging often performs on par with or better than individually fine-tuned, task-specific LoRA adapters. For instance, linear merging reaches 70.95% accuracy on PIQA and 77.62% on RTE, greatly surpassing single-task baselines of 46% and 52%, respectively. Similar gains are observed across other tasks, showing good zero-shot generalization to diverse NLP problems.", "conclusion": "Retrieval-based dynamic composition of LoRA adapters is an effective and scalable way to achieve parameter-efficient multitask generalization. By using a vector database of task examples and simple fusion methods, models can adapt to new tasks without retraining or modifying base embeddings, offering an interpretable and efficient path toward broader zero-shot capability with modular adapters."}}
{"id": "2602.22067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22067", "abs": "https://arxiv.org/abs/2602.22067", "authors": ["Giuseppe Canonaco", "Alberto Pozanco", "Daniel Borrajo"], "title": "Semantic Partial Grounding via LLMs", "comment": null, "summary": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.", "AI": {"tldr": "The paper introduces SPG-LLM, an LLM-based approach that prunes irrelevant elements from PDDL planning tasks before grounding to speed up grounding drastically while preserving plan quality.", "motivation": "Classical planning requires grounding abstract operators and predicates into concrete actions and atoms, but this grounding step can be a major computational bottleneck because the number of ground actions and atoms grows exponentially with problem size. Existing partial grounding methods mitigate this by incrementally grounding based on predictive models, but they mostly rely on relational features or learned embeddings and overlook the rich textual and structural information encoded in PDDL domain and problem descriptions. There is a need for a method that better exploits these cues to more aggressively but safely cut down the grounding space.", "method": "The authors propose SPG-LLM, a system that uses large language models to inspect PDDL domain and problem files before the grounding phase. The LLM is prompted to heuristically infer which objects, actions, and predicates are likely to be irrelevant for solving the planning task. These elements are pruned prior to grounding, thus shrinking the grounded representation. The approach is evaluated on several hard-to-ground benchmark domains, comparing its grounding time and resulting plan quality against existing grounding and partial grounding baselines.", "result": "On seven benchmarks known to be difficult to ground, SPG-LLM substantially reduces grounding time, often by orders of magnitude, compared with traditional and partial grounding approaches. Despite the aggressive pruning, the resulting plans have comparable quality, and in some domains SPG-LLM even attains better plan costs than baselines, indicating that the heuristic pruning does not significantly harm solution quality and may sometimes help by simplifying the search space.", "conclusion": "LLM-based analysis of PDDL descriptions is an effective strategy for pre-grounding pruning in classical planning. By exploiting textual and structural cues that prior methods ignore, SPG-LLM dramatically accelerates grounding while generally preserving or improving plan quality. This suggests that integrating LLMs into the planning toolchain can alleviate a key bottleneck and opens up new directions for leveraging language models in automated planning and related reasoning tasks."}}
{"id": "2602.21223", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21223", "abs": "https://arxiv.org/abs/2602.21223", "authors": ["Yilin Geng", "Omri Abend", "Eduard Hovy", "Lea Frermann"], "title": "Measuring Pragmatic Influence in Large Language Model Instructions", "comment": null, "summary": "It is not only what we ask large language models (LLMs) to do that matters, but also how we prompt. Phrases like \"This is urgent\" or \"As your supervisor\" can shift model behavior without altering task content. We study this effect as pragmatic framing, contextual cues that shape directive interpretation rather than task specification. While prior work exploits such cues for prompt optimization or probes them as security vulnerabilities, pragmatic framing itself has not been treated as a measurable property of instruction following. Measuring this influence systematically remains challenging, requiring controlled isolation of framing cues. We introduce a framework with three novel components: directive-framing decomposition separating framing context from task specification; a taxonomy organizing 400 instantiations of framing into 13 strategies across 4 mechanism clusters; and priority-based measurement that quantifies influence through observable shifts in directive prioritization. Across five LLMs of different families and sizes, influence mechanisms cause consistent and structured shifts in directive prioritization, moving models from baseline impartiality toward favoring the framed directive. This work establishes pragmatic framing as a measurable and predictable factor in instruction-following systems.", "AI": {"tldr": "The paper shows that the way we phrase instructions to LLMs (pragmatic framing) systematically changes which directives they prioritize, and provides a framework to measure this effect.", "motivation": "Although we know that prompt wording affects LLM behavior, subtle contextual phrases (e.g., expressing urgency or social role) are not well-understood as a distinct, measurable phenomenon. Existing work mainly uses such cues for prompt hacking or optimization rather than treating them as a core property of instruction following. The authors aim to rigorously isolate and quantify how these pragmatic frames influence which instructions models treat as more important.", "method": "The authors propose a framework with three pieces: (1) directive\u2013framing decomposition, which cleanly separates the core task from surrounding contextual wording; (2) a taxonomy of 400 concrete framing variations grouped into 13 strategies and 4 broader mechanism clusters; and (3) a priority-based measurement scheme that observes how LLMs change which of multiple directives they prioritize when different frames are applied. They test this across five LLMs of varying architectures and sizes.", "result": "They find that different influence mechanisms (framing strategies) reliably and systematically shift models\u2019 directive prioritization away from a neutral baseline and toward the directive that is pragmatically framed. These shifts appear structured and consistent across five distinct LLM families and scales, indicating that pragmatic framing exerts a predictable influence rather than random noise.", "conclusion": "Pragmatic framing is a stable, quantifiable factor in how LLMs follow instructions. By decomposing prompts, organizing framing strategies, and measuring priority changes, the paper demonstrates that contextual phrasing like urgency or authority meaningfully and predictably alters directive prioritization. This turns framing from an ad-hoc trick or vulnerability into a formal property of instruction-following systems that can be analyzed, compared across models, and potentially controlled."}}
{"id": "2602.22070", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22070", "abs": "https://arxiv.org/abs/2602.22070", "authors": ["Jessica Y. Bo", "Lillio Mok", "Ashton Anderson"], "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts", "comment": "Second Conference of the International Association for Safe and Ethical Artificial Intelligence (IASEAI 2026)", "summary": "Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.", "AI": {"tldr": "The paper studies how large language models (LLMs) treat advice from human experts versus algorithms, finding conflicting behaviors depending on how tasks are presented.", "motivation": "As LLMs are increasingly embedded in decision-making workflows, they must often aggregate or choose between recommendations from humans and other algorithms. Human decision-makers are known to show algorithm aversion (a bias against algorithmic advice). It is important to know whether LLMs display similar or different biases, and how stable those biases are across evaluation formats, to assess risks when deploying LLMs in high-stakes settings.", "method": "The authors adapt experimental paradigms from behavioral economics used to study human algorithm aversion. They evaluate eight different LLMs on delegation and trust tasks under two conditions: (1) stated preferences, where models directly rate how much they trust a human expert vs. an algorithmic agent across diverse tasks; and (2) revealed preferences, where models are given in-context examples showing the historical performance of a human and an algorithm, then asked to choose one on which to place an incentivized bet. They compare LLM responses to known human behavioral patterns.", "result": "Across models and tasks, LLMs explicitly rate human experts as more trustworthy than algorithms when asked directly, mirroring human survey results. However, in the revealed-preference betting tasks, LLMs tend to choose the algorithmic agent more often, even when the algorithm is shown to have worse performance than the human expert. This shows a divergence between stated and revealed preferences and an over-selection of algorithms in consequential choices.", "conclusion": "LLMs exhibit internally inconsistent biases toward human and algorithmic agents: they verbally express greater trust in humans but behaviorally favor algorithms, sometimes irrationally. This inconsistency is sensitive to how tasks are framed and presented, implying that evaluation format strongly influences measured \u201cbiases\u201d and decision policies of LLMs. These findings highlight the need for careful scrutiny of LLM behavior in high-stakes applications, including robustness of evaluations and implications for AI safety, given their role in mediating between human and algorithmic recommendations."}}
{"id": "2602.21224", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21224", "abs": "https://arxiv.org/abs/2602.21224", "authors": ["Yuetao Chen", "Xuliang Wang", "Xinzhou Zheng", "Ming Li", "Peng Wang", "Hong Xu"], "title": "Make Every Draft Count: Hidden State based Speculative Decoding", "comment": null, "summary": "Speculative decoding has emerged as a pivotal technique to accelerate LLM inference by employing a lightweight draft model to generate candidate tokens that are subsequently verified by the target model in parallel. However, while this paradigm successfully increases the arithmetic intensity of memory-bound inference, it causes significant compute inefficiency: the majority of draft tokens fail verification and are discarded, resulting in waste of computation. Motivated by the goal of recollecting this wasted computation, we propose a novel system that transforms discarded drafts into reusable tokens. Our key insight is to perform auto-regressive prediction at the hidden states level and postpone the integrating token information after the hidden states generation, so the draft hidden states are not contaminated by incorrect tokens, enabling hidden state reuse. To implement such a system, first we introduce a draft model architecture based on auto-regressive hidden states, which preserves richer semantics than token-based drafters to facilitate draft repurposing. Second, we design an efficient token information injection mechanism that leverages our specialized draft model to construct high-quality draft token trees and enables resampling tokens from verification failures. Third, we eliminate the overhead hidden in our design to further maximize hardware utilization. We conducted extensive evaluations against various baselines, demonstrating up to a 3.3x speedup against standard speculative decoding.", "AI": {"tldr": "The paper improves speculative decoding for LLMs by reusing computation from discarded draft tokens via hidden-state\u2013level autoregression, achieving up to 3.3\u00d7 speedup.", "motivation": "Standard speculative decoding accelerates LLM inference but wastes computation because most draft tokens from a small model fail verification by the large model and are discarded. This compute waste reduces overall efficiency despite better arithmetic intensity. The authors aim to design a system that can salvage and reuse this discarded computation to further speed up inference without harming quality.", "method": "1) Introduce a new draft model that autoregresses over hidden states instead of tokens, delaying the incorporation of token information so that hidden states are not corrupted by incorrect token guesses; 2) Design a token information injection mechanism that uses the richer hidden-state semantics to build draft token trees and resample tokens even when initial token guesses fail verification; 3) Optimize the system pipeline and implementation to remove hidden overheads and better utilize hardware parallelism.", "result": "In experiments comparing against standard speculative decoding and other baselines, the proposed system achieves up to 3.3\u00d7 speedup over standard speculative decoding while maintaining output quality. The evaluations show that reusing hidden states from failed draft tokens substantially improves compute efficiency.", "conclusion": "Reframing speculative decoding around autoregressive hidden states and postponed token integration allows reuse of computation that would otherwise be discarded when draft tokens fail verification. With appropriate token injection and system optimizations, this approach significantly accelerates LLM inference, demonstrating a more compute-efficient speculative decoding paradigm."}}
{"id": "2602.22094", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22094", "abs": "https://arxiv.org/abs/2602.22094", "authors": ["Nguyen Cong Nhat Le", "John G. Rogers", "Claire N. Bonial", "Neil T. Dantam"], "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning", "comment": "16 pages, 5 figures. Submitted to 17th World Symposium on the Algorithmic Foundations of Robotics (WAFR) on 01/14/2026", "summary": "Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.", "AI": {"tldr": "The paper introduces a Petri-net-based relaxation for planning that enables robust invariant synthesis, efficient detection of unreachable goals, and clear explanations of infeasible planning tasks, while also supporting incremental updates to goals and constraints.", "motivation": "In real-world planning problems, plans often need to be revised as circumstances or requirements change, and sometimes no feasible plan exists for the current requirements. Existing planners are mainly optimized for one-shot planning assuming feasibility, offering limited support for identifying infeasibility, explaining why a goal cannot be reached, or efficiently updating plans as goals and constraints change. There is thus a need for planning methods that can reliably detect unreachability, explain it, and adapt efficiently to updates.", "method": "The authors propose a relaxation of the planning problem based on Petri net reachability. This relaxation is used to automatically synthesize invariants (properties that must hold in all reachable states), to detect when a goal is unreachable, and to generate explanations of infeasibility. They integrate incremental constraint solvers to reuse computation when goals or constraints are updated, enabling efficient sequential plan updates rather than solving each new instance from scratch. The framework is evaluated empirically against baseline planners.", "result": "Empirical evaluation shows that the proposed system: (1) generates a number of invariants comparable to existing techniques, indicating similar descriptive power; (2) detects up to twice as many infeasible planning instances as the baselines, demonstrating stronger unreachability detection; (3) performs on par with traditional planners in standard one-shot planning scenarios; and (4) surpasses baselines in settings with sequential plan updates, where goals and constraints change over time.", "conclusion": "The Petri-net-reachability-based relaxation provides a robust foundation for enhanced analysis in planning: it supports strong invariant synthesis, significantly improves infeasibility detection and explanation, and handles sequential updates to planning problems more efficiently than standard approaches. As a result, it offers a more practical and adaptive planning framework for dynamic or requirement-heavy domains where goal feasibility is uncertain and goals/constraints frequently change."}}
{"id": "2602.21225", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21225", "abs": "https://arxiv.org/abs/2602.21225", "authors": ["Mohammed Hamdan", "Vincenzo Dentamaro", "Giuseppe Pirlo", "Mohamed Cheriet"], "title": "Architecture-Agnostic Curriculum Learning for Document Understanding: Empirical Evidence from Text-Only and Multimodal", "comment": null, "summary": "We investigate whether progressive data scheduling -- a curriculum learning strategy that incrementally increases training data exposure (33\\%$\\rightarrow$67\\%$\\rightarrow$100\\%) -- yields consistent efficiency gains across architecturally distinct document understanding models. By evaluating BERT (text-only, 110M parameters) and LayoutLMv3 (multimodal, 126M parameters) on the FUNSD and CORD benchmarks, we establish that this schedule reduces wall-clock training time by approximately 33\\%, commensurate with the reduction from 6.67 to 10.0 effective epoch-equivalents of data. To isolate curriculum effects from compute reduction, we introduce matched-compute baselines (Standard-7) that control for total gradient updates. On the FUNSD dataset, the curriculum significantly outperforms the matched-compute baseline for BERT ($\u0394$F1 = +0.023, $p=0.022$, $d_z=3.83$), constituting evidence for a genuine scheduling benefit in capacity-constrained models. In contrast, no analogous benefit is observed for LayoutLMv3 ($p=0.621$), whose multimodal representations provide sufficient inductive bias. On the CORD dataset, all conditions converge to equivalent F1 scores ($\\geq$0.947) irrespective of scheduling, indicating a performance ceiling. Schedule ablations comparing progressive, two-phase, reverse, and random pacing confirm that the efficiency gain derives from reduced data volume rather than ordering. Taken together, these findings demonstrate that progressive scheduling is a reliable compute-reduction strategy across model families, with curriculum-specific benefits contingent on the interaction between model capacity and task complexity.", "AI": {"tldr": "The paper studies a progressive data scheduling (curriculum) strategy that gradually increases the fraction of training data, showing it consistently saves compute and sometimes improves performance depending on model capacity and task difficulty.", "motivation": "Training large document understanding models is computationally expensive. Curriculum learning via progressive data exposure may reduce training time and possibly improve generalization, but it is unclear whether such benefits are consistent across different model architectures (text-only vs multimodal) and datasets of varying complexity. The paper aims to quantify the efficiency gains, disentangle pure compute savings from curriculum effects, and understand when curriculum-specific benefits actually occur.", "method": "They implement a progressive data scheduling scheme where models are trained first on 33% of the data, then 67%, then 100%. They test this on two document understanding benchmarks (FUNSD and CORD) using BERT (text-only, 110M parameters) and LayoutLMv3 (multimodal, 126M parameters). They compare progressive scheduling against standard training with equivalent total compute (Standard-7 matched-compute baseline) to isolate curriculum effects from mere reduction in gradient updates. They also perform schedule ablations (progressive, two-phase, reverse, random pacing) to see whether data ordering or total volume is responsible for observed gains. Statistical tests (p-values, effect sizes) are used to assess significance.", "result": "Progressive scheduling reduces wall-clock training time by about 33%, mirroring the drop in effective training epochs from 10.0 to 6.67. On FUNSD, BERT with progressive scheduling significantly outperforms the matched-compute baseline (\u0394F1 = +0.023, p=0.022, dz=3.83), showing a genuine curriculum benefit for a capacity-limited text-only model. For LayoutLMv3 on FUNSD, no significant curriculum benefit is observed (p=0.621), suggesting its multimodal inductive biases already suffice. On CORD, all configurations, including different schedules, reach similar high F1 scores (\u22650.947), indicating a performance ceiling. Ablation of schedule variants shows that efficiency gains stem from reduced data volume, not the particular ordering of examples.", "conclusion": "Progressive data scheduling is a robust method to cut training compute across distinct document understanding models, yielding around one-third savings in training time. Curriculum-specific performance gains are not universal; they emerge mainly when model capacity is constrained and the task is sufficiently challenging, as with BERT on FUNSD. For stronger multimodal models or easier/high-accuracy tasks like CORD, progressive scheduling still saves compute but does not improve final accuracy. The primary benefit is dependable compute reduction, with curriculum advantages depending on the interaction between model capacity and task complexity."}}
{"id": "2602.21226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21226", "abs": "https://arxiv.org/abs/2602.21226", "authors": ["Ezieddin Elmahjub", "Junaid Qadir", "Abdullah Mushtaq", "Rafay Naeem", "Ibrahim Ghaznavi", "Waleed Iqbal"], "title": "IslamicLegalBench: Evaluating LLMs Knowledge and Reasoning of Islamic Law Across 1,200 Years of Islamic Pluralist Legal Traditions", "comment": "This manuscript has been submitted for review to Artificial Intelligence \\& Law", "summary": "As millions of Muslims turn to LLMs like GPT, Claude, and DeepSeek for religious guidance, a critical question arises: Can these AI systems reliably reason about Islamic law? We introduce IslamicLegalBench, the first benchmark evaluating LLMs across seven schools of Islamic jurisprudence, with 718 instances covering 13 tasks of varying complexity. Evaluation of nine state-of-the-art models reveals major limitations: the best model achieves only 68% correctness with 21% hallucination, while several models fall below 35% correctness and exceed 55% hallucination. Few-shot prompting provides minimal gains, improving only 2 of 9 models by >1%. Moderate-complexity tasks requiring exact knowledge show the highest errors, whereas high-complexity tasks display apparent competence through semantic reasoning. False premise detection indicates risky sycophancy, with 6 of 9 models accepting misleading assumptions at rates above 40%. These results highlight that prompt-based methods cannot compensate for missing foundational knowledge. IslamicLegalBench offers the first systematic framework to evaluate Islamic legal reasoning in AI, revealing critical gaps in tools increasingly relied on for spiritual guidance.", "AI": {"tldr": "The paper introduces IslamicLegalBench, the first benchmark to systematically evaluate how well large language models (LLMs) handle Islamic legal reasoning, and finds that current models perform poorly and hallucinate frequently.", "motivation": "As many Muslims are starting to use general-purpose LLMs for religious and legal guidance, there is an urgent need to understand whether these models can reliably reason within the formal structures of Islamic jurisprudence across different schools of thought.", "method": "The authors construct IslamicLegalBench, a dataset of 718 instances spanning 13 task types and seven schools of Islamic jurisprudence, then evaluate nine state-of-the-art LLMs on this benchmark, including analyses of correctness, hallucination rates, few-shot prompting effects, task complexity differences, and susceptibility to false premises and sycophancy.", "result": "The best-performing model reaches only 68% correctness with a 21% hallucination rate, while several models perform below 35% correctness and above 55% hallucination; few-shot prompting yields negligible improvements (only 2 of 9 models improve by more than 1%), moderate-complexity knowledge-heavy tasks see the most errors, high-complexity semantic reasoning tasks can look deceptively strong, and most models readily accept misleading premises, with 6 of 9 models doing so over 40% of the time.", "conclusion": "Prompt-engineering strategies cannot make up for missing domain knowledge in current LLMs, which exhibit serious reliability and hallucination issues when applied to Islamic legal reasoning; IslamicLegalBench provides a structured way to evaluate and expose these gaps, underscoring the risks of relying on present-day models for spiritual and legal guidance in Islam."}}
{"id": "2602.21227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21227", "abs": "https://arxiv.org/abs/2602.21227", "authors": ["Caiqi Zhang", "Menglin Xia", "Xuchao Zhang", "Daniel Madrigal", "Ankur Mallick", "Samuel Kessler", "Victor Ruehle", "Saravan Rajmohan"], "title": "Budget-Aware Agentic Routing via Boundary-Guided Training", "comment": null, "summary": "As large language models (LLMs) evolve into autonomous agents that execute long-horizon workflows, invoking a high-capability model at every step becomes economically unsustainable. While model routing is effective for single-turn queries, agentic routing is a sequential, path-dependent problem: early mistakes compound, feedback is often at the end of the episode, and deployments often demand strict per-task spending limits. We propose Budget-Aware Agentic Routing, which selects between a cheap and an expensive model at each step to optimize the cost--success frontier and to operate under strict per-task budgets. We propose Boundary-Guided Training, which leverages two boundary policies (always-small vs.\\ always-large) to build a difficulty taxonomy and to anchor learning under sparse rewards. Our approach warms start with boundary-guided SFT data synthesis via stratified sampling of cost-efficient trajectories, then applies Boundary-Guided Policy Optimization (BoPO), combining boundary-relative rewards with a reference-guided advantage to avoid degenerate cheap-failure solutions. Experiment results show that our method improves the efficiency frontier, matching strong routing baselines at substantially lower cost while demonstrating generalization to strict inference-time budget constraints. Overall, our work establishes a foundational framework for agentic routing, shifting the paradigm from static model selection to dynamic, budget-aware sequential decision-making.", "AI": {"tldr": "The paper introduces a framework for budget-aware routing between cheap and expensive LLMs within multi-step agents, improving task success for a given cost and handling strict per-task budget limits.", "motivation": "As LLM-based agents execute longer and more complex workflows, using a high-capability model at every step becomes too expensive. Existing model routing methods largely focus on single-turn queries and do not address the sequential, path-dependent nature of multi-step agent decisions, where early routing mistakes can compound and feedback often comes only at the end. There is a need for methods that can make step-wise routing decisions that respect strict per-task budgets while maximizing task success.", "method": "The authors propose Budget-Aware Agentic Routing, a policy that chooses between a cheap and an expensive model at each step of an agent\u2019s workflow. To train this policy, they introduce Boundary-Guided Training, which relies on two extreme baseline policies: always use the small (cheap) model or always use the large (expensive) model. These boundary policies are used to construct a difficulty taxonomy of trajectories and to provide learning signals under sparse end-of-episode rewards. The training pipeline first performs boundary-guided supervised fine-tuning (SFT) data synthesis by stratified sampling of cost-efficient trajectories, then applies Boundary-Guided Policy Optimization (BoPO). BoPO uses boundary-relative rewards and a reference-guided advantage function to steer learning away from degenerate policies that save cost but fail tasks, and instead push the policy toward better cost\u2013success trade-offs.", "result": "Empirical results show that the proposed method shifts the cost\u2013success efficiency frontier: it can achieve similar or better success rates than strong existing routing baselines while incurring significantly lower cost. Additionally, the learned policy generalizes well to strict inference-time budget constraints that may differ from those used during training, indicating robustness of the budget-aware routing strategy.", "conclusion": "The paper establishes a general framework for agentic routing that treats model selection as a dynamic, sequential decision problem under budget constraints rather than a static choice for single-turn queries. By leveraging boundary-guided training and policy optimization, the method enables LLM agents to intelligently allocate expensive model calls only when needed, thereby improving overall cost efficiency without sacrificing performance."}}
{"id": "2602.21228", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21228", "abs": "https://arxiv.org/abs/2602.21228", "authors": ["Yuancheng Yang", "Lin Yang", "Xu Wang", "Chao Tong", "Haihua Yang"], "title": "ImpRIF: Stronger Implicit Reasoning Leads to Better Complex Instruction Following", "comment": null, "summary": "As applications of large language models (LLMs) become increasingly complex, the demand for robust complex instruction following capabilities is growing accordingly. We argue that a thorough understanding of the instruction itself, especially the latent reasoning structure embedded between the lines, is crucial for improving instruction following. Therefore we target complex instructions that involve implicit reasoning, intricate logical relations, and multi-constraint dependencies. We propose ImpRIF, a method to enhance LLMs' understanding of implicit reasoning instructions, thereby improving its ability to follow complex instructions. We formalize such instructions as verifiable reasoning graphs, enabling programmatic verification and graph-driven chain-of-thought reasoning. Based on this formulation, we synthesize large-scale single- and multi-turn data, propose fine-tuning with graph reasoning, and apply reinforcement learning to explicitly train models to reason along the graph. On five complex instruction following benchmarks, our models substantially outperform their base models. These results demonstrate that enhancing implicit reasoning capabilities can significantly improve complex instruction following. This project will be open-sourced in the near future.", "AI": {"tldr": "The paper introduces ImpRIF, a framework that models complex, implicitly reasoned instructions as verifiable reasoning graphs and trains LLMs (via supervised fine-tuning and RL) to reason along these graphs, significantly improving complex instruction following performance.", "motivation": "Existing LLMs struggle with complex instructions that contain implicit reasoning, hidden logical structure, and multiple interdependent constraints. Simple prompt-based or surface-level pattern learning is insufficient for reliably following such instructions, especially as real-world applications become more complex. The authors aim to explicitly capture and train models on the latent reasoning structure underlying instructions so that LLMs can understand and execute complex, multi-step, and logically entangled tasks more robustly.", "method": "1) Formalize complex, implicitly reasoned instructions as verifiable reasoning graphs, where nodes/edges encode reasoning steps, logical relations, and constraint dependencies; 2) Use this graph representation to support programmatic verification of whether an instruction is correctly followed; 3) Synthesize large-scale training data (both single-turn and multi-turn interactions) annotated with these reasoning graphs; 4) Fine-tune LLMs with graph-driven chain-of-thought reasoning, encouraging the model to follow the reasoning graph when generating solutions; 5) Apply reinforcement learning with feedback based on graph verification, explicitly training models to reason along the graph to maximize correctness and adherence to constraints.", "result": "On five complex instruction-following benchmarks, models trained with ImpRIF achieve substantial performance gains over their base counterparts. The improvements indicate better handling of implicit reasoning, logical relations, and multi-constraint dependencies, as measured by benchmark metrics that evaluate success at following complex instructions.", "conclusion": "Explicitly modeling the latent reasoning structure of complex instructions as verifiable reasoning graphs and training LLMs to reason along these graphs\u2014via fine-tuning and reinforcement learning\u2014significantly enhances complex instruction-following ability. This suggests that strengthening implicit reasoning understanding is a key pathway to more robust and capable LLM-based systems. The authors plan to open-source the project, enabling broader use and further research."}}
{"id": "2602.21230", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21230", "abs": "https://arxiv.org/abs/2602.21230", "authors": ["Yanyu Chen", "Jiyue Jiang", "Jiahong Liu", "Yifei Zhang", "Xiao Guo", "Irwin King"], "title": "TRACE: Trajectory-Aware Comprehensive Evaluation for Deep Research Agents", "comment": "Accepted by WWW 2026", "summary": "The evaluation of Deep Research Agents is a critical challenge, as conventional outcome-based metrics fail to capture the nuances of their complex reasoning. Current evaluation faces two primary challenges: 1) a reliance on singular metrics like Pass@1, creating a \"high-score illusion\" that ignores the quality, efficiency, and soundness of the reasoning process; and 2) the failure of static benchmarks to quantify crucial attributes like robustness and latent capability. To address these gaps, we introduce TRACE (Trajectory-Aware Comprehensive Evaluation), a framework that holistically assesses the entire problem-solving trajectory. To counter the \"high-score illusion\", we propose a Hierarchical Trajectory Utility Function that quantifies process efficiency and cognitive quality, including evidence grounding, alongside accuracy. To measure deeper attributes, TRACE introduces a Scaffolded Capability Assessment protocol, quantifying an agent's latent ability by determining the minimum guidance needed for success. Our contributions include the TRACE framework, its novel metrics, and the accompanying DeepResearch-Bench with controllable complexity. Experiments show TRACE delivers a granular ranking that uncovers critical trade-offs between agent accuracy, efficiency, and robustness entirely missed by singular metrics.", "AI": {"tldr": "Introduces TRACE, a trajectory-aware evaluation framework for deep research agents that goes beyond single-shot accuracy metrics to assess full reasoning processes, including efficiency, robustness, and latent capability.", "motivation": "Traditional evaluation of deep research agents relies on single outcome metrics like Pass@1 and static benchmarks, which create a misleading 'high-score illusion' and cannot capture process quality, robustness, or hidden capabilities. There is a need for an evaluation paradigm that reflects how well agents reason, use evidence, and respond to varying task difficulties and guidance levels.", "method": "Proposes TRACE, a Trajectory-Aware Comprehensive Evaluation framework that inspects the full problem-solving trajectory instead of just final answers. TRACE defines a Hierarchical Trajectory Utility Function to jointly measure process efficiency, cognitive quality (e.g., evidence grounding), and accuracy. Additionally, it introduces a Scaffolded Capability Assessment protocol that measures latent capability by determining the minimum external guidance required for an agent to succeed. A new benchmark, DeepResearch-Bench, is constructed with controllable task complexity to support these evaluations.", "result": "Using TRACE on DeepResearch-Bench, experiments yield fine-grained rankings of deep research agents that reveal important trade-offs between accuracy, efficiency, and robustness. These trade-offs are not visible when using standard single metrics like Pass@1, demonstrating that TRACE captures richer and more discriminative aspects of agent performance.", "conclusion": "Evaluating deep research agents requires trajectory-aware, multi-dimensional metrics rather than singular outcome-based scores. TRACE provides such a holistic framework, combining trajectory utility and scaffolded capability assessment to expose nuanced differences in agent behavior. This leads to more informative and realistic assessments of accuracy, efficiency, robustness, and latent abilities than conventional benchmarks can offer."}}
{"id": "2602.21262", "categories": ["cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21262", "abs": "https://arxiv.org/abs/2602.21262", "authors": ["Sasha Robinson", "Kerem Oktar", "Katherine M. Collins", "Ilia Sucholutsky", "Kelsey R. Allen"], "title": "Under the Influence: Quantifying Persuasion and Vigilance in Large Language Models", "comment": null, "summary": "With increasing integration of Large Language Models (LLMs) into areas of high-stakes human decision-making, it is important to understand the risks they introduce as advisors. To be useful advisors, LLMs must sift through large amounts of content, written with both benevolent and malicious intent, and then use this information to convince a user to take a specific action. This involves two social capacities: vigilance (the ability to determine which information to use, and which to discard) and persuasion (synthesizing the available evidence to make a convincing argument). While existing work has investigated these capacities in isolation, there has been little prior investigation of how these capacities may be linked. Here, we use a simple multi-turn puzzle-solving game, Sokoban, to study LLMs' abilities to persuade and be rationally vigilant towards other LLM agents. We find that puzzle-solving performance, persuasive capability, and vigilance are dissociable capacities in LLMs. Performing well on the game does not automatically mean a model can detect when it is being misled, even if the possibility of deception is explicitly mentioned. % as part of the prompt. However, LLMs do consistently modulate their token use, using fewer tokens to reason when advice is benevolent and more when it is malicious, even if they are still persuaded to take actions leading them to failure. To our knowledge, our work presents the first investigation of the relationship between persuasion, vigilance, and task performance in LLMs, and suggests that monitoring all three independently will be critical for future work in AI safety.", "AI": {"tldr": "The paper studies how well LLMs can both persuade and be vigilant against misleading advice in a controlled game setting, and shows these abilities are separable from raw task performance.", "motivation": "As LLMs are increasingly deployed as advisors in high-stakes domains, it is crucial to understand not only their problem-solving skills but also how they handle information from others\u2014distinguishing helpful from harmful input\u2014and how they persuade users. Existing research tends to examine persuasion or vigilance separately, leaving a gap in understanding how these two social capacities interact and how they relate to actual task success. This gap poses safety concerns because an LLM might be good at a task yet still be easily misled or overly persuasive in harmful ways.", "method": "The authors design a multi-turn puzzle-solving environment based on the game Sokoban, where LLM agents receive advice from other LLMs that can be benevolent or malicious. They then measure three aspects: (1) puzzle-solving performance (task success), (2) the persuasive power of advising models, and (3) the vigilance of receiving models\u2014i.e., their ability to detect and discount misleading advice. They also analyze how models allocate tokens for reasoning under different advice conditions, comparing behavior when advice is benevolent versus malicious and when the possibility of deception is made explicit in the prompt.", "result": "They find that the three capacities\u2014puzzle-solving performance, persuasion, and vigilance\u2014are dissociable: a model that solves puzzles well is not necessarily good at detecting deceptive advice. Even when prompted about the possibility of deception, models can still be successfully misled. Nonetheless, LLMs systematically adjust their reasoning verbosity: they tend to use fewer tokens when processing benevolent advice and more when handling malicious advice, indicating some sensitivity to the nature of the advice, though this does not reliably prevent them from being persuaded into failing actions.", "conclusion": "The study provides initial evidence that task performance, persuasive capability, and vigilance are distinct dimensions of LLM behavior that do not automatically align. Therefore, evaluating or improving only one of these dimensions is insufficient for ensuring safe deployment of LLMs as advisors. The authors argue that future AI safety work must separately monitor and optimize persuasion, vigilance, and task performance to mitigate risks when LLMs participate in high-stakes decision-making."}}
{"id": "2602.21265", "categories": ["cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21265", "abs": "https://arxiv.org/abs/2602.21265", "authors": ["Hyeonje Choi", "Jeongsoo Lee", "Hyojun Lee", "Jay-Yoon Lee"], "title": "ToolMATH: A Math Tool Benchmark for Realistic Long-Horizon Multi-Tool Reasoning", "comment": "Conference : Submitted to ICML 2026. 8 pages (+ abstract 16 pages), 5 figures", "summary": "We introduce \\ToolMATH, a math-grounded benchmark that evaluates tool-augmented language models in realistic multi-tool environments where the output depends on calling schema-specified tools and sustaining multi-step execution. It turns math problems into a controlled, correctness-checkable benchmark with tool sets, enabling systematic evaluation of model reliability under (1) large, overlapping tool catalogs and (2) the absence of the intended capability. \\ToolMATH provides actionable diagnostic evidence of failure modes in tool-augmented agents, helping identify the control mechanisms required for robustness. \\ToolMATH roughly contains 8k questions and 12k tools; we provide an additional hard-set \\ToolMATHHard with questions and tools. Our evaluation reveals that the key failure factor is due to the inability to reason, leading to the accumulation of intermediate results' errors and constrain later decisions. Tool-list redundancy do not simply add noise, but amplify small early deviations into irreversible execution drift. The benchmark highlights that when the intended capability is missing, distractor tools can sometimes serve as partial substitutes in solution paths, yet they can also mislead models into ungrounded tool trajectories. Finally, comparisons between tool-use protocols emphasize that improvements come less from local action selection and more from long-range plan coherence and disciplined use of observations.", "AI": {"tldr": "ToolMATH is a math-based benchmark to systematically test and diagnose tool-augmented language models in complex multi-tool settings.", "motivation": "Existing benchmarks for tool-using language models lack a controlled, correctness-checkable setting that stresses realistic multi-tool environments, including large overlapping tool catalogs and cases where the required capability is missing. The authors want a way to precisely measure reliability, reasoning quality, and failure modes of tool-augmented agents under these conditions.", "method": "They construct ToolMATH, which converts math problems into tool-using tasks whose outputs must be obtained via schema-specified tools and multi-step execution. The benchmark defines tool sets, including distractor and overlapping tools, and a hard subset ToolMATHHard. By designing tasks where correct answers can be checked and where intended tools may or may not exist, they can systematically probe how models select tools, chain calls, and adapt when capabilities are missing.", "result": "On ToolMATH, models often fail primarily due to weak reasoning, leading to compounding errors in intermediate steps that corrupt later decisions. Large, redundant tool lists do not just add random noise; they magnify early mistakes into irreversible execution drift. When the intended capability is absent, models sometimes exploit distractor tools as partial substitutes, but these can also cause ungrounded and misleading tool-use trajectories. Comparisons across tool-use protocols show that performance gains are more tied to having coherent long-range plans and disciplined use of observations than to better one-step action choices.", "conclusion": "ToolMATH exposes core reliability issues in tool-augmented LMs, especially their difficulty in maintaining correct multi-step reasoning in rich tool environments. It shows that robustness depends less on local action heuristics and more on global plan coherence, error control in intermediate steps, and careful interpretation of observations. The benchmark serves as a diagnostic instrument for designing better control mechanisms and protocols for tool-using language agents."}}
{"id": "2602.21346", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21346", "abs": "https://arxiv.org/abs/2602.21346", "authors": ["Mengxuan Hu", "Vivek V. Datla", "Anoop Kumar", "Zihan Guan", "Sheng Li", "Alfy Samuel", "Daben Liu"], "title": "Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment", "comment": null, "summary": "Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise harmful intent through indirect or deceptive phrasing. Using causal intervention, we empirically demonstrate that this vulnerability stems from shallow alignment mechanisms that lack deep reasoning, often rejecting harmful prompts without truly understanding why they are harmful. To mitigate this vulnerability, we propose enhancing alignment through reasoning-aware post-training. We construct and release a novel Chain-of-Thought (CoT) fine-tuning dataset that includes both utility-oriented and safety-critical prompts with step-by-step rationales. Fine-tuning on this dataset encourages models to produce principled refusals grounded in reasoning, outperforming standard SFT baselines. Furthermore, inspired by failure patterns in CoT fine-tuning, we introduce Alignment-Weighted DPO, which targets the most problematic parts of an output by assigning different preference weights to the reasoning and final-answer segments. This produces finer-grained, targeted updates than vanilla DPO and improves robustness to diverse jailbreak strategies. Extensive experiments across multiple safety and utility benchmarks show that our method consistently improves alignment robustness while maintaining overall model utility.", "AI": {"tldr": "The paper identifies that current LLM alignment methods (SFT, RLHF, DPO) are shallow and vulnerable to jailbreaks, and proposes reasoning-aware post-training with a Chain-of-Thought dataset plus Alignment-Weighted DPO to improve safety robustness without hurting utility.", "motivation": "Although modern alignment techniques improve LLM safety, models can still be jailbroken by prompts that obscure harmful intent. The authors hypothesize that this is because alignment is superficial: models often refuse on surface patterns rather than through genuine understanding of harm. There is a need for alignment methods that incorporate explicit reasoning about safety so that refusals are principled and robust to indirect or deceptive phrasing.", "method": "1) Use causal intervention to empirically show that safety behaviors are shallow and not truly grounded in reasoning. 2) Build a new Chain-of-Thought fine-tuning dataset that includes both utility and safety prompts along with step-by-step rationales, especially for refusals. Fine-tune LLMs on this dataset to induce reasoning-based safety behavior. 3) Analyze remaining failure cases and design Alignment-Weighted DPO, a variant of DPO that treats the reasoning segment and the final answer segment with different preference weights, so that training focuses more on problematic parts of responses. 4) Evaluate models on multiple safety and utility benchmarks and compare against standard SFT and vanilla DPO baselines.", "result": "Models fine-tuned with the reasoning-aware CoT dataset produce more principled, well-justified refusals and are less vulnerable to jailbreak attempts than models trained with standard SFT. Alignment-Weighted DPO further improves robustness by enabling finer-grained optimization: it specifically strengthens safety-relevant reasoning while preserving or enhancing task performance. Across diverse safety and utility benchmarks, the proposed approach consistently outperforms baselines in robustness to jailbreaks while maintaining high utility.", "conclusion": "Shallow alignment in current LLMs leads to vulnerability against indirect or deceptive harmful prompts. Explicitly training models to reason about safety, via CoT-style rationales and an Alignment-Weighted DPO objective, yields deeper, more principled alignment. This approach improves robustness to jailbreak strategies without sacrificing general utility, suggesting that future alignment work should incorporate structured reasoning and segment-aware optimization rather than relying solely on outcome-level preferences."}}
{"id": "2602.21374", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21374", "abs": "https://arxiv.org/abs/2602.21374", "authors": ["Mohammadreza Ghaffarzadeh-Esfahani", "Nahid Yousefian", "Ebrahim Heidari-Farsani", "Ali Akbar Omidvarian", "Sepehr Ghahraei", "Atena Farangi", "AmirBahador Boroumand"], "title": "Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages", "comment": "16 pages, 3 figures, 2 supplementary files", "summary": "Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) -- Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it -- for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B--8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications.", "AI": {"tldr": "The paper evaluates a translation-plus-small-language-model pipeline to extract 13 clinical features from low-resource Persian medical call-center transcripts, showing that larger open-source SLMs with Persian\u2192English translation can achieve strong, practical performance without fine-tuning, especially for physiological symptoms.", "motivation": "Clinical NLP tools are mostly developed for high-resource languages like English, leaving a gap for low-resource languages such as Persian, particularly in sensitive domains like palliative cancer care. Manual annotation is costly and there are privacy and infrastructure constraints in healthcare settings. The authors aim to find a practical, low-resource, and privacy-preserving method for structured clinical information extraction from Persian transcripts using open-source models that can run with limited compute, while understanding how model size and translation strategies affect performance.", "method": "The authors design a two-step pipeline: (1) use Aya-expanse-8B as a Persian-to-English translation model for medical call-center transcripts; (2) apply five open-source instruction-tuned small language models (Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, Gemma-3-1B-it) to perform binary extraction of 13 predefined clinical features on 1,221 anonymized cancer palliative care calls. They employ few-shot prompting without any task-specific fine-tuning. Performance is evaluated using macro-averaged F1, Matthews Correlation Coefficient, sensitivity, and specificity to account for class imbalance. They also perform a bilingual analysis comparing performance on original Persian vs translated English transcripts to understand the effect of translation on extraction quality.", "result": "Qwen2.5-7B-Instruct achieved the best performance (median macro-F1 \u2248 0.899; MCC \u2248 0.797), while the smallest Gemma-3-1B-it model performed worst. Models with 7B\u20138B parameters consistently outperformed 1\u20133B models, especially in sensitivity and MCC. Translating Persian transcripts to English using Aya-expanse-8B improved sensitivity, reduced missing or null outputs, and increased metrics robust to class imbalance (such as MCC), but caused a modest drop in specificity and precision. At the feature level, physiological symptoms were extracted reliably by most models; in contrast, psychological complaints, administrative or logistical requests, and more complex somatic issues were much harder to detect accurately.", "conclusion": "The study demonstrates that a translation-augmented pipeline using open-source small language models can effectively extract structured clinical information from low-resource language medical transcripts without fine-tuning, and with limited computing resources. It offers a practical, privacy-preserving blueprint for deploying clinical NLP in multilingual, resource-constrained healthcare environments. The findings emphasize that choosing an appropriate model size (favoring 7B\u20138B SLMs) and carefully selecting the input language (via translation when beneficial) are crucial design choices for building robust, sensitive information extraction systems in healthcare applications."}}
{"id": "2602.21377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21377", "abs": "https://arxiv.org/abs/2602.21377", "authors": ["Felix Schneider", "Maria Gogolev", "Sven Sickert", "Joachim Denzler"], "title": "Beyond Subtokens: A Rich Character Embedding for Low-resource and Morphologically Complex Languages", "comment": "12 content pages, 2 figures, 8 tables, one example textbox", "summary": "Tokenization and sub-tokenization based models like word2vec, BERT and the GPTs are the state-of-the-art in natural language processing. Typically, these approaches have limitations with respect to their input representation. They fail to fully capture orthographic similarities and morphological variations, especially in highly inflected and under-resource languages. To mitigate this problem, we propose to computes word vectors directly from character strings, integrating both semantic and syntactic information. We denote this transformer-based approach Rich Character Embeddings (RCE). Furthermore, we propose a hybrid model that combines transformer and convolutional mechanisms. Both vector representations can be used as a drop-in replacement for dictionary- and subtoken-based word embeddings in existing model architectures. It has the potential to improve performance for both large context-based language models like BERT and small models like word2vec for under-resourced and morphologically rich languages. We evaluate our approach on various tasks like the SWAG, declension prediction for inflected languages, metaphor and chiasmus detection for various languages. Our experiments show that it outperforms traditional token-based approaches on limited data using OddOneOut and TopK metrics.", "AI": {"tldr": "Paper proposes Rich Character Embeddings (RCE), a transformer-based method computing word vectors directly from characters, plus a transformer\u2013CNN hybrid, to better handle morphology and orthography, outperforming token-based models on low-resource, morphologically rich languages.", "motivation": "Token- and subtoken-based models like word2vec, BERT, and GPT struggle to represent orthographic similarities and morphological variation, especially in highly inflected and under-resourced languages, limiting their effectiveness on such languages.", "method": "Introduce Rich Character Embeddings (RCE), a transformer-based model that produces word embeddings directly from raw character sequences, integrating semantic and syntactic cues. Additionally, design a hybrid architecture combining transformer layers with convolutional layers. Both resulting embeddings are intended as drop-in replacements for traditional dictionary- or subtoken-based embeddings in existing NLP architectures and language models.", "result": "On multiple tasks\u2014including SWAG, declension prediction in inflected languages, and metaphor/chiasmus detection across languages\u2014the proposed character-based and hybrid embeddings outperform standard token-based embeddings when training data is limited, as measured by OddOneOut and TopK evaluation metrics.", "conclusion": "Direct character-level transformer embeddings and a transformer\u2013CNN hybrid can better capture morphology and orthography than token-based embeddings, improving performance in low-resource, morphologically rich settings and serving as compatible replacements within existing NLP models."}}
{"id": "2602.21379", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21379", "abs": "https://arxiv.org/abs/2602.21379", "authors": ["Daniel Tamayo", "I\u00f1aki Lacunza", "Paula Rivera-Hidalgo", "Severino Da Dalt", "Javier Aula-Blasco", "Aitor Gonzalez-Agirre", "Marta Villegas"], "title": "MrBERT: Modern Multilingual Encoders via Vocabulary, Domain, and Dimensional Adaptation", "comment": "24 pages, 14 tables and 4 figures", "summary": "We introduce MrBERT, a family of 150M-300M parameter encoders built on the ModernBERT architecture and pre-trained on 35 languages and code. Through targeted adaptation, this model family achieves state-of-the-art results on Catalan- and Spanish-specific tasks, while establishing robust performance across specialized biomedical and legal domains. To bridge the gap between research and production, we incorporate Matryoshka Representation Learning (MRL), enabling flexible vector sizing that significantly reduces inference and storage costs. Ultimately, the MrBERT family demonstrates that modern encoder architectures can be optimized for both localized linguistic excellence and efficient, high-stakes domain specialization. We open source the complete model family on Huggingface.", "AI": {"tldr": "MrBERT is a multilingual encoder family (150M\u2013300M params) based on ModernBERT, delivering SOTA performance for Catalan/Spanish and strong results in biomedical and legal domains, while using Matryoshka Representation Learning to allow flexible, smaller embeddings for cheaper inference and storage. The models are released on Hugging Face.", "motivation": "Existing encoders often trade off between (1) strong performance in specific languages (like Catalan and Spanish), (2) robustness in specialized domains (biomedical, legal), and (3) production efficiency (inference speed, storage). The authors aim to show that a modern encoder architecture can be tuned to excel in these languages and domains while remaining practical and efficient for real-world use.", "method": "They build a family of 150M\u2013300M parameter encoder models on top of the ModernBERT architecture, pre-trained jointly on 35 natural languages plus code. They then perform targeted adaptation to Catalan, Spanish, and domain-specific corpora (biomedical, legal). To reduce deployment cost, they use Matryoshka Representation Learning (MRL), which trains embeddings to be useful at multiple vector dimensions so that shorter vectors can be used in production with minimal performance loss.", "result": "The resulting MrBERT models reach state-of-the-art performance on tasks specific to Catalan and Spanish, and show strong, robust performance on benchmarks in biomedical and legal domains. Due to MRL, the models can operate with reduced embedding sizes, cutting inference and storage overhead while maintaining high accuracy.", "conclusion": "Modern encoder architectures can be designed to simultaneously achieve (a) high accuracy for particular languages, (b) strong specialization in high-stakes domains, and (c) practical efficiency via flexible representation sizes. MrBERT exemplifies this combination and is made publicly available on Hugging Face for research and production use."}}
{"id": "2602.21461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21461", "abs": "https://arxiv.org/abs/2602.21461", "authors": ["Xiaoke Huang", "Bhavul Gauri", "Kam Woh Ng", "Tony Ng", "Mengmeng Xu", "Zhiheng Liu", "Weiming Ren", "Zhaochong An", "Zijian Zhou", "Haonan Qiu", "Yuyin Zhou", "Sen He", "Ziheng Wang", "Tao Xiang", "Xiao Han"], "title": "VecGlypher: Unified Vector Glyph Generation with Language Models", "comment": "Accepted to CVPR'26. Project page: https://xk-huang.github.io/VecGlypher/", "summary": "Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.", "AI": {"tldr": "VecGlypher is a multimodal language model that directly generates editable vector font glyphs (SVG paths) from text or images, bypassing rasterization.", "motivation": "Existing learning-based font/glyph generation methods rely on curated exemplar sheets and raster-to-vector postprocessing, which hurts accessibility, editability, and scalability. There is a need for a system that can natively generate high-quality vector glyphs conditioned on language and images, and that generalizes across font families.", "method": "They build a single autoregressive multimodal language model that outputs SVG path tokens for glyphs given a style prompt, optional reference glyph images, and a target character. Training uses a two-stage typography-aware pipeline: (1) large-scale pretraining/continuation on 39K noisy Envato fonts to learn SVG syntax and long-range geometry, and (2) post-training on 2.5K curated Google Fonts with expert tags and exemplars to align language and imagery with geometry. Preprocessing normalizes coordinate frames, canonicalizes and de-duplicates paths, and quantizes coordinates for stable long-sequence decoding. They compare coordinate serializations and model scales via ablations.", "result": "On cross-family out-of-distribution tests, VecGlypher significantly outperforms general-purpose LLMs and specialized vector-font models for text-only glyph generation and achieves state-of-the-art performance for image-referenced generation, improving notably over DeepVecFont-v2 and DualVector. Ablations show that larger models and the two-stage training recipe are crucial, and that absolute-coordinate serialization yields the best geometric quality.", "conclusion": "VecGlypher can reliably synthesize watertight, editable vector glyphs from text and image prompts, surpassing prior methods and removing the need for raster intermediates. This reduces the barrier to font creation, enables designing fonts via natural language or exemplar glyphs, and offers a scalable base for future multimodal design tools and workflows."}}
{"id": "2602.21485", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.21485", "abs": "https://arxiv.org/abs/2602.21485", "authors": ["Deja Dunlap", "R. Thomas McCoy"], "title": "Evaluating the Usage of African-American Vernacular English in Large Language Models", "comment": null, "summary": "In AI, most evaluations of natural language understanding tasks are conducted in standardized dialects such as Standard American English (SAE). In this work, we investigate how accurately large language models (LLMs) represent African American Vernacular English (AAVE). We analyze three LLMs to compare their usage of AAVE to the usage of humans who natively speak AAVE. We first analyzed interviews from the Corpus of Regional African American Language and TwitterAAE to identify the typical contexts where people use AAVE grammatical features such as ain't. We then prompted the LLMs to produce text in AAVE and compared the model-generated text to human usage patterns. We find that, in many cases, there are substantial differences between AAVE usage in LLMs and humans: LLMs usually underuse and misuse grammatical features characteristic of AAVE. Furthermore, through sentiment analysis and manual inspection, we found that the models replicated stereotypes about African Americans. These results highlight the need for more diversity in training data and the incorporation of fairness methods to mitigate the perpetuation of stereotypes.", "AI": {"tldr": "The paper evaluates how well large language models represent African American Vernacular English (AAVE) and finds substantial mismatches and harmful stereotypes.", "motivation": "Most NLP evaluations focus on standardized dialects like Standard American English, neglecting non-standard but widely spoken dialects such as AAVE. This raises concerns about fairness, representation, and the potential reinforcement of stereotypes when LLMs handle these dialects poorly. The authors aim to understand whether LLMs accurately model AAVE and whether they propagate biases related to African American speakers.", "method": "The authors examine three large language models and compare their AAVE usage to that of native AAVE speakers. They analyze interviews from the Corpus of Regional African American Language and data from TwitterAAE to determine real-world contexts and patterns where AAVE grammatical features (e.g., the use of \"ain't\") occur. Then they prompt LLMs to generate text in AAVE and systematically compare model-generated usage with human usage patterns. Additionally, they conduct sentiment analysis and manual qualitative inspection of the models' outputs to detect stereotypical or biased content.", "result": "The study finds substantial differences between AAVE usage by humans and by LLMs. The models generally underuse and misuse key grammatical features that characterize AAVE, indicating an inaccurate representation of the dialect. Sentiment analysis and manual review further reveal that the LLMs often reproduce negative stereotypes about African Americans in their AAVE outputs.", "conclusion": "The paper concludes that current LLMs do not faithfully model AAVE and can reinforce harmful stereotypes. This underscores the importance of including more diverse linguistic data in model training and explicitly incorporating fairness and debiasing methods to reduce stereotyping and improve equitable language representation across dialects."}}
{"id": "2602.21543", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.21543", "abs": "https://arxiv.org/abs/2602.21543", "authors": ["Barah Fazili", "Koustava Goswami"], "title": "Enhancing Multilingual Embeddings via Multi-Way Parallel Text Alignment", "comment": null, "summary": "Multilingual pretraining typically lacks explicit alignment signals, leading to suboptimal cross-lingual alignment in the representation space. In this work, we show that training standard pretrained models for cross-lingual alignment with a multi-way parallel corpus in a diverse pool of languages can substantially improve multilingual and cross-lingual representations for NLU tasks. We construct a multi-way parallel dataset using translations of English text from an off-the-shelf NMT model for a pool of six target languages and achieve strong cross-lingual alignment through contrastive learning. This leads to substantial performance gains across both seen and unseen languages for multiple tasks from the MTEB benchmark evaluated for XLM-Roberta and multilingual BERT base models. Using a multi-way parallel corpus for contrastive training yields substantial gains on bitext mining (21.3%), semantic similarity (5.3%), and classification (28.4%) compared to English-centric (En-X) bilingually parallel data, where X is sampled from a pool of multiple target languages. Furthermore, finetuning mE5 model on a small dataset with multi-way parallelism significantly improves bitext mining compared to one without, underscoring the importance of multi-way cross-lingual supervision even for models already pretrained for high-quality sentence embeddings.", "AI": {"tldr": "They improve multilingual sentence representations by aligning multiple languages at once using contrastive learning on a multi-way parallel corpus, yielding big gains on cross-lingual NLU tasks.", "motivation": "Standard multilingual pretrained language models (like mBERT and XLM-R) don\u2019t explicitly align representations across languages, so cross-lingual transfer and retrieval can be suboptimal. Existing bilingual (English\u2013X) contrastive training may also be limited because it doesn\u2019t exploit richer supervision when many languages express the same content. The authors want to investigate whether using multi-way parallel data\u2014where the same sentence is available in several languages simultaneously\u2014can better align multilingual embeddings and improve performance on downstream cross-lingual tasks, including for languages not seen during the alignment training.", "method": "They build a multi-way parallel corpus by translating English sentences into six target languages using an off\u2011the\u2011shelf neural machine translation system. Each English sentence plus its six translations forms a multi-way parallel set. They use these sets to contrastively fine-tune standard multilingual encoders (XLM\u2011Roberta base, multilingual BERT base, and mE5): sentences that are translations of each other are pulled together in the embedding space, while non-equivalent sentences are pushed apart. They compare this multi-way contrastive training with a more common English-centric bilingual setting, where each English sentence is paired with a single translation in one language sampled from the same pool. They then evaluate on multiple tasks from MTEB, focusing on bitext mining, semantic similarity, and classification across seen and unseen languages.", "result": "Multi-way contrastive training substantially improves cross-lingual alignment and yields large performance gains on MTEB tasks compared with English\u2013X bilingual contrastive training. Specifically, they report relative improvements of about 21.3% on bitext mining, 5.3% on semantic similarity, and 28.4% on classification. The improvements hold for both languages used in training and unseen languages. Additionally, even for mE5, a model already trained for strong multilingual sentence embeddings, further fine-tuning on a relatively small multi-way parallel dataset noticeably boosts bitext mining performance over fine-tuning without multi-way parallel supervision.", "conclusion": "Explicit cross-lingual alignment with multi-way parallel data is a powerful and generally applicable strategy to enhance multilingual encoders. Multi-way contrastive learning produces better-aligned embedding spaces than English-centric bilingual training, leading to consistent and sometimes large gains on diverse cross-lingual NLU tasks, including for languages not present in the alignment data. The results suggest that incorporating multi-way cross-lingual supervision is beneficial even for strong pretrained sentence-embedding models and should be preferred when such data can be constructed, e.g., using machine translation."}}
{"id": "2602.21608", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21608", "abs": "https://arxiv.org/abs/2602.21608", "authors": ["Kazi Samin Yasar Alam", "Md Tanbir Chowdhury", "Tamim Ahmed", "Ajwad Abrar", "Md Rafid Haque"], "title": "MixSarc: A Bangla-English Code-Mixed Corpus for Implicit Meaning Identification", "comment": "Under Review", "summary": "Bangla-English code-mixing is widespread across South Asian social media, yet resources for implicit meaning identification in this setting remain scarce. Existing sentiment and sarcasm models largely focus on monolingual English or high-resource languages and struggle with transliteration variation, cultural references, and intra-sentential language switching. To address this gap, we introduce MixSarc, the first publicly available Bangla-English code-mixed corpus for implicit meaning identification. The dataset contains 9,087 manually annotated sentences labeled for humor, sarcasm, offensiveness, and vulgarity. We construct the corpus through targeted social media collection, systematic filtering, and multi-annotator validation. We benchmark transformer-based models and evaluate zero-shot large language models under structured prompting. Results show strong performance on humor detection but substantial degradation on sarcasm, offense, and vulgarity due to class imbalance and pragmatic complexity. Zero-shot models achieve competitive micro-F1 scores but low exact match accuracy. Further analysis reveals that over 42\\% of negative sentiment instances in an external dataset exhibit sarcastic characteristics. MixSarc provides a foundational resource for culturally aware NLP and supports more reliable multi-label modeling in code-mixed environments.", "AI": {"tldr": "The paper introduces MixSarc, the first publicly available Bangla-English code-mixed dataset for identifying implicit meanings (humor, sarcasm, offensiveness, vulgarity) in social media texts, benchmarks transformer and zero-shot LLM baselines, and highlights challenges in sarcasm and offensive language detection due to class imbalance and pragmatic complexity.", "motivation": "Bangla-English code-mixing is very common on South Asian social media, but current NLP resources and models are focused on monolingual or high-resource languages and fail on transliteration variation, cultural cues, and intra-sentential language switching. There is a particular lack of datasets and models for detecting implicit meanings like sarcasm, humor, and offensive or vulgar language in this code-mixed setting, limiting progress on culturally aware, robust NLP for these communities.", "method": "The authors curate MixSarc by collecting targeted social media posts, applying systematic filtering, and performing manual multi-annotator labeling of 9,087 Bangla-English code-mixed sentences for four implicit meaning dimensions: humor, sarcasm, offensiveness, and vulgarity. They then benchmark several transformer-based models on this multi-label task and evaluate zero-shot large language models using structured prompting schemes, analyzing performance by label and task difficulty, as well as examining sarcastic characteristics in an external negative-sentiment dataset.", "result": "Transformer-based models show strong performance for humor detection but perform notably worse on sarcasm, offensive, and vulgarity labels, with degradation linked to class imbalance and the pragmatic complexity of these phenomena. Zero-shot large language models reach competitive micro-F1 scores relative to supervised baselines but lag in exact match accuracy, indicating difficulties in jointly predicting all labels correctly. Additionally, a detailed analysis of an external dataset finds that over 42% of negative sentiment instances exhibit sarcastic traits, underscoring the prevalence and importance of sarcasm in this context.", "conclusion": "MixSarc offers a novel, publicly available Bangla-English code-mixed corpus tailored to implicit meaning identification, enabling more robust multi-label modeling for humor, sarcasm, offensiveness, and vulgarity in culturally specific, code-mixed social media text. The mixed benchmark results highlight that while current models can handle humor reasonably well, sarcasm and offensive language remain challenging due to pragmatic subtleties and data imbalance. The findings emphasize the necessity of specialized resources and modeling strategies for code-mixed, culturally rich language, and position MixSarc as a foundational dataset for future advances in this area."}}
{"id": "2602.21619", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21619", "abs": "https://arxiv.org/abs/2602.21619", "authors": ["Muku Akasaka", "Soyeon Caren Han"], "title": "When More Is Less: A Systematic Analysis of Spatial and Commonsense Information for Visual Spatial Reasoning", "comment": "5 pages, 6 figures, Under review", "summary": "Visual spatial reasoning (VSR) remains challenging for modern vision-language models (VLMs), despite advances in multimodal architectures. A common strategy is to inject additional information at inference time, such as explicit spatial cues, external commonsense knowledge, or chain-of-thought (CoT) reasoning instructions. However, it remains unclear when such information genuinely improves reasoning and when it introduces noise. In this paper, we conduct a hypothesis-driven analysis of information injection for VSR across three representative VLMs and two public benchmarks. We examine (i) the type and number of spatial contexts, (ii) the amount and relevance of injected commonsense knowledge, and (iii) the interaction between spatial grounding and CoT prompting. Our results reveal a consistent pattern: more information does not necessarily yield better reasoning. Targeted single spatial cues outperform multi-context aggregation, excessive or weakly relevant commonsense knowledge degrades performance, and CoT prompting improves accuracy only when spatial grounding is sufficiently precise. These findings highlight the importance of selective, task-aligned information injection and provide practical guidance for designing reliable multimodal reasoning pipelines.", "AI": {"tldr": "The paper studies when adding extra information (spatial cues, commonsense knowledge, chain-of-thought prompts) actually helps or hurts visual spatial reasoning in vision-language models, finding that more information is not always better and that targeted, relevant signals work best.", "motivation": "Visual spatial reasoning is still hard for current vision-language models, and many recent methods try to fix this by injecting extra information at inference time (e.g., spatial annotations, knowledge, or CoT prompts). However, it is unclear under what conditions such additional signals truly help rather than introduce noise. The paper aims to systematically understand the effectiveness and failure modes of different information-injection strategies for improving VSR.", "method": "The authors perform a hypothesis-driven empirical analysis using three representative vision-language models and two public VSR benchmarks. They systematically vary: (i) the type and number of injected spatial contexts, (ii) the quantity and relevance of commonsense knowledge added, and (iii) the presence and role of chain-of-thought prompting combined with different levels of spatial grounding. Performance and error patterns are compared across these controlled settings to test specific hypotheses about information injection.", "result": "Experiments show that (1) targeted single spatial cues outperform aggregating multiple spatial contexts; (2) injecting too much or weakly relevant commonsense knowledge reduces performance; and (3) chain-of-thought prompting improves accuracy only when the underlying spatial grounding is already precise, otherwise it can propagate or amplify errors.", "conclusion": "The study concludes that more information is not inherently better for visual spatial reasoning in VLMs. Instead, carefully selected, task-aligned, and well-grounded information is crucial. The work advocates for selective information injection strategies and offers practical guidance for building more robust multimodal reasoning pipelines rather than blindly adding additional context or reasoning prompts."}}
{"id": "2602.21628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21628", "abs": "https://arxiv.org/abs/2602.21628", "authors": ["Yukun Chen", "Jiaming Li", "Longze Chen", "Ze Gong", "Jingpeng Li", "Zhen Qin", "Hengyu Chang", "Ancheng Xu", "Zhihao Yang", "Hamid Alinejad-Rokny", "Qiang Qu", "Bo Zheng", "Min Yang"], "title": "RuCL: Stratified Rubric-Based Curriculum Learning for Multimodal Large Language Model Reasoning", "comment": "8 pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a prevailing paradigm for enhancing reasoning in Multimodal Large Language Models (MLLMs). However, relying solely on outcome supervision risks reward hacking, where models learn spurious reasoning patterns to satisfy final answer checks. While recent rubric-based approaches offer fine-grained supervision signals, they suffer from high computational costs of instance-level generation and inefficient training dynamics caused by treating all rubrics as equally learnable. In this paper, we propose Stratified Rubric-based Curriculum Learning (RuCL), a novel framework that reformulates curriculum learning by shifting the focus from data selection to reward design. RuCL generates generalized rubrics for broad applicability and stratifies them based on the model's competence. By dynamically adjusting rubric weights during training, RuCL guides the model from mastering foundational perception to tackling advanced logical reasoning. Extensive experiments on various visual reasoning benchmarks show that RuCL yields a remarkable +7.83% average improvement over the Qwen2.5-VL-7B model, achieving a state-of-the-art accuracy of 60.06%.", "AI": {"tldr": "The paper introduces RuCL, a rubric-based curriculum learning framework to improve reasoning in multimodal LLMs trained with verifiable rewards, achieving state-of-the-art performance and significant gains over a strong baseline.", "motivation": "Existing RL with verifiable rewards for MLLMs often relies on outcome-only supervision, leading to reward hacking and spurious reasoning. Rubric-based methods provide richer supervision but are computationally expensive and assume all rubrics are equally easy to learn, which slows and destabilizes training. There is a need for a more efficient, structured way to use rubrics as rewards that aligns with the model\u2019s evolving competence.", "method": "RuCL (Stratified Rubric-based Curriculum Learning) reframes curriculum learning as a reward-design problem. It first generates generalized, reusable rubrics that capture different aspects of visual reasoning. These rubrics are then stratified into levels according to difficulty and the model\u2019s current competence. During training, RuCL dynamically adjusts the weights of different rubric strata, guiding the model from easier, perception-oriented criteria to harder, logic-oriented criteria, instead of treating all rubric components equally throughout training.", "result": "On multiple visual reasoning benchmarks, applying RuCL to the Qwen2.5-VL-7B model yields an average accuracy improvement of +7.83%, reaching 60.06% accuracy, which is reported as state-of-the-art for the evaluated setting.", "conclusion": "Curriculum learning over rubric-based rewards\u2014via generalized, stratified rubrics with adaptive weighting\u2014more effectively trains MLLMs under RL with verifiable rewards. RuCL mitigates reward hacking, improves training efficiency and dynamics, and substantially boosts visual reasoning performance compared to prior rubric- and outcome-based approaches."}}
{"id": "2602.21647", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21647", "abs": "https://arxiv.org/abs/2602.21647", "authors": ["Tangsang Chongbang", "Pranesh Pyara Shrestha", "Amrit Sarki", "Anku Jaiswal"], "title": "Mitigating Structural Noise in Low-Resource S2TT: An Optimized Cascaded Nepali-English Pipeline with Punctuation Restoration", "comment": "13 pages, 4 figures, 12 tables", "summary": "This paper presents and evaluates an optimized cascaded Nepali speech-to-English text translation (S2TT) system, focusing on mitigating structural noise introduced by Automatic Speech Recognition (ASR). We first establish highly proficient ASR and NMT components: a Wav2Vec2-XLS-R-300m model achieved a state-of-the-art 2.72% CER on OpenSLR-54, and a multi-stage fine-tuned MarianMT model reached a 28.32 BLEU score on the FLORES-200 benchmark. We empirically investigate the influence of punctuation loss, demonstrating that unpunctuated ASR output significantly degrades translation quality, causing a massive 20.7% relative BLEU drop on the FLORES benchmark. To overcome this, we propose and evaluate an intermediate Punctuation Restoration Module (PRM). The final S2TT pipeline was tested across three configurations on a custom dataset. The optimal configuration, which applied the PRM directly to ASR output, achieved a 4.90 BLEU point gain over the direct ASR-to-NMT baseline (BLEU 36.38 vs. 31.48). This improvement was validated by human assessment, which confirmed the optimized pipeline's superior Adequacy (3.673) and Fluency (3.804). This work validates that targeted punctuation restoration is the most effective intervention for mitigating structural noise in the Nepali S2TT pipeline. It establishes an optimized baseline and demonstrates a critical architectural insight for developing cascaded speech translation systems for similar low-resource languages.", "AI": {"tldr": "The paper builds a cascaded Nepali speech-to-English translation system and shows that inserting a punctuation restoration module between ASR and MT substantially boosts translation quality.", "motivation": "Cascaded speech translation for low-resource languages like Nepali suffers from \"structural noise\" coming from ASR, especially the loss of punctuation, which harms downstream machine translation. There is limited work quantifying this effect or systematically optimizing cascaded pipelines for such languages.", "method": "1) Train and fine-tune strong individual components: a Wav2Vec2-XLS-R-300m ASR model for Nepali and a multi-stage fine-tuned MarianMT model for Nepali\u2013English NMT. 2) Empirically analyze how removing punctuation from source text affects translation quality (BLEU) on FLORES-200, simulating unpunctuated ASR output. 3) Design an intermediate Punctuation Restoration Module (PRM) to reinsert punctuation into ASR hypotheses. 4) Build three S2TT pipeline configurations (with/without PRM and different placements), and evaluate them on a custom Nepali S2TT dataset using BLEU and human adequacy/fluency scores.", "result": "The ASR model achieved a state-of-the-art 2.72% character error rate on OpenSLR-54. The MarianMT model reached 28.32 BLEU on FLORES-200. Removing punctuation caused about a 20.7% relative drop in BLEU, showing punctuation loss seriously harms translation. In the full S2TT system, the configuration inserting the PRM directly after ASR achieved BLEU 36.38, a 4.90 BLEU point gain over the baseline cascaded system without PRM (31.48). Human evaluation corroborated these gains, with higher adequacy (3.673) and fluency (3.804) scores for the optimized pipeline.", "conclusion": "Punctuation loss is a major source of structural noise in Nepali speech-to-text translation cascades, and explicitly restoring punctuation between ASR and NMT is the most effective mitigation. The proposed pipeline sets a strong optimized baseline for Nepali S2TT and offers an architectural insight\u2014targeted punctuation restoration\u2014for building better cascaded speech translation systems in other low-resource languages with similar challenges."}}
{"id": "2602.21638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21638", "abs": "https://arxiv.org/abs/2602.21638", "authors": ["Anqi Li", "Ruihan Wang", "Zhaoming Chen", "Yuqian Chen", "Yu Lu", "Yi Zhu", "Yuan Xie", "Zhenzhong Lan"], "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs", "comment": "8 pages", "summary": "Effectively addressing client resistance is a sophisticated clinical skill in psychological counseling, yet practitioners often lack timely and scalable supervisory feedback to refine their approaches. Although current NLP research has examined overall counseling quality and general therapeutic skills, it fails to provide granular evaluations of high-stakes moments where clients exhibit resistance. In this work, we present a comprehensive pipeline for the multi-dimensional evaluation of human counselors' interventions specifically targeting client resistance in text-based therapy. We introduce a theory-driven framework that decomposes counselor responses into four distinct communication mechanisms. Leveraging this framework, we curate and share an expert-annotated dataset of real-world counseling excerpts, pairing counselor-client interactions with professional ratings and explanatory rationales. Using this data, we perform full-parameter instruction tuning on a Llama-3.1-8B-Instruct backbone to model fine-grained evaluative judgments of response quality and generate explanations underlying. Experimental results show that our approach can effectively distinguish the quality of different communication mechanisms (77-81% F1), substantially outperforming GPT-4o and Claude-3.5-Sonnet (45-59% F1). Moreover, the model produces high-quality explanations that closely align with expert references and receive near-ceiling ratings from human experts (2.8-2.9/3.0). A controlled experiment with 43 counselors further confirms that receiving these AI-generated feedback significantly improves counselors' ability to respond effectively to client resistance.", "AI": {"tldr": "The paper introduces an NLP-based evaluation system that provides fine-grained, theory-based feedback on how counselors respond to client resistance in text therapy, showing strong performance versus frontier LLMs and measurable benefits for practicing counselors.", "motivation": "Client resistance is a critical, high-stakes moment in counseling, and handling it well requires nuanced skills. Human supervision is costly and not scalable, and existing NLP work only assesses broad counseling quality rather than the specific, moment-level strategies counselors use when clients are resistant. The authors aim to fill this gap so that counselors can receive targeted, scalable feedback and training support.", "method": "The authors design a theory-driven framework that breaks counselor responses aimed at handling client resistance into four communication mechanisms. They compile a dataset of real counseling excerpts where domain experts annotate counselor-client exchanges with ratings and explanatory rationales according to this framework. Using this expert-annotated corpus, they perform full-parameter instruction tuning on a Llama-3.1-8B-Instruct model so it can (1) evaluate the quality of each mechanism in a given response and (2) generate natural-language explanations for its judgments. They then benchmark the tuned model against GPT-4o and Claude-3.5-Sonnet and run a controlled user study with 43 counselors who receive the model\u2019s feedback and are later tested on their responses to client resistance.", "result": "The tuned Llama-based model accurately distinguishes quality levels of the four communication mechanisms, achieving 77\u201381% F1, which substantially outperforms GPT-4o and Claude-3.5-Sonnet baselines (45\u201359% F1). Its generated explanations align closely with expert rationales and are rated highly by professional evaluators (around 2.8\u20132.9 out of 3). In the controlled experiment with 43 practicing counselors, those who received the model\u2019s feedback showed significant improvement in their subsequent responses to client resistance compared to controls.", "conclusion": "A theory-grounded, instruction-tuned LLM can give reliable, fine-grained evaluations and explanations of counselor interventions specifically targeting client resistance. This AI feedback not only matches expert judgments at the mechanism level but also leads to measurable skill gains in real counselors, suggesting that such systems can augment clinical supervision and training for text-based therapy at scale."}}
{"id": "2602.21652", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21652", "abs": "https://arxiv.org/abs/2602.21652", "authors": ["Minhao Jiang", "Zhikai Li", "Xuewen Liu", "Jing Zhang", "Mengjuan Chen", "Qingyi Gu"], "title": "Sparsity Induction for Accurate Post-Training Pruning of Large Language Models", "comment": "5 pages, 1 figure, 4 tables", "summary": "Large language models have demonstrated capabilities in text generation, while their increasing parameter scales present challenges in computational and memory efficiency. Post-training sparsity (PTS), which reduces model cost by removing weights from dense networks, is an effective approach. However, native dense matrices lack high sparsity, making existing approaches that directly remove weights disrupt model states, resulting in unsatisfactory performance recovery even with post-tuning. We propose Sparsity Induction, which promotes models toward higher sparsity at both distribution and feature levels before pruning, to push the limits of PTS. At the distribution level, we enhance distributional sparsity through mathematically equivalent scaling transformations, which are fully absorbable and incur no extra parameters or inference-time overhead. At the feature level, we introduce Spectral Norm Loss to promote feature sparsity from a low-rank perspective. Experiments across diverse model architectures and tasks demonstrate that our method further enhances sparsity-friendliness, achieving superior pruning performance over existing approaches.", "AI": {"tldr": "The paper proposes a new sparsity induction strategy that makes large language models more pruning-friendly, leading to better post-training sparsity without hurting performance.", "motivation": "Large language models are computationally and memory intensive. Post-training sparsity (pruning weights after training) can reduce cost, but directly pruning dense models causes large performance degradation that is hard to recover, even with post-tuning. Existing dense models are not inherently sparsity-friendly, so there is a need to reshape models before pruning so they can achieve higher sparsity with less performance loss.", "method": "The authors introduce a two-level sparsity induction framework applied before pruning. (1) Distribution-level: they apply mathematically equivalent scaling transformations to the model parameters/activations that increase distributional sparsity but can be fully absorbed into existing parameters, adding no new parameters or inference overhead. (2) Feature-level: they add a Spectral Norm Loss term during training/fine-tuning to encourage low-rank structure and feature sparsity from a spectral perspective. After this induction phase, standard pruning is applied, now on a more sparsity-friendly model.", "result": "Across various model architectures and tasks, models treated with the proposed sparsity induction achieve higher sparsity at comparable or better accuracy compared with baseline post-training sparsity methods. The approach consistently yields superior pruning performance, indicating improved sparsity-friendliness of the transformed models.", "conclusion": "Inducing sparsity before pruning\u2014via distribution-level scaling transformations and feature-level spectral norm regularization\u2014substantially improves the effectiveness of post-training sparsity in large language models. This strategy enables higher sparsity with better performance than existing approaches, without adding inference overhead, and offers a general way to make dense LLMs more amenable to pruning."}}
{"id": "2602.21720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21720", "abs": "https://arxiv.org/abs/2602.21720", "authors": ["Andrea Silvi", "Ponrawee Prasertsom", "Jennifer Culbertson", "Devdatt Dubhashi", "Moa Johansson", "Kenny Smith"], "title": "Evaluating the relationship between regularity and learnability in recursive numeral systems using Reinforcement Learning", "comment": null, "summary": "Human recursive numeral systems (i.e., counting systems such as English base-10 numerals), like many other grammatical systems, are highly regular. Following prior work that relates cross-linguistic tendencies to biases in learning, we ask whether regular systems are common because regularity facilitates learning. Adopting methods from the Reinforcement Learning literature, we confirm that highly regular human(-like) systems are easier to learn than unattested but possible irregular systems. This asymmetry emerges under the natural assumption that recursive numeral systems are designed for generalisation from limited data to represent all integers exactly. We also find that the influence of regularity on learnability is absent for unnatural, highly irregular systems, whose learnability is influenced instead by signal length, suggesting that different pressures may influence learnability differently in different parts of the space of possible numeral systems. Our results contribute to the body of work linking learnability to cross-linguistic prevalence.", "AI": {"tldr": "The paper tests whether the regular structure of human numeral systems makes them easier to learn, using reinforcement learning models to compare the learnability of regular vs irregular numeral systems.", "motivation": "Across languages, numeral systems are strikingly regular, typically recursive systems like base-10, but the reason for this regularity is unclear. One hypothesis is that learning biases favor regular systems, so they become more common cross-linguistically. The authors want to rigorously test whether the structural regularity of numeral systems directly facilitates learning and thus helps explain why these patterns are so prevalent.", "method": "The authors adopt methods from reinforcement learning: they construct artificial agents that must learn numeral systems from limited data and then generalize to represent all integers exactly. They compare the learning performance of agents on human-like, highly regular recursive numeral systems versus alternative, logically possible but irregular systems. They also explore the learnability of extremely unnatural and highly irregular systems, examining how factors such as signal length influence learning in that part of the design space.", "result": "The simulations show that highly regular, human-like recursive numeral systems are indeed easier for RL agents to learn than unattested but logically possible irregular systems. This asymmetry arises when systems are evaluated on how well they support generalization from finite training data to all integers. However, in the region of the design space consisting of very unnatural and highly irregular systems, regularity no longer predicts learnability; instead, learning is more strongly affected by signal length and related factors.", "conclusion": "The study supports the idea that learnability biases favor regular recursive numeral systems, helping to explain why such systems are common across languages. Yet, it also reveals that different regions of the space of possible numeral systems are governed by different learning pressures: for extremely irregular systems, factors like signal length dominate over regularity. Overall, the results strengthen the link between learning biases and cross-linguistic prevalence, specifically in the domain of numeral systems."}}
{"id": "2602.22072", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22072", "abs": "https://arxiv.org/abs/2602.22072", "authors": ["Christian Nickel", "Laura Schrewe", "Florian Mai", "Lucie Flek"], "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models", "comment": null, "summary": "Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks, the corresponding spaces of valid reasoning chains for correct task completion, subsequent reasoning faithfulness, task solutions, and propose metrics to evaluate reasoning chain correctness and to what extent final answers are faithful to reasoning traces of the generated CoT. We show a steep drop in ToM capabilities under task perturbation for all evaluated LLMs, questioning the notion of any robust form of ToM being present. While CoT prompting improves the ToM performance overall in a faithful manner, it surprisingly degrades accuracy for some perturbation classes, indicating that selective application is necessary.", "AI": {"tldr": "The paper investigates whether large language models truly possess Theory of Mind (ToM) by testing their robustness on perturbed false-belief tasks and by analyzing the role of Chain-of-Thought (CoT) prompting.", "motivation": "There is an ongoing debate about whether LLMs genuinely understand others\u2019 mental states (Theory of Mind) or are just pattern-matching. Existing ToM benchmarks often use standard false-belief tasks without testing robustness to small changes, and they rarely evaluate the faithfulness of the models\u2019 intermediate reasoning. The authors aim to create a more stringent, structured way to probe ToM in LLMs and to see if CoT helps both performance and interpretability.", "method": "The authors construct a handcrafted, richly annotated ToM dataset based on classic false-belief tasks and systematically perturbed versions of those tasks. For each task, they define spaces of valid reasoning chains that would lead to correct answers. They then run multiple LLMs on these tasks, with and without Chain-of-Thought prompting, and propose metrics to assess (1) correctness of the generated reasoning chains relative to the valid reasoning spaces, and (2) faithfulness of final answers to the reasoning traces (does the answer actually follow from the CoT).", "result": "All evaluated LLMs show a pronounced drop in performance on ToM tasks when the tasks are perturbed, indicating poor robustness and casting doubt on strong ToM abilities. CoT prompting generally improves ToM task accuracy and produces reasoning traces that are mostly faithful to the final answers. However, for certain classes of perturbations, CoT actually reduces accuracy compared to no-CoT prompting.", "conclusion": "Current LLMs do not exhibit robust Theory of Mind: their performance on false-belief tasks is fragile under systematic perturbations. While Chain-of-Thought prompting can enhance both accuracy and faithfulness of reasoning in many cases, it is not universally beneficial and can even harm performance for some perturbation types. Therefore, ToM evaluations must consider robustness and reasoning faithfulness, and CoT should be applied selectively rather than as a blanket solution."}}
{"id": "2602.22207", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22207", "abs": "https://arxiv.org/abs/2602.22207", "authors": ["Hanna Yukhymenko", "Anton Alexandrov", "Martin Vechev"], "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets", "comment": null, "summary": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.", "AI": {"tldr": "The paper introduces an automated framework to create high-quality multilingual benchmark translations for evaluating LLMs, mitigating semantic drift and context loss in existing translated datasets.", "motivation": "Multilingual evaluation of LLMs is unreliable because many translated benchmarks are low quality, suffering from semantic drift, loss of context, and poor preservation of task structure and linguistic nuance. This yields misleading performance metrics and hampers fair, robust multilingual model assessment.", "method": "The authors build a fully automated translation and localization framework that leverages test-time compute scaling strategies. They adapt Universal Self-Improvement (USI) and propose a new multi-round ranking strategy called T-RANK, where multiple candidate translations are generated and iteratively ranked/refined to preserve semantic content and task structure. The framework is applied to translate existing benchmarks into eight Eastern and Southern European languages.", "result": "The resulting translated benchmarks for eight target languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek) exhibit higher quality than existing translated resources, as measured by reference-based automatic metrics and LLM-as-a-judge evaluations. The improved benchmarks better preserve original semantic content, context, and task structure.", "conclusion": "Automated, compute-scaled translation with USI and T-RANK can reliably produce high-quality localized benchmarks, significantly improving the reliability of multilingual LLM evaluation. By releasing both the framework and the new benchmarks, the work supports more robust, accurate, and reproducible multilingual AI development and assessment."}}
{"id": "2602.21669", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21669", "abs": "https://arxiv.org/abs/2602.21669", "authors": ["Duc Trung Vu", "Pham Khanh Chi", "Dat Phi Van", "Linh Ngo Van", "Sang Dinh", "Trung Le"], "title": "DWA-KD: Dual-Space Weighting and Time-Warped Alignment for Cross-Tokenizer Knowledge Distillation", "comment": "EACL Findings", "summary": "Knowledge Distillation (KD) has emerged as a crucial technique for compressing Large Language Models (LLMs). Although existing cross-tokenizer KD methods have made notable progress, their effectiveness remains constrained by suboptimal alignment across sequence and vocabulary levels. To address these limitations, we introduce Dual-Space Weighting and Time-Warped Alignment (DWA-KD), a novel cross-tokenizer distillation framework that enhances token-wise distillation through dual-space entropy-based weighting and achieves precise sequence-level alignment by leveraging both lexical and semantic information. At the token level, DWA-KD maps teacher representations into the student space and vice versa, performing dual-space KD via Kullback-Leibler divergence (KL). The process is modulated by dual-space weights that up-weight tokens where the student is uncertain and the teacher is confident, thereby focusing learning on informative tokens rather than treating all positions equally. At the sequence level, DWA-KD applies Soft Dynamic Time Warping (Soft-DTW) to both the embedding and final hidden-state layers, enabling robust alignment of lexical and contextual semantics between teacher and student sequences. Extensive experiments across diverse NLP benchmarks demonstrate that DWA-KD outperforms state-of-the-art KD baselines, while ablation studies confirm the complementary contributions of entropy-based token weighting and embedding and final hidden state layer Soft-DTW alignment.", "AI": {"tldr": "This paper proposes DWA-KD, a cross-tokenizer knowledge distillation framework for LLM compression that improves both token-level and sequence-level alignment between teacher and student models using dual-space entropy weighting and Soft-DTW-based alignment.", "motivation": "Existing cross-tokenizer knowledge distillation methods for compressing large language models struggle with suboptimal alignment at both sequence and vocabulary levels, limiting the transfer of knowledge from teacher to student. There is a need for a more precise and informative alignment mechanism that can effectively guide smaller student models, especially when teacher and student use different tokenizers.", "method": "DWA-KD introduces two key ideas. (1) Dual-space token-wise distillation: teacher representations are mapped into the student space and student representations into the teacher space, and KL-divergence-based distillation is performed in both mapped spaces. Entropy-based weights are computed in both spaces to emphasize tokens where the teacher is confident but the student is uncertain, focusing learning on informative positions. (2) Time-warped sequence-level alignment: Soft Dynamic Time Warping (Soft-DTW) is applied at both the embedding layer and the final hidden-state layer to align sequences from teacher and student, capturing both lexical and contextual/semantic correspondence even when tokenization differs or lengths mismatch.", "result": "Across multiple NLP benchmarks, DWA-KD consistently surpasses state-of-the-art knowledge distillation baselines for LLM compression. The framework yields better student performance under comparable model sizes and training settings. Ablation studies show that removing either the dual-space entropy-based token weighting or the Soft-DTW alignment at embedding/final layers degrades performance, indicating each component provides distinct benefits.", "conclusion": "DWA-KD effectively addresses the alignment limitations of prior cross-tokenizer KD approaches by combining dual-space, entropy-aware token-level distillation with Soft-DTW-based sequence-level alignment in both lexical and semantic spaces. This leads to more accurate and efficient transfer of knowledge from teacher to student LLMs and establishes a new state of the art for cross-tokenizer model compression, with both components proven complementary through ablation experiments."}}
{"id": "2602.21728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21728", "abs": "https://arxiv.org/abs/2602.21728", "authors": ["Shiqi Yan", "Yubo Chen", "Ruiqi Zhou", "Zhengxi Yao", "Shuai Chen", "Tianyi Zhang", "Shijie Zhang", "Wei Qiang Zhang", "Yongfeng Huang", "Haixin Duan", "Yunqi Zhang"], "title": "Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling", "comment": "Published as a conference paper at ICLR 2026", "summary": "The reasoning process of Large Language Models (LLMs) is often plagued by hallucinations and missing facts in question-answering tasks. A promising solution is to ground LLMs' answers in verifiable knowledge sources, such as Knowledge Graphs (KGs). Prevailing KG-enhanced methods typically constrained LLM reasoning either by enforcing rules during generation or by imitating paths from a fixed set of demonstrations. However, they naturally confined the reasoning patterns of LLMs within the scope of prior experience or fine-tuning data, limiting their generalizability to out-of-distribution graph reasoning problems. To tackle this problem, in this paper, we propose Explore-on-Graph (EoG), a novel framework that encourages LLMs to autonomously explore a more diverse reasoning space on KGs. To incentivize exploration and discovery of novel reasoning paths, we propose to introduce reinforcement learning during training, whose reward is the correctness of the reasoning paths' final answers. To enhance the efficiency and meaningfulness of the exploration, we propose to incorporate path information as additional reward signals to refine the exploration process and reduce futile efforts. Extensive experiments on five KGQA benchmark datasets demonstrate that, to the best of our knowledge, our method achieves state-of-the-art performance, outperforming not only open-source but also even closed-source LLMs.", "AI": {"tldr": "The paper proposes Explore-on-Graph (EoG), a reinforcement-learning-based framework that lets LLMs actively explore diverse reasoning paths over knowledge graphs for question answering, achieving state-of-the-art results on multiple benchmarks.", "motivation": "LLMs often hallucinate or miss facts in question answering, and existing KG-enhanced approaches overly constrain LLM reasoning by forcing them to follow fixed rules or demonstration paths, which limits generalization to out-of-distribution graph reasoning. There is a need for a method that can leverage KGs while allowing LLMs to discover new, valid reasoning patterns beyond predefined experiences.", "method": "The authors introduce Explore-on-Graph (EoG), which connects LLMs with knowledge graphs and trains them via reinforcement learning to explore reasoning paths. The core idea is to treat reasoning over the KG as an exploration process, where the LLM proposes paths and receives a reward based on the correctness of the final answer. Additionally, they enrich the reward with path-level information to guide exploration toward more meaningful and efficient trajectories, reducing wasted searches over unpromising paths.", "result": "On five knowledge-graph question answering (KGQA) benchmark datasets, EoG achieves state-of-the-art performance. It outperforms both open-source and closed-source LLM-based baselines, indicating that autonomous exploration with path-aware reinforcement learning can substantially improve KG-grounded reasoning quality and robustness.", "conclusion": "Autonomously exploring KG reasoning spaces via reinforcement learning enables LLMs to move beyond rigid rule-based or demonstration-based constraints, improving generalization to challenging KGQA tasks. The EoG framework, with answer- and path-informed rewards, offers an effective way to reduce hallucinations and enhance factual correctness, setting a new performance bar on multiple benchmarks."}}
{"id": "2602.21741", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.21741", "abs": "https://arxiv.org/abs/2602.21741", "authors": ["MD. Sagor Chowdhury", "Adiba Fairooz Chowdhury"], "title": "Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization", "comment": "6 pages, 5 figures, 3 tables; system paper submitted to DL Sprint 4.0 (Kaggle)", "summary": "We describe our end-to-end system for Bengali long-form speech recognition (ASR) and speaker diarization submitted to the DL Sprint 4.0 competition on Kaggle. Bengali presents substantial challenges for both tasks: a large phoneme inventory, significant dialectal variation, frequent code-mixing with English, and a relative scarcity of large-scale labelled corpora. For ASR we achieve a best private Word Error Rate (WER) of 0.37738 and public WER of 0.36137, combining a BengaliAI fine-tuned Whisper medium model with Demucs source separation for vocal isolation, silence-boundary chunking, and carefully tuned generation hyperparameters. For speaker diarization we reach a best private Diarization Error Rate (DER) of 0.27671 and public DER of 0.20936 by replacing the default segmentation model inside the pyannote.audio pipeline with a Bengali-fine-tuned variant, pairing it with wespeaker-voxceleb-resnet34-LM embeddings and centroid-based agglomerative clustering. Our experiments demonstrate that domain-specific fine-tuning of the segmentation component, vocal source separation, and natural silence-aware chunking are the three most impactful design choices for low-resource Bengali speech processing.", "AI": {"tldr": "This paper presents an end-to-end system for Bengali long-form automatic speech recognition and speaker diarization, achieving competitive WER and DER on the DL Sprint 4.0 Kaggle competition through targeted fine-tuning and preprocessing strategies.", "motivation": "Bengali is a challenging, relatively low-resource language for long-form ASR and diarization due to its large phoneme inventory, dialect variation, English code-mixing, and lack of large labeled corpora. The authors aim to build a strong, practical system for a competitive benchmark and to identify which design choices matter most in this setting.", "method": "For ASR, they fine-tune a Whisper medium model on BengaliAI data, apply Demucs for vocal source separation, segment audio using silence-boundary chunking, and carefully tune decoding hyperparameters. For diarization, they modify the pyannote.audio pipeline by replacing the default segmentation model with a Bengali-fine-tuned version, use wespeaker-voxceleb-resnet34-LM embeddings, and perform centroid-based agglomerative clustering for speaker grouping.", "result": "Their ASR system attains a private WER of 0.37738 and public WER of 0.36137; their diarization system achieves a private DER of 0.27671 and public DER of 0.20936 on the DL Sprint 4.0 competition leaderboard.", "conclusion": "Domain-specific fine-tuning of the segmentation model, applying vocal source separation, and using natural silence-aware chunking are identified as the three most impactful contributions for improving low-resource Bengali long-form ASR and diarization performance."}}
{"id": "2602.21763", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21763", "abs": "https://arxiv.org/abs/2602.21763", "authors": ["Heng Wang", "Changxing Wu"], "title": "Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs", "comment": "AAAI26'0ral", "summary": "Implicit Discourse Relation Recognition (IDRR) remains a challenging task due to the requirement for deep semantic understanding in the absence of explicit discourse markers. A further limitation is that existing methods only predict relations without providing any supporting explanations. Recent advances in large language models (LLMs) have shown strong reasoning capabilities in both deep language understanding and natural language explanation generation. In this work, we propose a simple yet effective approach to distill the reasoning capabilities of LLMs into lightweight IDRR models to improve both performance and interpretability. Specifically, we first prompt an LLM to generate explanations for each training instance conditioned on its gold label. Then, we introduce a novel classification-generation framework that jointly performs relation prediction and explanation generation, and train it with the additional supervision of LLM-generated explanations. Our framework is plug-and-play, enabling easy integration with most existing IDRR models. Experimental results on PDTB demonstrate that our approach significantly improves IDRR performance, while human evaluation further confirms that the generated explanations enhance model interpretability. Furthermore, we validate the generality of our approach on sentiment classification and natural language inference", "AI": {"tldr": "They use large language models to generate explanations for discourse relations, then train smaller models to both classify relations and produce explanations, improving accuracy and interpretability.", "motivation": "Implicit Discourse Relation Recognition is difficult because it requires deep semantic understanding without explicit markers, and current models only output labels without explanations. There is a need for models that are both more accurate and more interpretable. Large language models have strong reasoning and explanation abilities, which could be distilled into smaller, task-specific models.", "method": "First, they prompt a large language model to generate natural language explanations for each training instance using the gold discourse relation label. Then they propose a joint classification-generation framework where a lightweight IDRR model is trained to both predict discourse relations and generate explanations, using the LLM-produced explanations as additional supervision. The framework is designed to be plug-and-play, so it can be attached to existing IDRR architectures. They also test the approach on other tasks such as sentiment classification and natural language inference.", "result": "On the PDTB benchmark, their approach yields significant performance gains in implicit discourse relation recognition compared with baseline models that lack explanation-based supervision. Human evaluations indicate that the explanations produced by their models are helpful and improve interpretability. Similar benefits are observed when applying the method to sentiment classification and natural language inference, suggesting the approach is broadly applicable.", "conclusion": "Distilling the reasoning and explanation capabilities of large language models into smaller IDRR models via a joint classification-generation framework leads to better accuracy and more interpretable predictions. The method is simple, compatible with existing models, and generalizes beyond IDRR to other text classification and reasoning tasks."}}
{"id": "2602.21786", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21786", "abs": "https://arxiv.org/abs/2602.21786", "authors": ["Shunsuke Ubukata"], "title": "D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models", "comment": "9 pages, 3 figures. Code: https://github.com/gitpullpull/DisciplinedChainOfThought | Benchmarks: https://huggingface.co/datasets/gitpullpull/D-CoT-Benchmarks | Dataset: https://huggingface.co/datasets/gitpullpull/D-CoT-datasets", "summary": "Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) often induces \"overthinking\" in Small Language Models (SLMs), leading to performance degradation and excessive token consumption. In this study, we propose Disciplined Chain-of-Thought (D-CoT), a novel framework that enforces a structured reasoning process using control tags -- such as <TEMP_LOW> for fact-checking and <TEMP_HIGH> for multi-perspective exploration -- as auxiliary scaffolding during training. By optimizing the CoT trajectory, D-CoT suppresses reasoning drift and simultaneously achieves token reduction and performance improvement. We demonstrate the efficacy of our approach on Qwen3-8B: with only 5,000 training samples, D-CoT significantly boosts accuracy on GPQA-diamond by 9.9% and MMLU-Pro (0-shot) by 9.1%, while drastically reducing computational costs. Furthermore, we confirm that the model internalizes this disciplined thought structure, maintaining high performance even without explicit control tags during inference.", "AI": {"tldr": "The paper introduces Disciplined Chain-of-Thought (D-CoT), a training framework that uses control tags to structure reasoning in small language models, reducing overthinking while improving accuracy and token efficiency when distilling from large models.", "motivation": "Existing Chain-of-Thought distillation from large to small language models often causes small models to \"overthink\"\u2014producing unnecessarily long, drifting reasoning chains that waste tokens and can hurt performance. There is a need for a method that preserves the benefits of CoT (better reasoning) while avoiding reasoning drift and excessive computation, especially in resource-constrained small models.", "method": "The authors propose Disciplined Chain-of-Thought (D-CoT), which augments CoT traces with explicit control tags that indicate different reasoning modes (e.g., <TEMP_LOW> for cautious, fact-checking reasoning and <TEMP_HIGH> for exploratory, multi-perspective reasoning). These tags serve as scaffolding during training, guiding the model through a structured reasoning trajectory. The model is trained on such tagged CoT data distilled from a larger teacher model, optimizing the shape and length of reasoning paths to suppress drift and overlong chains while preserving key reasoning steps.", "result": "Applied to Qwen3-8B with only 5,000 training samples, D-CoT significantly improves benchmark performance: +9.9% accuracy on GPQA-diamond and +9.1% on MMLU-Pro (0-shot), while also substantially reducing token usage and computational costs compared with standard CoT distillation. The resulting model achieves both better accuracy and greater efficiency.", "conclusion": "D-CoT provides an effective way to discipline CoT reasoning in small language models by structuring their thought processes with control tags during training. This approach curbs overthinking and reasoning drift, yields shorter and more focused reasoning traces, and simultaneously improves accuracy and efficiency. Importantly, the learned disciplined reasoning behavior persists even when explicit tags are removed at inference, indicating that the model has internalized a more structured reasoning procedure."}}
{"id": "2602.21854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21854", "abs": "https://arxiv.org/abs/2602.21854", "authors": ["Mustafa Dogan", "Ilker Kesen", "Iacer Calixto", "Aykut Erdem", "Erkut Erdem"], "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning", "comment": "Preprint. 49 pages, 38 Figures, 5 Tables", "summary": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench", "AI": {"tldr": "FewMMBench is a new benchmark to systematically evaluate multimodal large language models (MLLMs) on few-shot learning, especially in-context learning and chain-of-thought prompting.", "motivation": "Although MLLMs are improving at handling mixed image-text inputs, it is unclear how well they actually perform under few-shot conditions and benefit from advanced prompting strategies like ICL and CoT. Existing evaluations do not provide a focused, systematic testbed for these abilities.", "method": "The authors build FewMMBench, a benchmark covering a broad range of multimodal understanding tasks (e.g., attribute recognition, temporal reasoning). They test 26 open-weight MLLMs from six model families under zero-shot, standard few-shot, and CoT-augmented few-shot setups, and also explore retrieval-based demonstrations and varying context lengths.", "result": "Instruction-tuned MLLMs show strong performance in zero-shot settings but gain little or sometimes perform worse when given few-shot demonstrations or CoT prompts. Retrieval-based example selection and longer contexts similarly provide only modest or negligible improvements.", "conclusion": "FewMMBench serves as a rigorous and challenging benchmark for probing and diagnosing the few-shot and in-context learning behavior of MLLMs, indicating that current models do not reliably benefit from few-shot demonstrations or CoT prompting and that there is significant room to improve their multimodal few-shot capabilities."}}
{"id": "2602.21862", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21862", "abs": "https://arxiv.org/abs/2602.21862", "authors": ["Chia Cheng Chang", "An-Zi Yen", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access", "comment": null, "summary": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, making them promising for personalized applications. In this work, we present a framework that leverages LLMs for proactive information access, integrating personal knowledge graphs to enhance the detection of access needs through a refined decision-making process. Our framework offers high flexibility, enabling the replacement of base models and the modification of fact retrieval methods for continuous improvement. Experimental results demonstrate that our approach effectively identifies forgotten events, supporting users in recalling past experiences more efficiently.", "AI": {"tldr": "The paper proposes a flexible framework using large language models and personal knowledge graphs to help users proactively recall forgotten life events without requiring extensive task-specific training data.", "motivation": "People often cannot accurately remember all details of their lives and mix up events. Existing memory recall systems mainly use deep learning methods that need large labeled lifelog datasets, which are hard to obtain, and they adapt slowly as personal data grows. There is a need for a more data-efficient, adaptable, and personalized system that can proactively help users access and recall past experiences.", "method": "The authors introduce a framework that leverages large language models (LLMs) for proactive information access. It integrates personal knowledge graphs representing users\u2019 lifelogs with LLM-based reasoning. A refined decision-making process is designed to detect when a user might need access to certain past events. The framework is modular and flexible, allowing different base LLMs to be plugged in and fact-retrieval mechanisms to be modified or improved over time, enabling continual enhancement as more data becomes available.", "result": "Experiments show that the proposed framework can effectively detect forgotten or relevant past events, outperforming or complementing previous approaches that rely solely on deep learning trained on limited lifelog data. The system improves the accuracy and usefulness of suggested memories when users need to recall past experiences.", "conclusion": "The paper concludes that combining LLMs with personal knowledge graphs provides an effective and flexible approach to proactive memory support. The proposed framework can better identify forgotten events, adapts over time as lifelogs grow, and can be continuously improved by swapping models and retrieval strategies. This demonstrates the promise of LLM-based, knowledge-graph-enhanced systems for personalized memory assistance."}}
{"id": "2602.21887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21887", "abs": "https://arxiv.org/abs/2602.21887", "authors": ["Changjiang Gao", "Zixian Huang", "Kaichen Yang", "Jiajun Chen", "Jixing Li", "Shujian Huang"], "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection", "comment": null, "summary": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs.", "AI": {"tldr": "Introduces ExpLang, an RL-based post-training pipeline for large reasoning models that dynamically selects the language of chain-of-thought during training, using multiple languages instead of only English, and achieves better performance and high language-trace compliance.", "motivation": "Most RL-based post-training for large reasoning models optimizes chain-of-thought reasoning only in English, even though multilingual reasoning can provide complementary strengths and users want native-language thinking traces. There is a gap in how to systematically leverage multiple languages as part of the RL process to enhance exploration and exploitation while controlling which language the model uses for its reasoning traces.", "method": "Propose ExpLang, a post-training pipeline where the reasoning language itself becomes an on-policy action in reinforcement learning. During RL, the model can choose which language to use for its chain-of-thought among multiple options, and is trained with this additional decision space. This extends the exploration space across languages while staying compatible (orthogonal) with existing RL algorithms. The system also encourages and measures compliance with the selected thinking language for both languages seen in training and unseen ones.", "result": "With the same training budget as English-only RL post-training, ExpLang consistently outperforms the English-only baseline on challenging reasoning tasks. It also yields high compliance of thinking traces with the chosen reasoning language, for both languages used during RL and languages that were not explicitly trained as thinking languages.", "conclusion": "Treating the choice of reasoning language as an action in RL allows large reasoning models to better exploit the advantages of non-English languages while enlarging the exploration space, leading to improved reasoning performance. The ExpLang framework is broadly compatible with existing RL methods and illustrates a new, effective way to use multilinguality to enhance large reasoning models."}}
{"id": "2602.21933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21933", "abs": "https://arxiv.org/abs/2602.21933", "authors": ["Bitan Majumder", "Anirban Sen"], "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text", "comment": null, "summary": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest overall accuracy of 84%, outperforming all of the LLMs in zero and few-shot set ups, using minimal LLM generated code-mixed data used for fine-tuning. These findings indicate that domain-adaptive fine-tuning of smaller transformer based models may significantly improve sarcasm detection over general LLM inference, in low-resource and data scarce settings.", "AI": {"tldr": "The paper evaluates large language models versus a fine-tuned DistilBERT for sarcasm detection in code-mixed Hinglish and finds the smaller fine-tuned model performs best.", "motivation": "Sarcasm detection in multilingual, code-mixed settings like Hinglish is difficult due to structural variation, informality, and scarcity of labeled data. With the rise of large language models, it is unclear whether general-purpose LLMs or smaller, domain-fine-tuned transformers are better suited for this low-resource, specialized task.", "method": "The authors compare four LLMs (Llama 3.1, Mistral, Gemma 3, Phi-4) used in zero- and few-shot configurations against a DistilBERT model that is sequentially fine-tuned on a small amount of LLM-generated code-mixed Hinglish sarcasm data. Performance is measured on a sarcasm detection benchmark in Hinglish text, focusing on overall accuracy.", "result": "The fine-tuned DistilBERT achieves the highest overall accuracy of 84%, outperforming all evaluated LLMs in both zero-shot and few-shot setups despite being smaller and trained on only a small quantity of generated code-mixed data.", "conclusion": "For sarcasm detection in low-resource, code-mixed settings, domain-adaptive fine-tuning of smaller transformer models can surpass general-purpose LLM inference. This suggests that, under data scarcity and domain specificity, carefully fine-tuned compact models may be more effective and efficient than relying solely on large, general LLMs."}}
{"id": "2602.21941", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21941", "abs": "https://arxiv.org/abs/2602.21941", "authors": ["Zhenyu Wang", "Xiaofen Xing", "Yirong Chen", "Xiangmin Xu"], "title": "MERRY: Semantically Decoupled Evaluation of Multimodal Emotional and Role Consistencies of Role-Playing Agents", "comment": "11 pages, 6 figures", "summary": "Multimodal Role-Playing Agents (MRPAs) are attracting increasing attention due to their ability to deliver more immersive multimodal emotional interactions. However, existing studies still rely on pure textual benchmarks to evaluate the text responses of MRPAs, while delegating the assessment of their multimodal expressions solely to modality-synthesis metrics. This evaluation paradigm, on the one hand, entangles semantic assessment with modality generation, leading to ambiguous error attribution, and on the other hand remains constrained by the heavy reliance on human judgment. To this end, we propose MERRY, a semantically decoupled evaluation framework for assessing Multimodal Emotional and Role consistencies of Role-playing agents. This framework introduce five refined metrics for EC and three for RC. Notably, we transform the traditional subjective scoring approach into a novel bidirectional-evidence-finding task, significantly improving the human agreement of LLM-as-Judge evaluations. Based on MERRY, we conduct extensive evaluations. Our empirical results primarily reveal that: (1) Training on synthetic datasets tends to reduce emotional consistency, whereas training on real-world datasets improves it; (2) Existing models suffer from emotional templatization and simplification, exhibiting positive-bias and performance bottleneck in fine-grained negative emotions; (3) Simple prompting method strengthens the weak models but constrains the strong ones, while simple fine-tuning method suffers from poor role generalization. Codes and dataset are available.", "AI": {"tldr": "MERRY is a new evaluation framework for multimodal role-playing agents that decouples semantic assessment from modality generation and provides more reliable automatic evaluation of emotional and role consistency.", "motivation": "Existing multimodal role-playing agents are evaluated mainly with text-only benchmarks for language quality and separate generation metrics for modalities. This mixes up semantic quality with modality synthesis quality and still relies heavily on subjective human scoring, making error attribution unclear and evaluations costly and noisy. The authors want a principled, scalable way to measure how well agents maintain emotional and role consistency across modalities.", "method": "They design MERRY, an evaluation framework targeting Multimodal Emotional Consistency (EC) and Role Consistency (RC). MERRY defines five detailed metrics for EC and three for RC, and it reformulates evaluation from subjective scalar scoring into a bidirectional evidence-finding task: the judge (an LLM) must find evidence in the agent\u2019s outputs that supports or contradicts the target emotions/roles, and vice versa. This structure is intended to increase reliability and inter-annotator (human\u2013LLM) agreement. Using this framework, they systematically evaluate different models, training data types, prompting strategies, and fine-tuning methods.", "result": "Empirical evaluations with MERRY show several patterns: (1) Models trained on synthetic data exhibit decreased emotional consistency, while those trained on real-world data show improved emotional consistency. (2) Current models show \u201cemotional templating\u201d and simplification, with a systematic positive bias and clear performance limits on nuanced negative emotions. (3) Simple prompting boosts weaker models but can hinder stronger ones, and simple fine-tuning strategies lead to poor generalization across different roles.", "conclusion": "MERRY provides a more semantically grounded, decoupled, and reliable evaluation scheme for multimodal role-playing agents, especially with respect to emotional and role consistency. The study highlights key weaknesses in current models\u2014over-reliance on synthetic data, biased and templated emotional responses, and limited generalization from na\u00efve prompting or fine-tuning\u2014suggesting that better data and training strategies are needed to improve fine-grained emotional and role-playing abilities."}}
{"id": "2602.21947", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21947", "abs": "https://arxiv.org/abs/2602.21947", "authors": ["Sohan Venkatesh", "Ashish Mahendran Kurapath", "Tejas Melkote"], "title": "Large Language Models are Algorithmically Blind", "comment": "20 pages, 11 figures, 14 tables", "summary": "Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider than true confidence intervals yet still fail to contain the true algorithmic mean in the majority of instances; most perform worse than random guessing and the marginal above-random performance of the best model is most consistent with benchmark memorization rather than principled reasoning. We term this failure algorithmic blindness and argue it reflects a fundamental gap between declarative knowledge about algorithms and calibrated procedural prediction.", "AI": {"tldr": "The paper evaluates large language models\u2019 ability to reason about algorithm performance using causal discovery as a testbed and finds systematic, near-total failure, calling this phenomenon 'algorithmic blindness.'", "motivation": "Although LLMs know a lot about algorithms in a declarative sense, practitioners lack evidence on whether LLMs can *procedurally* reason about algorithm behavior (e.g., performance, confidence intervals) to guide algorithm selection and deployment. This gap is practically important for data science and ML workflows that increasingly rely on LLM advice.", "method": "Use causal discovery as a controlled testbed. Run large-scale executions of causal discovery algorithms to obtain ground-truth statistics (means, confidence intervals of performance). Prompt eight frontier LLMs to predict these quantities and compare their predictions to the empirical ground truth and to random guessing. Analyze whether any above-random performance is due to genuine reasoning or benchmark memorization.", "result": "LLMs produce estimated ranges that are much wider than the true confidence intervals, yet still miss the true algorithmic mean in most cases. Most models perform worse than random guessing on key prediction tasks, and the small above-random performance of the best model appears to be driven by memorization of benchmark patterns instead of calibrated reasoning about algorithms.", "conclusion": "The authors identify a phenomenon they call 'algorithmic blindness': LLMs can state facts about algorithms but fail to make calibrated, quantitative predictions about algorithmic behavior. This gap between declarative algorithmic knowledge and procedural, predictive reasoning suggests current LLMs are unreliable tools for tasks like algorithm selection and performance forecasting, and that closing this gap is an open research challenge."}}
{"id": "2602.21950", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21950", "abs": "https://arxiv.org/abs/2602.21950", "authors": ["Boqi Chen", "Xudong Liu", "Jiachuan Peng", "Marianne Frey-Marti", "Bang Zheng", "Kyle Lam", "Lin Li", "Jianing Qiu"], "title": "MEDSYN: Benchmarking Multi-EviDence SYNthesis in Complex Clinical Cases for Multimodal Large Language Models", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown great potential in medical applications, yet existing benchmarks inadequately capture real-world clinical complexity. We introduce MEDSYN, a multilingual, multimodal benchmark of highly complex clinical cases with up to 7 distinct visual clinical evidence (CE) types per case. Mirroring clinical workflow, we evaluate 18 MLLMs on differential diagnosis (DDx) generation and final diagnosis (FDx) selection. While top models often match or even outperform human experts on DDx generation, all MLLMs exhibit a much larger DDx--FDx performance gap compared to expert clinicians, indicating a failure mode in synthesis of heterogeneous CE types. Ablations attribute this failure to (i) overreliance on less discriminative textual CE ($\\it{e.g.}$, medical history) and (ii) a cross-modal CE utilization gap. We introduce Evidence Sensitivity to quantify the latter and show that a smaller gap correlates with higher diagnostic accuracy. Finally, we demonstrate how it can be used to guide interventions to improve model performance. We will open-source our benchmark and code.", "AI": {"tldr": "MEDSYN is a new multilingual, multimodal benchmark of complex clinical cases used to evaluate multimodal large language models (MLLMs) on diagnosis tasks, revealing a systematic failure to synthesize diverse evidence and proposing a metric, Evidence Sensitivity, to quantify and mitigate this issue.", "motivation": "Existing benchmarks for medical MLLMs do not adequately reflect the complexity of real-world clinical practice, where clinicians must integrate multiple heterogeneous pieces of evidence (e.g., images, history, labs) across modalities and languages. There is a need for a benchmark that more faithfully mirrors clinical workflows and allows detailed analysis of how MLLMs use different evidence types when making diagnoses.", "method": "The authors construct MEDSYN, a benchmark consisting of complex clinical cases that include up to seven distinct types of visual clinical evidence, in a multilingual setting. They design tasks that mirror clinical workflows: generating a differential diagnosis (DDx) list and selecting a final diagnosis (FDx). They evaluate 18 existing MLLMs on these tasks and perform ablation studies that selectively remove or vary evidence types to identify where models fail. They define a new metric, Evidence Sensitivity, to measure how much models\u2019 predictions change in response to different cross-modal pieces of evidence and analyze its relationship with diagnostic performance. They then explore interventions guided by this metric to improve model behavior.", "result": "On MEDSYN, top-performing MLLMs can match or exceed human experts in generating differential diagnoses but show much worse performance than clinicians when selecting the final diagnosis, creating a larger DDx\u2013FDx gap. Analysis shows models over-rely on less discriminative textual information such as medical history and underutilize more discriminative visual clinical evidence, revealing a cross-modal utilization gap. The newly proposed Evidence Sensitivity metric captures this gap and is empirically correlated with diagnostic accuracy. Using insights from this metric, the authors design interventions that yield measurable improvements in MLLM diagnostic performance.", "conclusion": "MEDSYN exposes a key failure mode of current medical MLLMs: difficulty in synthesizing heterogeneous multimodal clinical evidence to converge on an accurate final diagnosis, despite strong performance in broad differential generation. The benchmark and the Evidence Sensitivity metric provide tools to systematically measure and analyze evidence use across modalities, and to guide targeted interventions that improve diagnostic accuracy. The authors plan to open-source MEDSYN and its codebase to support future research on more clinically realistic and reliable medical MLLMs."}}
{"id": "2602.21951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21951", "abs": "https://arxiv.org/abs/2602.21951", "authors": ["Bo Xue", "Yuan Jin", "Luoyi Fu", "Jiaxin Ding", "Xinbing Wang"], "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning", "comment": null, "summary": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypassing generation-induced hallucinations. Across four benchmarks, RADAR achieves 5-6% relative gains on link prediction and triple classification over strong LLM baselines, while increasing task-relevant mutual information in intermediate representations by 62.9%, indicating more robust and transferable relational reasoning.", "AI": {"tldr": "RADAR reframes knowledge graph reasoning with LLMs as a discriminative entity selection problem using reinforcement learning, improving generalization and robustness over generative approaches.", "motivation": "Existing LLM-based knowledge graph reasoning methods often rely on generative modeling that captures superficial co-occurrence patterns instead of true relational semantics, which harms out-of-distribution generalization and leads to issues like hallucinations. A more principled framework is needed that encourages genuine relational reasoning and robust, transferable representations.", "method": "RADAR converts knowledge graph reasoning from a generative token-prediction task into a discriminative entity selection task. It uses reinforcement learning to enforce relative separability between candidate entities, going beyond simple likelihood imitation of training data. Inference is then conducted directly in the learned representation space, consistent with this discriminative training and avoiding generation steps that can introduce hallucinations.", "result": "On four standard benchmarks, RADAR outperforms strong LLM-based baselines, yielding 5\u20136% relative improvements on link prediction and triple classification tasks. It also increases task-relevant mutual information in intermediate representations by 62.9%, suggesting that the learned embeddings capture more meaningful relational information.", "conclusion": "Recasting KGR as discriminative relational reasoning with reinforcement learning-based entity separability leads to more robust, generalizable, and semantically grounded reasoning than generative LLM approaches, reducing hallucinations and enhancing the transferability of relational representations."}}
{"id": "2602.21978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21978", "abs": "https://arxiv.org/abs/2602.21978", "authors": ["Miyu Oba", "Saku Sugawara"], "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models", "comment": null, "summary": "Recent work has examined language models from a linguistic perspective to better understand how they acquire language. Most existing benchmarks focus on judging grammatical acceptability, whereas the ability to interpret meanings conveyed by grammatical forms has received much less attention. We introduce the Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models (CxMP), a benchmark grounded in Construction Grammar that treats form-meaning pairings, or constructions, as fundamental linguistic units. CxMP evaluates whether models can interpret the semantic relations implied by constructions, using a controlled minimal-pair design across nine construction types, including the let-alone, caused motion, and ditransitive constructions. Our results show that while syntactic competence emerges early, constructional understanding develops more gradually and remains limited even in large language models (LLMs). CxMP thus reveals persistent gaps in how language models integrate form and meaning, providing a framework for studying constructional understanding and learning trajectories in language models.", "AI": {"tldr": "The paper introduces CxMP, a Construction Grammar-based minimal-pair benchmark showing that LLMs have relatively good syntax but weak constructional (form-meaning) understanding.", "motivation": "Existing linguistic evaluations of language models focus mostly on grammatical acceptability, not on whether models understand the meanings encoded by specific grammatical constructions. From a Construction Grammar perspective, form-meaning pairings (constructions) are fundamental, so we need a benchmark that tests whether models grasp the semantic relations these constructions imply. This helps uncover how language models actually integrate form and meaning and how their linguistic abilities develop.", "method": "The authors create CxMP, a Linguistic Minimal-Pair Benchmark grounded in Construction Grammar. It uses tightly controlled minimal pairs of sentences that differ in grammatical construction but not in surface content, across nine construction types such as let-alone, caused motion, and ditransitive constructions. Models are evaluated on whether they correctly interpret the semantic relations implied by each construction, which isolates constructional understanding from mere surface-level cues. They then test models of varying sizes to analyze learning trajectories.", "result": "The evaluation shows that language models acquire syntactic competence relatively early\u2014i.e., they handle grammatical form\u2014but their ability to correctly interpret the meanings associated with constructions develops more slowly and remains limited even for large LLMs. Performance gaps are consistent across the nine construction types tested, indicating that constructional understanding is an area of weakness in current models.", "conclusion": "CxMP exposes a persistent disconnect between syntactic proficiency and genuine constructional (form-meaning) understanding in language models. The benchmark offers a systematic way to study how and when models learn the semantics of grammatical constructions, and it suggests that improving LLMs will require more than scaling: we need methods that better integrate form and meaning at the level of constructions."}}
{"id": "2602.22014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22014", "abs": "https://arxiv.org/abs/2602.22014", "authors": ["Louis Est\u00e8ve", "Christophe Servan", "Thomas Lavergne", "Agata Savary"], "title": "A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT", "comment": null, "summary": "Diversity has been gaining interest in the NLP community in recent years. At the same time, state-of-the-art transformer models such as ModernBERT use very large pre-training datasets, which are driven by size rather than by diversity. This summons for an investigation of the impact of diversity on the ModernBERT pre-training. We do so in this study, with the express intent of reducing pre-training dataset size, while retaining at least comparable performance. We compare diversity-driven sampling algorithms, so as to pick the best one. We find that diversity-driven sampling allows in some tasks to gain 10 points relative to randomly-sampled pre-training data of commensurate size. We also see that a model pre-trained for 483h on a diversity-driven dataset of 150M tokens can yield a commensurate performance to a model pre-trained for 1,775h on a randomly-driven dataset of 2.4B tokens.", "AI": {"tldr": "The paper studies how diversity-driven sampling of pre-training data for ModernBERT-style transformers can reduce dataset size and compute while maintaining or improving performance.", "motivation": "While diversity is increasingly valued in NLP, modern transformer models rely on massive pre-training corpora chosen mostly by quantity rather than diversity. This raises questions about whether more diverse but smaller datasets could yield similar or better performance with less computation, prompting a systematic study of diversity\u2019s impact on ModernBERT pre-training.", "method": "The authors experiment with ModernBERT pre-training using different sampling strategies for constructing the pre-training corpus. They implement and compare multiple diversity-driven sampling algorithms against standard random sampling, controlling for dataset size and training time. They then evaluate the resulting models on downstream tasks, measuring performance differences attributable to the sampling strategy and data scale.", "result": "Diversity-based sampling substantially improves efficiency. On some tasks, models pre-trained on diversity-sampled data outperform those trained on randomly sampled data of the same size by up to 10 performance points. Moreover, a model pre-trained for 483 hours on a 150M-token, diversity-driven dataset achieves performance comparable to a model trained for 1,775 hours on a 2.4B-token, randomly sampled dataset, indicating large savings in data and compute.", "conclusion": "Carefully selecting pre-training data based on diversity, rather than scaling data size alone, can maintain or even improve ModernBERT performance while dramatically reducing dataset size and training compute. Diversity-aware sampling is therefore a promising strategy for more efficient and sustainable pre-training of large language models."}}
{"id": "2602.22045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22045", "abs": "https://arxiv.org/abs/2602.22045", "authors": ["Walter Hernandez Cruz", "Peter Devine", "Nikhil Vadgama", "Paolo Tasca", "Jiahua Xu"], "title": "DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain", "comment": null, "summary": "We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution.\n  We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in a virtuous cycle where research precedes and enables economic growth that funds further innovation.\n  We publicly release the full DLT-Corpus; LedgerBERT, a domain-adapted model achieving 23% improvement over BERT-base on a DLT-specific Named Entity Recognition (NER) task; and all associated tools and code.", "AI": {"tldr": "They build and release the largest text corpus for Distributed Ledger Technology (DLT), show how it can be used to study technology and market dynamics, and provide a domain-adapted language model that significantly outperforms generic BERT on DLT tasks.", "motivation": "Existing NLP work in the DLT/blockchain space is narrow, mostly focusing on price prediction and smart contracts. There is no broad, domain-specific textual resource to study the language, innovation dynamics, and socio-technical aspects of DLT, despite its large economic scale and rapid change. The authors aim to fill this gap with a comprehensive corpus and tools to enable more systematic, data-driven DLT research.", "method": "They construct DLT-Corpus by aggregating and cleaning 2.98B tokens from 22.12M DLT-related documents drawn from three main sources: scientific publications, USPTO patents, and social media posts. They then use this corpus to analyze technology emergence patterns (tracking when and where concepts appear across sources) and to study correlations between innovation activity and market dynamics. They also pretrain or domain-adapt a BERT-based language model, LedgerBERT, on this corpus and evaluate it on a DLT-specific NER benchmark, comparing it to BERT-base.", "result": "Empirically, they find that new DLT technologies tend to appear first in scientific literature, then in patents, and finally in social media, mirroring classical technology transfer trajectories. Social media sentiment remains predominantly positive even during market downturns, whereas publication and patenting activity grow in a way that is largely decoupled from short-term price cycles and instead follows overall market expansion. LedgerBERT achieves a 23% performance gain over BERT-base on a DLT NER task, demonstrating the value of domain adaptation. They also successfully compile and release the large-scale DLT-Corpus and associated tooling.", "conclusion": "DLT-Corpus is a valuable resource that enables rigorous NLP-driven analysis of the DLT ecosystem, revealing that innovation originates in research and patents ahead of social discourse and that scientific and patent activity are more structurally linked to long-term market growth than to short-term sentiment. The substantial NER gains of LedgerBERT show that domain-specific pretraining substantially improves performance on DLT tasks. By releasing the corpus, model, and tools, the authors provide foundational infrastructure for future DLT-focused NLP and innovation studies."}}
{"id": "2602.22090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22090", "abs": "https://arxiv.org/abs/2602.22090", "authors": ["Bo-Wei Chen", "Chung-Chi Chen", "An-Zi Yen"], "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference", "comment": "Accepted by EACL 2026 Findings", "summary": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20\\% to 40\\%. When applied to GPT-4o API calls, it reduces token usage by approximately 60\\%, further improving cost efficiency. These findings indicate the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications.", "AI": {"tldr": "The paper proposes a confidence-based mechanism to route tasks between small and large LLMs, achieving similar accuracy to the largest model with significantly lower computational and API costs.", "motivation": "Larger LLMs typically yield better performance but incur much higher compute and monetary costs, limiting their use in real-world and resource-constrained scenarios. There is a need for intelligent mechanisms that selectively use large models only when necessary, while relying on cheaper models when they are likely to be sufficiently accurate.", "method": "The authors introduce a confidence-driven model selection strategy. For each task, they estimate (1) the likelihood that a given model knows the correct answer and (2) the probability that its generated response is accurate. If the smaller model is sufficiently confident, its answer is used; otherwise, the task is escalated to a larger model. This routing mechanism is evaluated on MMLU and on real GPT-4o API usage in terms of accuracy and compute/token savings.", "result": "On the MMLU benchmark, the confidence-based routing achieves accuracy close to that of the largest model while reducing computational cost by 20\u201340%. In experiments with GPT-4o API calls, the approach reduces token usage by about 60%, indicating substantial cost savings with minimal performance loss.", "conclusion": "Confidence-based model selection can effectively balance performance and cost in LLM deployment. By delegating only uncertain or complex tasks to larger models, the system maintains near-maximum accuracy while significantly reducing computation and API expenses, making it especially suitable for edge devices and commercial applications with limited resources."}}
{"id": "2602.22125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22125", "abs": "https://arxiv.org/abs/2602.22125", "authors": ["Thanmay Jayakumar", "Mohammed Safi Ur Rahman Khan", "Raj Dabre", "Ratish Puduppully", "Anoop Kunchukuttan"], "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages", "comment": "8 pages + Appendix", "summary": "Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) carefully localized for Indic contexts, and IndicIFEval-Ground, synthetically generated instructions grounded in native Indic content. We conduct a comprehensive evaluation of major open-weight and proprietary models spanning both reasoning and non-reasoning models. While models maintain strong adherence to formatting constraints, they struggle significantly with lexical and cross-lingual tasks -- and despite progress in high-resource languages, instruction-following across the broader Indic family lags significantly behind English. We release IndicIFEval and its evaluation scripts to support progress on multilingual constrained generation (http://github.com/ai4bharat/IndicIFEval).", "AI": {"tldr": "Introduces IndicIFEval, a multilingual instruction-following benchmark for 14 Indic languages focused on constrained generation.", "motivation": "Existing instruction-following benchmarks are heavily English-centric, leaving Indic languages under-evaluated despite their large speaker base.", "method": "Create IndicIFEval with ~800 human-verified, automatically verifiable, rule-based instruction-following examples per language, split into two subsets: (1) IndicIFEval-Ground, localized translations of IFEval prompts; (2) IndicIFEval-Ground, synthetically generated instructions grounded in native Indic content. Then evaluate a range of open-weight and proprietary LLMs, including reasoning and non-reasoning models, on formatting, lexical, and cross-lingual tasks.", "result": "Models perform well on formatting constraints but perform poorly on lexical and cross-lingual tasks in Indic languages; instruction-following quality in Indic languages is notably weaker than in English, particularly for lower-resource Indic languages.", "conclusion": "IndicIFEval exposes a substantial gap between English and Indic instruction-following abilities in current LLMs and provides an open benchmark and scripts to drive research on multilingual constrained generation for Indic languages."}}
{"id": "2602.22157", "categories": ["cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22157", "abs": "https://arxiv.org/abs/2602.22157", "authors": ["Leon Pielage", "Ole H\u00e4tscher", "Mitja Back", "Bernhard Marschall", "Benjamin Risse"], "title": "Dynamic Personality Adaptation in Large Language Models via State Machines", "comment": "22 pages, 5 figures, submitted to ICPR 2026", "summary": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.", "AI": {"tldr": "The paper introduces a model-agnostic framework that dynamically modulates an LLM\u2019s personality in dialogue using state machines and continuous personality scoring, demonstrated in a medical education setting.", "motivation": "Current LLMs struggle to adjust their expressed personality as conversations evolve, which limits their effectiveness in complex, interactive tasks such as training, support, and de-escalation scenarios. There is a need for a principled, flexible way to simulate and adapt personality over time, independent of any specific LLM or personality theory.", "method": "The authors design a dynamic personality simulation framework based on state machines whose latent personality states transition according to probabilities that depend on the conversational context. They build a modular pipeline that continuously scores dialogue on latent personality axes in a model-agnostic way. These scores are treated as dynamic state variables that automatically modify the system prompt to realign the LLM\u2019s behavior. They instantiate this framework using the Interpersonal Circumplex (IPC) model in a medical education scenario and compare implementations using large LLM-based scorers versus lightweight fine-tuned classifiers.", "result": "The framework can successfully adapt the simulated personality state in response to user input and, importantly, also affects user behavior in ways that support de-escalation training. The continuous scoring pipeline achieves similar precision when implemented with lightweight fine-tuned classifiers as with larger LLM-based scoring models, indicating that the approach does not depend on heavyweight components.", "conclusion": "Dynamic, personality-adaptive architectures for LLMs are feasible and effective. By separating personality modeling, scoring, and prompt control into modular components and using state machines for personality dynamics, the framework can be applied across domains and models. The demonstrated use case in medical education suggests broader applicability to customer support and general human-computer interaction, with efficient implementations possible using smaller classifiers."}}
{"id": "2602.22175", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22175", "abs": "https://arxiv.org/abs/2602.22175", "authors": ["Xi Ye", "Wuwei Zhang", "Fangcong Yin", "Howard Yen", "Danqi Chen"], "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs", "comment": null, "summary": "Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads specialized for long-context retrieval--to identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LMs. Across multiple instruction-tuned and reasoning models, DySCO consistently improves performance on challenging long-context reasoning benchmarks, yielding relative gains of up to 25% on MRCR and LongBenchV2 at 128K context length with modest additional compute. Further analysis highlights the importance of both dynamic attention rescaling and retrieval-head-guided selection for the effectiveness of the method, while providing interpretability insights into decoding-time attention behavior. Our code is available at https://github.com/princeton-pli/DySCO.", "AI": {"tldr": "Proposes DySCO, a training-free decoding algorithm that dynamically re-weights attention to relevant long-context tokens using specialized retrieval heads, significantly improving long-context reasoning performance for existing LMs.", "motivation": "Long-context language models can accept very long inputs, but their reasoning accuracy often drops as context length increases because the model fails to focus attention on the most relevant parts of the input during decoding. There is a need for a method that can better exploit long contexts at inference time, without retraining or modifying the model, to improve performance on tasks that require reasoning over large documents or many prior turns.", "method": "Introduce DySCO, a decoding-time algorithm that: (1) identifies \"retrieval heads\"\u2014attention heads that specialize in long-context retrieval; (2) at each generation step, uses these heads to locate task-relevant tokens in the long context; and (3) explicitly up-weights these tokens in the attention distribution, effectively rescaling attention dynamically during decoding. The procedure is training-free and wraps around existing off-the-shelf instruction-tuned or reasoning LMs, adding only modest computational overhead.", "result": "Applied to several instruction-tuned and reasoning models on long-context benchmarks such as MRCR and LongBenchV2 at 128K token context length, DySCO yields consistent accuracy improvements, with relative gains up to 25% compared to standard decoding. Empirical ablations show that both components\u2014dynamic attention rescaling and retrieval-head-guided token selection\u2014are crucial for the observed performance gains.", "conclusion": "DySCO demonstrates that decoding-time manipulation of attention, guided by specialized retrieval heads, can substantially improve long-context reasoning in existing LMs without additional training. The method not only boosts performance with modest compute overhead but also offers interpretability into how attention behaves during generation, suggesting a promising direction for inference-time control of long-context language models."}}
{"id": "2602.22182", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.22182", "abs": "https://arxiv.org/abs/2602.22182", "authors": ["Sourav Saha", "Dwaipayan Roy", "Mandar Mitra"], "title": "LiCQA : A Lightweight Complex Question Answering System", "comment": null, "summary": "Over the last twenty years, significant progress has been made in designing and implementing Question Answering (QA) systems. However, addressing complex questions, the answers to which are spread across multiple documents, remains a challenging problem. Recent QA systems that are designed to handle complex questions work either on the basis of knowledge graphs, or utilise contem- porary neural models that are expensive to train, in terms of both computational resources and the volume of training data required. In this paper, we present LiCQA, an unsupervised question answer- ing model that works primarily on the basis of corpus evidence. We empirically compare the effectiveness and efficiency of LiCQA with two recently presented QA systems, which are based on different underlying principles. The results of our experiments show that LiCQA significantly outperforms these two state-of-the-art systems on benchmark data with noteworthy reduction in latency.", "AI": {"tldr": "The paper introduces LiCQA, an unsupervised, corpus-based QA system that effectively answers complex, multi-document questions with lower latency, outperforming recent state-of-the-art systems.", "motivation": "Although QA systems have advanced over the past two decades, complex questions whose answers are distributed across multiple documents remain difficult. Existing strong systems often rely on knowledge graphs or large neural models that are costly to train and require extensive data. There is a need for a more efficient, less resource-intensive approach that still performs well on complex question answering.", "method": "The authors propose LiCQA, an unsupervised QA model that primarily exploits evidence found directly in a text corpus rather than relying on knowledge graphs or supervised neural training. They then empirically evaluate LiCQA by comparing its performance and efficiency against two recent state-of-the-art QA systems that use different underlying principles.", "result": "Experimental results on benchmark datasets show that LiCQA significantly outperforms the two compared state-of-the-art QA systems in terms of effectiveness (answer quality) while also achieving a notable reduction in latency (faster response times).", "conclusion": "LiCQA demonstrates that an unsupervised, corpus-based approach can handle complex multi-document QA effectively and efficiently, rivaling and surpassing more resource-intensive, state-of-the-art systems. This suggests that high-quality complex QA does not necessarily require heavy reliance on knowledge graphs or large supervised neural models."}}
{"id": "2602.22193", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22193", "abs": "https://arxiv.org/abs/2602.22193", "authors": ["Melody Ma", "John Hewitt"], "title": "Improving Parametric Knowledge Access in Reasoning Language Models", "comment": null, "summary": "We study reasoning for accessing world knowledge stored in a language model's parameters. For example, recalling that Canberra is Australia's capital may benefit from thinking through major cities and the concept of purpose-built capitals. While reasoning language models are trained via reinforcement learning to produce reasoning traces on tasks such as mathematics, they may not reason well for accessing their own world knowledge. We first find that models do not generate their best world knowledge reasoning by default: adding a simple \"think step-by-step\" cue demonstrates statistically significant improvement in knowledge recall but not math. Motivated by this, we propose training models to reason over their parametric knowledge using world-knowledge question answering as a verifiable reward. After reinforcement learning on TriviaQA (+9.9%), performance also improves on Natural Questions, HotpotQA, SimpleQA, and StrategyQA by 4.2%, 2.1%, 0.6%, and 3.0%, respectively. Reasoning models are under-optimized for parametric knowledge access, but can be easily trained to reason better.", "AI": {"tldr": "The paper investigates how to improve language models\u2019 ability to reason when retrieving world knowledge stored in their parameters, showing that explicit step-by-step reasoning prompts and RL training on QA tasks significantly enhance factual recall.", "motivation": "Reasoning-optimized language models are typically trained and evaluated on tasks like math or logical puzzles, but not specifically on accessing their own stored world knowledge. As a result, they may fail to retrieve facts effectively even though the information is present in their parameters. The authors are motivated to understand whether prompting and training for explicit reasoning can improve factual recall from parametric knowledge and to quantify this effect across multiple QA benchmarks.", "method": "1) Empirically test whether adding a simple chain-of-thought style cue (e.g., \u201cthink step-by-step\u201d) improves factual recall and compare its impact on world-knowledge QA vs. math tasks. 2) Design a reinforcement learning (RL) setup where the model is trained to produce reasoning traces for world-knowledge question answering, using correctness on a dataset such as TriviaQA as a verifiable reward signal. 3) Evaluate the RL-trained model\u2019s performance on TriviaQA and transfer to other QA benchmarks (Natural Questions, HotpotQA, SimpleQA, StrategyQA) to measure generalization of improved parametric-knowledge reasoning.", "result": "The authors find that models do not naturally generate their best reasoning traces for factual recall. Adding a \u201cthink step-by-step\u201d cue yields statistically significant gains in world-knowledge recall but does not significantly affect math performance, indicating that such models are under-optimized specifically for parametric knowledge access. After RL training on TriviaQA to encourage reasoning over parametric knowledge, the model\u2019s performance on TriviaQA improves by 9.9 percentage points. Moreover, this training generalizes to other QA benchmarks, improving scores on Natural Questions by 4.2 points, HotpotQA by 2.1, SimpleQA by 0.6, and StrategyQA by 3.0.", "conclusion": "Reasoning-oriented language models are not automatically good at reasoning over their own stored world knowledge. However, simple prompting with step-by-step reasoning cues and targeted RL training with a verifiable QA reward substantially improve factual recall. This shows that parametric knowledge access is a distinct capability that can be explicitly optimized, and that doing so yields measurable gains across multiple QA benchmarks."}}
{"id": "2602.22200", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22200", "abs": "https://arxiv.org/abs/2602.22200", "authors": ["Cole Simmons", "Richard Diehl Martinez", "Dan Jurafsky"], "title": "SumTablets: A Transliteration Dataset of Sumerian Tablets", "comment": "11 pages with 3 figures", "summary": "Sumerian transliteration is a conventional system for representing a scholar's interpretation of a tablet in the Latin script. Thanks to visionary digital Assyriology projects such as ETCSL, CDLI, and Oracc, a large number of Sumerian transliterations have been published online, and these data are well-structured for a variety of search and analysis tasks. However, the absence of a comprehensive, accessible dataset pairing transliterations with a digital representation of the tablet's cuneiform glyphs has prevented the application of modern Natural Language Processing (NLP) methods to the task of Sumerian transliteration.\n  To address this gap, we present SumTablets, a dataset pairing Unicode representations of 91,606 Sumerian cuneiform tablets (totaling 6,970,407 glyphs) with the associated transliterations published by Oracc. We construct SumTablets by first preprocessing and standardizing the Oracc transliterations before mapping each reading back to the Unicode representation of the source glyph. Further, we retain parallel structural information (e.g., surfaces, newlines, broken segments) through the use of special tokens. We release SumTablets as a Hugging Face Dataset (CC BY 4.0) and open source data preparation code via GitHub.\n  Additionally, we leverage SumTablets to implement and evaluate two transliteration baselines: (1) weighted sampling from a glyph's possible readings, and (2) fine-tuning an autoregressive language model. Our fine-tuned language model achieves an average transliteration character-level F-score (chrF) of 97.55, demonstrating the immediate potential of transformer-based transliteration models in allowing experts to rapidly verify generated transliterations rather than manually transliterating tablets one-by-one.", "AI": {"tldr": "They build and release SumTablets, a large paired dataset of Sumerian cuneiform glyphs and their Latin-script transliterations, and show that transformer models can transliterate very accurately.", "motivation": "Modern NLP has not been widely applied to Sumerian because there has been no large, accessible dataset that directly aligns cuneiform glyphs with their expert transliterations. Existing online transliterations are abundant and structured, but they are not paired at the glyph level with machine-readable representations of the original tablets, blocking automatic transliteration and related tasks.", "method": "They aggregate Sumerian transliterations from Oracc, then preprocess and standardize them. Each transliterated reading is mapped back to a Unicode representation of the corresponding cuneiform glyphs from 91,606 tablets, while preserving structural information like tablet surfaces, line breaks, and damaged segments with special tokens. The resulting aligned corpus is released as a Hugging Face dataset with open-source preparation code. Using this dataset, they implement two transliteration baselines: a simple heuristic that samples readings based on glyph-level ambiguity and a transformer-based autoregressive language model fine-tuned on SumTablets.", "result": "The outcome is SumTablets, a dataset containing 6,970,407 cuneiform glyphs aligned with expert Sumerian transliterations plus structural annotations, available under CC BY 4.0 along with reproducible code. Using SumTablets, they train and evaluate two baseline transliteration systems. The heuristic weighted-sampling baseline provides a simple reference point, while the fine-tuned autoregressive transformer achieves a high average chrF of 97.55 on transliteration, indicating strong automatic performance.", "conclusion": "A large-scale, glyph-aligned Sumerian cuneiform\u2013to\u2013Latin transliteration dataset can be constructed from existing digital corpora and enables highly accurate transformer-based transliteration. The release of SumTablets and baselines opens the door to applying modern NLP to Sumerian, potentially transforming scholarly workflows by shifting experts from doing manual transliteration to validating high-quality machine-generated ones."}}
